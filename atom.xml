<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Qiuyi&#39;s Blog</title>
  
  <subtitle>Researcher✨Qiuyi Wu</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://qiuyiwu.github.io/"/>
  <updated>2019-01-01T20:46:51.677Z</updated>
  <id>https://qiuyiwu.github.io/</id>
  
  <author>
    <name>Qiuyi Wu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Reading Note for Astrostatistics</title>
    <link href="https://qiuyiwu.github.io/2018/12/24/NoteAstro/"/>
    <id>https://qiuyiwu.github.io/2018/12/24/NoteAstro/</id>
    <published>2018-12-24T19:41:01.000Z</published>
    <updated>2019-01-01T20:46:51.677Z</updated>
    
    <content type="html"><![CDATA[<p>I’m starting to read Feigelson &amp; Babu’s <em>Modern Statistical Methods for Astronomy</em> this Christmas and hope to finish it before Spring break in March, 2019. This book covers the fundamental statistics theories and methodologies in application on Astronomy. It also aims to help astronomers perceive megadatas from celestial objects via modern statistical analysis and interpret cosmic phenomena in advanced statistical language. It is the bible for Astrostatistics! I take notes and record here for myself better understanding this fantastic field. </p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Collaborations betweeen astronomers and statisticians:<br><a href="http://hea-www.harvard.edu/AstroStat" target="_blank" rel="noopener">California–Harvard Astro-Statistical Collaboration</a><br><a href="http://www.incagroup.org" target="_blank" rel="noopener">International Computational Astrostatistics Group centered in Pittsburgh</a><br><a href="http://astrostatistics.psu.edu" target="_blank" rel="noopener">Center for Astrostatistics at Penn State</a></p><h2 id="Astronomy"><a href="#Astronomy" class="headerlink" title="Astronomy"></a>Astronomy</h2><h3 id="Term"><a href="#Term" class="headerlink" title="Term:"></a>Term:</h3><p><strong>Astronomy</strong>: the observational study of matter beyond Earth.<br><strong>Astrophysics</strong>: the study of intrinsic nature of astronomical bodies and the processes by which they binteract and evolve. </p><a id="more"></a><h3 id="Fields"><a href="#Fields" class="headerlink" title="Fields:"></a>Fields:</h3><p><strong>Planetary Astronomer</strong>: study the Solar System and sxtra-solar planetary system<br><strong>Solar Physicist</strong>: study the Sun<br><strong>Stellar Astronomer</strong>: study other stars<br><strong>Galactic Astronomer</strong>: study Milky Way Galaxy<br><strong>Extragalactic Astronomer</strong>: study other galaxies<br><strong>Cosmologist</strong>: study the Universe as a whole</p><h2 id="Statistics"><a href="#Statistics" class="headerlink" title="Statistics"></a>Statistics</h2><p>Limitations of the spectrograph and observing conditions lead to uncertainty in the measured radial velocities. The uncertainty in our knowledge could be due to the current level of understanding of the phenomenon, which could be reduced in the future. The uncertainty of our knowledge could be due to future choices or events. </p><h3 id="Space-and-Event"><a href="#Space-and-Event" class="headerlink" title="Space and Event"></a>Space and Event</h3><p><strong>Outcome space/sample space</strong>: The set of all outcomes $\Omega$ of an experiment<br><strong>Event</strong>: subset of a sample space<br><strong>Discrete sample space</strong>: A finite (or countably infinite) sample space</p><p>Astronomers deal with both countable space (such as the number of stars in the Galaxy, or the set of photons from a quasar arriving at a detector) and uncountable spaces (such as the variability characteristics of a quasar, or the background noise in an image constructed from interferometry observations).</p><h3 id="Axioms-of-Probability"><a href="#Axioms-of-Probability" class="headerlink" title="Axioms of Probability"></a>Axioms of Probability</h3><p><strong>Probability space</strong>: $(\Omega, \mathcal{F}, P)$ where $\Omega$ is sample space, $\mathcal{F}$ is a class of events, and $P$ is function that assigns probability to events in $\mathcal{F}$.</p><p><strong>Axiom 1</strong>: $0\leq P(A) \leq 1$, for all events $A$<br><strong>Axiom 2</strong>: $P(\Omega) = 1$<br><strong>Axiom 3</strong>: For mutually exclusive (pairwise disjoint) events $A_1, A_2,…,$ we have $P(A_1\cup A_2\cup A_3 …) = P(A_1)+P(A_2)+P(A_3)+…$<br>$\qquad \qquad$  So if for all $i\neq j, A_i \cap A_j = \emptyset$, then $ P( \bigcup_{i=1}^\infty A_i ) = \sum_{i=1}^\infty P(A_i)  $<br><br><br>The generalization to $n$ events, $E_1,…,E_n$ below is called the <strong>inclusion-exclusion formula</strong>:<br><span>$$\begin{align*}P(E_1\cup E_2\cup ...\cup E_n ) &amp;= \sum_{i=1}^\infty P(E_i) - \sum_{ i_1 &lt; i_2 } P(E_{i_1}\cap E_{i_2}) +... \\&amp;+  (-1)^{r+1} \sum_{ i_1 &lt; i_2 &lt; ... &lt; i_r} P(E_{i_1}\cap E_{i_2} \cap ... \cap E_{i_r})  + ...\\&amp;+  (-1)^{n+1} P(E_1\cap E_2 \cap ... \cap E_n) \end{align*}$$</span><!-- Has MathJax --></p><p><strong>Multiplication rule</strong>: $P(A_1\cap A_2\cap … \cap A_n ) = P(A_1)\times P(A_2|A_1)…P(A_{n-1}|A_1,…A_{n-2})\times P(A_{n}|A_1,…A_{n-1})$</p><p>Except for the rare circumstance when an entirely new phenomenon is discovered, astronomers are measuring properties of celestial bodies or populations for which some distinctive properties are already available. Consider, for example, a subpopulation of galaxies found to exhibit Seyfert-like spectra in the optical band (property A) that have already been examined for nonthermal lobes in the radio band (property B). Then the conditional probability that a galaxy has a Seyfert nucleus given that it also has radio lobes is given by Equation $P(A|B) = \frac{P(A\cap B)}{P(B)}$, and this probability can be estimated from careful study of galaxy samples. The composition of a Solar System minor body can be predominately ices or rock. Icy bodies are more common at large orbital distances and show spectral signatures of water (or other) ice rather than the spectral signatures of silicates. The probability that a given asteroid, comet or Kuiper Belt Object is mostly icy is then conditioned on its semi-major axis and spectral characteristics.</p><p><span style="color:red"><em>To be continue…</em></span></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I’m starting to read Feigelson &amp;amp; Babu’s &lt;em&gt;Modern Statistical Methods for Astronomy&lt;/em&gt; this Christmas and hope to finish it before Spring break in March, 2019. This book covers the fundamental statistics theories and methodologies in application on Astronomy. It also aims to help astronomers perceive megadatas from celestial objects via modern statistical analysis and interpret cosmic phenomena in advanced statistical language. It is the bible for Astrostatistics! I take notes and record here for myself better understanding this fantastic field. &lt;/p&gt;
&lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;p&gt;Collaborations betweeen astronomers and statisticians:&lt;br&gt;&lt;a href=&quot;http://hea-www.harvard.edu/AstroStat&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;California–Harvard Astro-Statistical Collaboration&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://www.incagroup.org&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;International Computational Astrostatistics Group centered in Pittsburgh&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://astrostatistics.psu.edu&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Center for Astrostatistics at Penn State&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Astronomy&quot;&gt;&lt;a href=&quot;#Astronomy&quot; class=&quot;headerlink&quot; title=&quot;Astronomy&quot;&gt;&lt;/a&gt;Astronomy&lt;/h2&gt;&lt;h3 id=&quot;Term&quot;&gt;&lt;a href=&quot;#Term&quot; class=&quot;headerlink&quot; title=&quot;Term:&quot;&gt;&lt;/a&gt;Term:&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Astronomy&lt;/strong&gt;: the observational study of matter beyond Earth.&lt;br&gt;&lt;strong&gt;Astrophysics&lt;/strong&gt;: the study of intrinsic nature of astronomical bodies and the processes by which they binteract and evolve. &lt;/p&gt;
    
    </summary>
    
      <category term="Astrostatistics" scheme="https://qiuyiwu.github.io/categories/Astrostatistics/"/>
    
    
      <category term="Astrostatistics, Astronomy, Statistics" scheme="https://qiuyiwu.github.io/tags/Astrostatistics-Astronomy-Statistics/"/>
    
  </entry>
  
  <entry>
    <title>Data Fusion</title>
    <link href="https://qiuyiwu.github.io/2018/12/20/DataFusion/"/>
    <id>https://qiuyiwu.github.io/2018/12/20/DataFusion/</id>
    <published>2018-12-20T19:47:15.000Z</published>
    <updated>2019-01-03T03:11:47.848Z</updated>
    
    <content type="html"><![CDATA[<p><center><br>Ongoing resaerch projects in SAMSI Program on Model Uncertainty.<br>To be announced in May, 2019.</center></p><p>Summary:<br>In fall 2018, I presented my previous work (collaborating with Ernest Fokoue) about music mining in the group meeting, and also submitted a paper [1] to introduce the idea of representing any given piece of music as a collection of “musical words” that we codenamed “muselets”, which are essentially musical words of various lengths. We specifically herein construct a naive dictionary featuring a corpus made up of African American, Chinese, Japanese and Arabic music, on which we perform both topic modelling and pattern recognition. Although some of the results based on the Naive Dictionary are reasonably good, we anticipate phenomenal predictive performances once we get around to actually building a full scale complete version of our intended dictionary of muselets. The idea of Data Fusion in this work is that we create uniform representation of music based on different sources and forms of musical data.<br>In spring 2019, I will collaborate with Dr. Jong-Min Kim who extensively studies Mixture of D-vine copulas [2]. We plan to combine text mining and copula methods in the application of movie markets. Specifically, we study Chinese and American movie markets, to see the effects of different types of movies on the returns. For the most profitable movies, we study multiple effects that make those movies win the sales (potential factors: movie types, movie director, actors, actress etc.). </p><a id="more"></a><p>[1] Wu, Qiuyi; Fokoue, Ernest. (2018) Naive Dictionary On Musical Corpora: From Knowledge Representation To Pattern Recognition. arXiv:1811.12802<br>[2] Kim, D., Kim, J. M., Liao, S. M., &amp; Jung, Y. S. (2013). Mixture of D-vine copulas for modeling dependence. Computational Statistics &amp; Data Analysis, 64, 1-19.</p><p>All rights reserved &copy; Copyright 2018, Qiuyi Wu.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;center&gt;&lt;br&gt;Ongoing resaerch projects in SAMSI Program on Model Uncertainty.&lt;br&gt;To be announced in May, 2019.&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Summary:&lt;br&gt;In fall 2018, I presented my previous work (collaborating with Ernest Fokoue) about music mining in the group meeting, and also submitted a paper [1] to introduce the idea of representing any given piece of music as a collection of “musical words” that we codenamed “muselets”, which are essentially musical words of various lengths. We specifically herein construct a naive dictionary featuring a corpus made up of African American, Chinese, Japanese and Arabic music, on which we perform both topic modelling and pattern recognition. Although some of the results based on the Naive Dictionary are reasonably good, we anticipate phenomenal predictive performances once we get around to actually building a full scale complete version of our intended dictionary of muselets. The idea of Data Fusion in this work is that we create uniform representation of music based on different sources and forms of musical data.&lt;br&gt;In spring 2019, I will collaborate with Dr. Jong-Min Kim who extensively studies Mixture of D-vine copulas [2]. We plan to combine text mining and copula methods in the application of movie markets. Specifically, we study Chinese and American movie markets, to see the effects of different types of movies on the returns. For the most profitable movies, we study multiple effects that make those movies win the sales (potential factors: movie types, movie director, actors, actress etc.). &lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="https://qiuyiwu.github.io/categories/Research/"/>
    
    
      <category term="Data Fusion" scheme="https://qiuyiwu.github.io/tags/Data-Fusion/"/>
    
  </entry>
  
  <entry>
    <title>Music Mining</title>
    <link href="https://qiuyiwu.github.io/2018/12/20/MusicMining/"/>
    <id>https://qiuyiwu.github.io/2018/12/20/MusicMining/</id>
    <published>2018-12-20T19:40:32.000Z</published>
    <updated>2018-12-21T04:55:25.310Z</updated>
    
    <content type="html"><![CDATA[<p>Music and text are similar in the way that both of them can be regraded as information carrier and emotion deliverer. People get daily information from reading newspaper, magazines, blogs etc., and they can also write diary or personal journal to reflect on daily life, let out pent up emotions, record ideas and experience. Same power could come from music! Composers express their feelings through music with different combinations of notes, diverse tempo, and dynamics levels, as another version of language. All these similarities drive people to ask questions like:</p><ul><li>Could music deliver information tantamount to text?</li><li>Can we efficiently use text mining approach in music field?</li><li>Why music from diverse culture can bring people so many different feelings?</li><li>What’s the similarity between music from different culture, or composers, or genres?</li><li>To what extend do people grasp the meaning behind each piece of music expressed by the composer?</li></ul><a id="more"></a><p>Take the tragedy Titanic as an example, we learn the tragedy from the newspaper and feel anguished, but we can also get the mourning from the song <em>My Heart Will Go On</em>. The melody contains a lot of minor keys (e.g. $D\flat$, $F\sharp$, $A\flat$), which are more likely to trigger the dissonance via two closely spaced notes hitting the ear simultaneously and thus to make people feel sad.</p><center><br><img class="left" src="/images/2018/12/titanic.png" width="80%" height="80%"><br></center><p>In this project we employ latent Dirichlet allocation model into the music concept. Assume an album, as a collection of songs, are the mixture of different topics (melodies). These topics are the distributions over a series of notes (left part of the figure). In each song, notes in every measure are chosen based on the topic assignments (colorful tokens), while the topic assignments are drawn from the document-topic distribution.</p><center><br><img class="left" src="/images/2018/12/music.png" width="80%" height="80%"><br></center><p>More details can be found in my <a href="https://arxiv.org/abs/1811.12802" target="_blank" rel="noopener">paper</a> and <a href="https://scholarworks.rit.edu/theses/9932/" target="_blank" rel="noopener">thesis</a>.</p><p>All rights reserved &copy; Copyright 2018, Qiuyi Wu.</p><p>Recommended Citation:<br>[1] Wu, Qiuyi, and Ernest Fokoue. “Naive Dictionary On Musical Corpora: From Knowledge Representation To Pattern Recognition.” arXiv preprint arXiv:1811.12802 (2018).</p><p>[2] Wu, Qiuyi, “Statistical Aspects of Music Mining: Naive Dictionary Representation” (2018). Thesis. Rochester Institute of Technology. Accessed from <a href="https://scholarworks.rit.edu/theses/9932" target="_blank" rel="noopener">https://scholarworks.rit.edu/theses/9932</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Music and text are similar in the way that both of them can be regraded as information carrier and emotion deliverer. People get daily information from reading newspaper, magazines, blogs etc., and they can also write diary or personal journal to reflect on daily life, let out pent up emotions, record ideas and experience. Same power could come from music! Composers express their feelings through music with different combinations of notes, diverse tempo, and dynamics levels, as another version of language. All these similarities drive people to ask questions like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Could music deliver information tantamount to text?&lt;/li&gt;
&lt;li&gt;Can we efficiently use text mining approach in music field?&lt;/li&gt;
&lt;li&gt;Why music from diverse culture can bring people so many different feelings?&lt;/li&gt;
&lt;li&gt;What’s the similarity between music from different culture, or composers, or genres?&lt;/li&gt;
&lt;li&gt;To what extend do people grasp the meaning behind each piece of music expressed by the composer?&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Research" scheme="https://qiuyiwu.github.io/categories/Research/"/>
    
    
      <category term="Music, Data Mining, Topic Modeling" scheme="https://qiuyiwu.github.io/tags/Music-Data-Mining-Topic-Modeling/"/>
    
  </entry>
  
  <entry>
    <title>Storm Surge</title>
    <link href="https://qiuyiwu.github.io/2018/12/19/StormSurge/"/>
    <id>https://qiuyiwu.github.io/2018/12/19/StormSurge/</id>
    <published>2018-12-20T03:19:21.000Z</published>
    <updated>2018-12-21T16:14:41.497Z</updated>
    
    <content type="html"><![CDATA[<p><center><br>Ongoing resaerch projects in SAMSI Program on Model Uncertainty.<br>To be announced in May, 2019.</center></p><p>Mid-term summary:<br>In fall 2018, I mainly focused on input distribution subgroup demonstrated my initial exploratory analysis of KE synthetic storm tracks, and compared the simulated tracks with the real IBTrack storm data. One of the main discussions in this semester is how to improve the current practice with the technique of spatial statistics, such as using hierarchical model to improve the estimation of input distribution, or spatial-temporal point process modeling for the storm occurrence rate. Diverse existing projects giving by different researchers are shared and discussed in weekly group meetings.<br>For spring 2019, we will (1.) Measure the model of storm evolution (e.g. the sudden change of the characteristics when the storms landed based on the coastline and how simulation data handle this); (2.) Think about how to impose some structure to improve the estimation concerning input distribution.</p><a id="more"></a><p>All rights reserved &copy; Copyright 2018, Qiuyi Wu.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;center&gt;&lt;br&gt;Ongoing resaerch projects in SAMSI Program on Model Uncertainty.&lt;br&gt;To be announced in May, 2019.&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;Mid-term summary:&lt;br&gt;In fall 2018, I mainly focused on input distribution subgroup demonstrated my initial exploratory analysis of KE synthetic storm tracks, and compared the simulated tracks with the real IBTrack storm data. One of the main discussions in this semester is how to improve the current practice with the technique of spatial statistics, such as using hierarchical model to improve the estimation of input distribution, or spatial-temporal point process modeling for the storm occurrence rate. Diverse existing projects giving by different researchers are shared and discussed in weekly group meetings.&lt;br&gt;For spring 2019, we will (1.) Measure the model of storm evolution (e.g. the sudden change of the characteristics when the storms landed based on the coastline and how simulation data handle this); (2.) Think about how to impose some structure to improve the estimation concerning input distribution.&lt;/p&gt;
    
    </summary>
    
      <category term="Research" scheme="https://qiuyiwu.github.io/categories/Research/"/>
    
    
      <category term="Spatial Statistics, Storm Surge" scheme="https://qiuyiwu.github.io/tags/Spatial-Statistics-Storm-Surge/"/>
    
  </entry>
  
  <entry>
    <title>Research on Public Policy Blogs</title>
    <link href="https://qiuyiwu.github.io/2018/06/05/Public-Policy-Blogs/"/>
    <id>https://qiuyiwu.github.io/2018/06/05/Public-Policy-Blogs/</id>
    <published>2018-06-05T15:03:24.000Z</published>
    <updated>2018-12-21T05:00:29.339Z</updated>
    
    <content type="html"><![CDATA[<p>Use different Topic Modeling approaches on Political Blogs to see the performance of diverse methods.</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="Types-of-Models-in-Comparison"><a href="#Types-of-Models-in-Comparison" class="headerlink" title="Types of Models in Comparison"></a>Types of Models in Comparison</h2><ul><li>General LDA (<a href="https://cran.r-project.org/web/packages/lda/lda.pdf" target="_blank" rel="noopener">R package</a>)</li><li>Supervised LDA (<a href="https://arxiv.org/pdf/1003.0783.pdf" target="_blank" rel="noopener">David M. Blei, Jon D. McAuliffe</a>)</li><li>Relational Topic Model (<a href="http://proceedings.mlr.press/v5/chang09a/chang09a.pdf" target="_blank" rel="noopener">Jonathan Chang, David M. Blei</a>)</li><li>Topic Link Block Model (Derek Owens-Oas)</li><li>Poisson Factor Modeling (<a href="http://people.ee.duke.edu/~lcarin/Mingyuan_PAMI_6.pdf" target="_blank" rel="noopener">Beta Negative Binomial Process Topic Model</a>)</li><li>Dynamic Text Network Model (<a href="https://arxiv.org/pdf/1610.05756.pdf" target="_blank" rel="noopener">Teague Henry, David Banks et al.</a>)</li></ul><a id="more"></a><h2 id="Key-Values-in-Cleaned-Blog-Posts"><a href="#Key-Values-in-Cleaned-Blog-Posts" class="headerlink" title="Key Values in Cleaned Blog Posts"></a>Key Values in Cleaned Blog Posts</h2><p>After preprocessing the text extracted from blog posts:</p><ul><li>dates: string of the given date in mm/dd/yy format</li><li>domains: string of the blog website where post was found  (remove “www.”)</li><li>links: string of other websites occured in the post as hyperlinks  (sorted alphabetically)</li><li>words: filtered words from raw text in the blog posts  (TFIDF variance threading used)</li><li>rawText: direct content from blog posts  (remove short posts and duplicate posts )</li><li>words_stem: stemmed words using Hunspell stemmer  (e.g., apples -&gt; apple)</li></ul><h1 id="Analysis-via-several-Topic-Modeling-Methods"><a href="#Analysis-via-several-Topic-Modeling-Methods" class="headerlink" title="Analysis via several Topic Modeling Methods"></a>Analysis via several Topic Modeling Methods</h1><h2 id="General-LDA"><a href="#General-LDA" class="headerlink" title="General LDA"></a>General LDA</h2><p>General LDA Model via Collapsed Gibbs Sampling Methods for Topic Models: </p><center><br><img class="left" src="/images/2018/06/lda.png" width="80%" height="80%"><br></center><h2 id="Supervised-LDA"><a href="#Supervised-LDA" class="headerlink" title="Supervised LDA"></a>Supervised LDA</h2><p>Here use Blog Site as labels.</p><center><br><img class="left" src="/images/2018/06/slda.png" width="80%" height="80%"><br></center><h2 id="Relational-Topic-Model"><a href="#Relational-Topic-Model" class="headerlink" title="Relational Topic Model"></a>Relational Topic Model</h2><p>RTM models the link as binary random variable that is conditioned on their text. The model can predict links between documents and predict words within them. The algorithm is based on variational EM algorithm.</p><ol><li>For each document $d$:<ol><li>Draw topic proportions $\theta_d|\alpha \sim \text{Dir}(\alpha)$</li><li>For each word $w_{d,n}$:</li></ol><ul><li>Draw assignment $z_{d,n}|\theta_d \sim \text{Mult}(\theta_d)$</li><li>Draw word w<sub>d,n</sub> | z<sub>d,n</sub>, $\beta$<sub>1:K</sub>$\sim \text{Mult}(\beta$<sub>z<sub>d,n</sub></sub>$)$</li></ul></li><li>For each pair of documents $d,d’$:<ul><li>Draw binary link indicator $y|z_d,z$ <sub>d’</sub> $\sim \psi (\cdot | z_d,z$ <sub>d’</sub> $)$</li></ul></li></ol><center><br><img class="left" src="/images/2018/06/rtm.png" width="80%" height="80%"><br></center><p>Compare the performance of link prediction with the one of LDA. The plot below shows the predicted link probabilities from RTM against the ones of LDA for each document, and also shows the most expressed topics by the cited document. (sample 100) </p><center><br><img class="left" src="/images/2018/06/rtm2.png" width="80%" height="80%"><br></center><p>All rights reserved &copy; Copyright 2018, Qiuyi Wu.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Use different Topic Modeling approaches on Political Blogs to see the performance of diverse methods.&lt;/p&gt;
&lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;h2 id=&quot;Types-of-Models-in-Comparison&quot;&gt;&lt;a href=&quot;#Types-of-Models-in-Comparison&quot; class=&quot;headerlink&quot; title=&quot;Types of Models in Comparison&quot;&gt;&lt;/a&gt;Types of Models in Comparison&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;General LDA (&lt;a href=&quot;https://cran.r-project.org/web/packages/lda/lda.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;R package&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Supervised LDA (&lt;a href=&quot;https://arxiv.org/pdf/1003.0783.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;David M. Blei, Jon D. McAuliffe&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Relational Topic Model (&lt;a href=&quot;http://proceedings.mlr.press/v5/chang09a/chang09a.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Jonathan Chang, David M. Blei&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Topic Link Block Model (Derek Owens-Oas)&lt;/li&gt;
&lt;li&gt;Poisson Factor Modeling (&lt;a href=&quot;http://people.ee.duke.edu/~lcarin/Mingyuan_PAMI_6.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Beta Negative Binomial Process Topic Model&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Dynamic Text Network Model (&lt;a href=&quot;https://arxiv.org/pdf/1610.05756.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Teague Henry, David Banks et al.&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Research" scheme="https://qiuyiwu.github.io/categories/Research/"/>
    
    
      <category term="Topic Modeling, Network Analysis" scheme="https://qiuyiwu.github.io/tags/Topic-Modeling-Network-Analysis/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow for Deep Learning 3</title>
    <link href="https://qiuyiwu.github.io/2018/01/22/TensorFlow-for-Deep-Learning-3/"/>
    <id>https://qiuyiwu.github.io/2018/01/22/TensorFlow-for-Deep-Learning-3/</id>
    <published>2018-01-23T01:44:46.000Z</published>
    <updated>2018-12-21T04:59:25.522Z</updated>
    
    <content type="html"><![CDATA[<p>Continue the learning process of Convolutional Neural Networks and Recurrent Neural Networks with TensorFlow in Jupyter Notebook.</p><h1 id="RNN-with-TensorFlow"><a href="#RNN-with-TensorFlow" class="headerlink" title="RNN with TensorFlow"></a>RNN with TensorFlow</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="comment"># class: create the data, generate the batches to send it back</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TimeSeriesData</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_points, xmin, xmax)</span>:</span></span><br><span class="line">        self.xmin = xmin</span><br><span class="line">        self.xmax = xmax</span><br><span class="line">        self.num_points = num_points</span><br><span class="line">        self.resolution = (xmax - xmin)/num_points</span><br><span class="line">        self.x_data = np.linspace(xmin, xmax, num_points)</span><br><span class="line">        self.y_true = np.sin(self.x_data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ret_true</span><span class="params">(self, x_series)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> np.sin(x_series)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next_batch</span><span class="params">(self, batch_size, steps, return_batch_ts = False)</span>:</span></span><br><span class="line">        <span class="comment"># grab a random starting point for each batch</span></span><br><span class="line">        random_start = np.random.rand(batch_size,<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># convert the data to TS</span></span><br><span class="line">        ts_start = random_start * (self.xmax - self.xmin - (steps * self.resolution))</span><br><span class="line">        <span class="comment"># create batch time series on the x axis</span></span><br><span class="line">        batch_ts = ts_start + np.arange(<span class="number">0.0</span>,steps+<span class="number">1</span>) * self.resolution</span><br><span class="line">        <span class="comment"># create the Y data for the time series x axis from previous step</span></span><br><span class="line">        y_batch = np.sin(batch_ts)</span><br><span class="line">        <span class="comment"># formatting for RNN</span></span><br><span class="line">        <span class="keyword">if</span> return_batch_ts:</span><br><span class="line">            <span class="keyword">return</span> y_batch[:,:<span class="number">-1</span>].reshape(<span class="number">-1</span>,steps,<span class="number">1</span>), y_batch[:,<span class="number">1</span>:].reshape(<span class="number">-1</span>,steps,<span class="number">1</span>), batch_ts</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> y_batch[:,:<span class="number">-1</span>].reshape(<span class="number">-1</span>,steps,<span class="number">1</span>), y_batch[:,<span class="number">1</span>:].reshape(<span class="number">-1</span>,steps,<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># original y_batch and y_batch shifted over 1 step in the future</span></span><br></pre></td></tr></table></figure><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Give the data and try the model!</span></span><br><span class="line">ts_data = TimeSeriesData(<span class="number">250</span>,<span class="number">0</span>,<span class="number">10</span>)</span><br><span class="line">plt.plot(ts_data.x_data, ts_data.y_true)</span><br></pre></td></tr></table></figure><center><br><img class="left" src="/images/2018/01/sin1.png" width="50%" height="50%"><br></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">num_time_steps = <span class="number">60</span></span><br><span class="line"><span class="comment"># Set 1 batch, 60 steps, return batch time series on x axis</span></span><br><span class="line">y1, y2, ts = ts_data.next_batch(<span class="number">1</span>, num_time_steps, <span class="keyword">True</span>)</span><br><span class="line">plt.plot(ts.flatten()[<span class="number">1</span>:], y2.flatten(), <span class="string">'*'</span>)</span><br><span class="line"><span class="comment"># ts.flatten(): Return a copy of the array collapsed into one dimension.</span></span><br><span class="line"><span class="comment"># ts has total 251 points (including the prediction), so start from 1, to match y2</span></span><br></pre></td></tr></table></figure><center><br><img class="left" src="/images/2018/01/sin2.png" width="50%" height="50%"><br></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(ts_data.x_data, ts_data.y_true, label =<span class="string">'Sin(t)'</span>)</span><br><span class="line">plt.plot(ts.flatten()[<span class="number">1</span>:], y2.flatten(), <span class="string">'*'</span>, label = <span class="string">'Single Training Example'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.tight_layout()   <span class="comment"># Automatically adjust subplot parameters to give specified padding.</span></span><br></pre></td></tr></table></figure><center><br><img class="left" src="/images/2018/01/sin3.png" width="50%" height="50%"><br></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Demonstrate what's going on of the training in the model, shift over 1 step</span></span><br><span class="line">num_time_steps = <span class="number">30</span></span><br><span class="line">train_example = np.linspace(<span class="number">5</span>, <span class="number">5</span>+ts_data.resolution*(num_time_steps + <span class="number">1</span>), num_time_steps + <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">'A Traning Example'</span>)</span><br><span class="line">plt.plot(train_example[:<span class="number">-1</span>], ts_data.ret_true(train_example[:<span class="number">-1</span>]), <span class="string">'bo'</span>, markersize = <span class="number">15</span>, alpha = <span class="number">0.5</span>, label = <span class="string">'Example'</span>)</span><br><span class="line">plt.plot(train_example[<span class="number">1</span>:], ts_data.ret_true(train_example[<span class="number">1</span>:]), <span class="string">'ko'</span>, markersize = <span class="number">7</span>, label = <span class="string">'Target'</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure><center><br><img class="left" src="/images/2018/01/sin4.png" width="50%" height="50%"><br></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create the model</span></span><br><span class="line">tf.reset_default_graph()</span><br><span class="line">num_inputs = <span class="number">1</span></span><br><span class="line">num_neurons = <span class="number">100</span></span><br><span class="line">num_outputs = <span class="number">1</span></span><br><span class="line">learning_rate = <span class="number">0.0001</span></span><br><span class="line">num_train_iterations = <span class="number">2000</span></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Placeholder</span></span><br><span class="line">X = tf.placeholder(tf.float32, [<span class="keyword">None</span>, num_time_steps, num_inputs])</span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="keyword">None</span>, num_time_steps, num_outputs])</span><br><span class="line"><span class="comment"># Run cell layer</span></span><br><span class="line">cell_input = tf.contrib.rnn.BasicRNNCell(num_units = num_neurons, activation = tf.nn.relu) <span class="comment"># Can try other cell: GRUCell etc.</span></span><br><span class="line">cell = tf.contrib.rnn.OutputProjectionWrapper(cell_input, output_size = num_outputs)</span><br><span class="line"></span><br><span class="line">outputs, states = tf.nn.dynamic_rnn(cell, X, dtype = tf.float32)</span><br><span class="line"><span class="comment"># MSE</span></span><br><span class="line">loss = tf.reduce_mean(tf.square(outputs-y))</span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)</span><br><span class="line">train = optimizer.minimize(loss)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="comment"># Session</span></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> iteration <span class="keyword">in</span> range(num_train_iterations):</span><br><span class="line">        X_batch, y_batch = ts_data.next_batch(batch_size, num_time_steps)</span><br><span class="line">        sess.run(train, feed_dict = &#123;X: X_batch, y: y_batch&#125;)</span><br><span class="line">        <span class="keyword">if</span> iteration % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            mse = loss.eval(feed_dict = &#123;X: X_batch, y: y_batch&#125;)</span><br><span class="line">            print(iteration, <span class="string">'\tMSE'</span>, mse)</span><br><span class="line">    saver.save(sess, <span class="string">'./rnn_time_series_model_codealong'</span>)</span><br></pre></td></tr></table></figure><center><br><img class="left" src="/images/2018/01/sin5.png" width="30%" height="30%"><br></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># predict 1 step in the future</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, <span class="string">'./rnn_time_series_model_codealong'</span>)</span><br><span class="line"></span><br><span class="line">    X_new = np.sin(np.array(train_example[:<span class="number">-1</span>].reshape(<span class="number">-1</span>,num_time_steps, num_inputs)))</span><br><span class="line">    y_pred = sess.run(outputs, feed_dict = &#123;X: X_new&#125;)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'Test The Model'</span>)</span><br><span class="line"><span class="comment"># Training example</span></span><br><span class="line">plt.plot(train_example[:<span class="number">-1</span>], np.sin(train_example[:<span class="number">-1</span>]),<span class="string">'bo'</span>, markersize = <span class="number">15</span>, alpha = <span class="number">0.5</span>, label = <span class="string">'Training Example'</span>)</span><br><span class="line"><span class="comment"># Target to predict</span></span><br><span class="line">plt.plot(train_example[<span class="number">1</span>:], np.sin(train_example[<span class="number">1</span>:]),<span class="string">'ko'</span>, markersize = <span class="number">10</span>, label = <span class="string">'Target'</span>)</span><br><span class="line"><span class="comment"># Model prediction</span></span><br><span class="line">plt.plot(train_example[<span class="number">1</span>:], y_pred[<span class="number">0</span>,:,<span class="number">0</span>], <span class="string">'r.'</span>, markersize = <span class="number">10</span>, label = <span class="string">'Prediction'</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">'Time'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.tight_layout()</span><br></pre></td></tr></table></figure><center><br><img class="left" src="/images/2018/01/sin6.png" width="50%" height="50%"><br></center><br>To further explore the performance of RNN model, we can change the number of iteration <code>num_train_iterations</code>, learning rate <code>learning_rate</code>, and model type <code>tf.contrib.rnn.BasicRNNCell()</code> and then play with it.<br><br>About just for time sequence that shifts 1 time step ahead, now we’ll generate a completely new sequence.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, <span class="string">'./rnn_time_series_model_codealong'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># seed zeros</span></span><br><span class="line">    <span class="comment"># 30 0s and the generated data</span></span><br><span class="line">    zero_seq_seed = [<span class="number">0.0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(num_time_steps)]</span><br><span class="line">    <span class="keyword">for</span> iteration <span class="keyword">in</span> range(len(ts_data.x_data) - num_time_steps):</span><br><span class="line">        X_batch = np.array(zero_seq_seed[-num_time_steps:]).reshape(<span class="number">1</span>, num_time_steps,<span class="number">1</span>)</span><br><span class="line">        y_pred = sess.run(outputs, feed_dict = &#123;X: X_batch&#125;)</span><br><span class="line">        zero_seq_seed.append(y_pred[<span class="number">0</span>,<span class="number">-1</span>,<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">plt.plot(ts_data.x_data, zero_seq_seed, <span class="string">'b-'</span>)</span><br><span class="line">plt.plot(ts_data.x_data[:num_time_steps], zero_seq_seed[:num_time_steps], <span class="string">'r'</span>, linewidth =<span class="number">3</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Time'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Y'</span>)</span><br></pre></td></tr></table></figure><br><br><center><br><img class="left" src="/images/2018/01/sin7.png" width="50%" height="50%"><br></center><p>Instead of zeros at the beginning, now use training example, other parts remain the same<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, <span class="string">'./rnn_time_series_model_codealong'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 30 training points and the generated data</span></span><br><span class="line">    training_example = list(ts_data.y_true[:<span class="number">30</span>])</span><br><span class="line">    <span class="keyword">for</span> iteration <span class="keyword">in</span> range(len(ts_data.x_data) - num_time_steps):</span><br><span class="line">        X_batch = np.array(training_example[-num_time_steps:]).reshape(<span class="number">1</span>, num_time_steps,<span class="number">1</span>)</span><br><span class="line">        y_pred = sess.run(outputs, feed_dict = &#123;X: X_batch&#125;)</span><br><span class="line">        training_example.append(y_pred[<span class="number">0</span>,<span class="number">-1</span>,<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">plt.plot(ts_data.x_data, training_example, <span class="string">'b-'</span>)</span><br><span class="line">plt.plot(ts_data.x_data[:num_time_steps], training_example[:num_time_steps], <span class="string">'r'</span>, linewidth =<span class="number">3</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Time'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Y'</span>)</span><br></pre></td></tr></table></figure></p><center><br><img class="left" src="/images/2018/01/sin8.png" width="50%" height="50%"><br></center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Continue the learning process of Convolutional Neural Networks and Recurrent Neural Networks with TensorFlow in Jupyter Notebook.&lt;/p&gt;
&lt;h1 id=&quot;RNN-with-TensorFlow&quot;&gt;&lt;a href=&quot;#RNN-with-TensorFlow&quot; class=&quot;headerlink&quot; title=&quot;RNN with TensorFlow&quot;&gt;&lt;/a&gt;RNN with TensorFlow&lt;/h1&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; np&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; tensorflow &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; tf&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; plt&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;%matplotlib inline&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# class: create the data, generate the batches to send it back&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;TimeSeriesData&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, num_points, xmin, xmax)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.xmin = xmin&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.xmax = xmax&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.num_points = num_points&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.resolution = (xmax - xmin)/num_points&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.x_data = np.linspace(xmin, xmax, num_points)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.y_true = np.sin(self.x_data)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;ret_true&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, x_series)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; np.sin(x_series)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;next_batch&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, batch_size, steps, return_batch_ts = False)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;comment&quot;&gt;# grab a random starting point for each batch&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        random_start = np.random.rand(batch_size,&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;comment&quot;&gt;# convert the data to TS&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ts_start = random_start * (self.xmax - self.xmin - (steps * self.resolution))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;comment&quot;&gt;# create batch time series on the x axis&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        batch_ts = ts_start + np.arange(&lt;span class=&quot;number&quot;&gt;0.0&lt;/span&gt;,steps+&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;) * self.resolution&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;comment&quot;&gt;# create the Y data for the time series x axis from previous step&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        y_batch = np.sin(batch_ts)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;comment&quot;&gt;# formatting for RNN&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; return_batch_ts:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; y_batch[:,:&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;].reshape(&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;,steps,&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;), y_batch[:,&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;:].reshape(&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;,steps,&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;), batch_ts&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; y_batch[:,:&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;].reshape(&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;,steps,&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;), y_batch[:,&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;:].reshape(&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;,steps,&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;comment&quot;&gt;# original y_batch and y_batch shifted over 1 step in the future&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Tensorflow" scheme="https://qiuyiwu.github.io/categories/Tensorflow/"/>
    
    
      <category term="CNN, RNN, API" scheme="https://qiuyiwu.github.io/tags/CNN-RNN-API/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow for Deep Learning 2</title>
    <link href="https://qiuyiwu.github.io/2018/01/21/TensorFlow-for-Deep-Learning-2/"/>
    <id>https://qiuyiwu.github.io/2018/01/21/TensorFlow-for-Deep-Learning-2/</id>
    <published>2018-01-21T18:50:21.000Z</published>
    <updated>2018-12-21T04:54:31.318Z</updated>
    
    <content type="html"><![CDATA[<p>Here I’ll give the theory part of Neural Networks, sepecifically three kinds of NN: Normal Neural Networks, Convolutional Neural Networks, Recurrent Neural Networks. </p><h1 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks"></a>Neural Networks</h1><h2 id="In-single-neuron"><a href="#In-single-neuron" class="headerlink" title="In single neuron:"></a>In single neuron:</h2><span>$$\begin{align*}z &amp;= Wx + b \\a &amp;= \sigma (z)\end{align*}$$</span><!-- Has MathJax --><h2 id="Activation-Function"><a href="#Activation-Function" class="headerlink" title="Activation Function"></a>Activation Function</h2><ul><li><strong>Perceptron:</strong> binary classifier, small changes are not reflected.<span>$$\begin{align*}f(x) = \begin{cases}1 &amp; \text{if } Wx+b&gt;0\\ 0 &amp; \text{otherwise}\end{cases}\end{align*}$$</span><!-- Has MathJax --></li></ul><a id="more"></a><center><br><img src="/images/2018/01/activation1.png" width="50%" height="50%"><br></center><br><em> <strong>Sigmoid:</strong> special case of the logistic function with S shape curve, more dynamic<br><span>$$\begin{align*}S(x) = \frac{1}{1 + e^{-x}} = \frac{e^x}{e^x +1} \end{align*}$$</span><!-- Has MathJax --><br><center><br><img src="/images/2018/01/activation2.png" width="50%" height="50%"><br></center></em> <strong>Hyperbolic Tangent</strong>: Tanh(z)<br><span>$$\begin{align*}cosh x &amp;= \frac{e^x + e^{-x}}{2}\\ sinh x &amp;= \frac{e^x - e^{-x}}{2}\\ tanh x &amp;= \frac{cosh x}{sinh x }\end{align*}$$</span><!-- Has MathJax --><br><center><br><img src="/images/2018/01/activation3.png" width="50%" height="50%"><br></center><ul><li><p><strong>ReLU (Rectified Linear Unit):</strong> max(0,z)<br><center><br><img src="/images/2018/01/activation4.png" width="50%" height="50%"><br></center><br>ReLU and tanh tend to have the best performance.</p></li><li><p><strong>Softmax Regression:</strong> </p><span>$$\begin{align*}z_i &amp;= \sum_jW_{i,j}x_j + b_i  \\softmax(z)_i &amp;= \frac{\text{exp}(z_i)}{\sum_j \text{exp}(z_j)}\end{align*}$$</span><!-- Has MathJax --></li></ul><h2 id="Cost-Loss-Function"><a href="#Cost-Loss-Function" class="headerlink" title="Cost/Loss Function"></a>Cost/Loss Function</h2><p>Cost function is the measurement of the error.<br><span>$$\begin{align*}z &amp;= Wx + b \\a &amp;= \sigma (z)\\C &amp;= \frac{1}{n}\sum(y_{true} - a)^2 &amp;\text{Quadratic Cost}\\C &amp;= -\frac{1}{n}\sum(y_{true}\cdot ln(a) + (1-y_{true})\cdot ln(1-a) ) &amp;\text{Cross Entropy}\end{align*}$$</span><!-- Has MathJax --><br><strong>Quadratic Cost:</strong><br>The larger errors are more prominent due to the squaring. It causes a slowdown in learning speed.<br><strong>Cross Entropy:</strong><br>It allows for faster learning. The larger the difference, the faster the neuron can learn.</p><h2 id="Gradient-Descent-amp-Backpropagation"><a href="#Gradient-Descent-amp-Backpropagation" class="headerlink" title="Gradient Descent &amp; Backpropagation"></a>Gradient Descent &amp; Backpropagation</h2><p><strong>Gradient Descent</strong> is an optimization algorithm for finding the minimum of a function. Here it minimizes the error to find the optiml value. 1-D example below shows the best parameter value (weights of the neuron inputs) we should choose to minimize the cost.</p><center><br><img src="/images/2018/01/GD.png" width="30%" height="30%"><br></center><br>For complicated cases more than 1 dimension, we use biilt-in algebra of Deep learning library to get the optimal parameters.<br><br>1. <em>Learning Rate:</em> defines the step size during gradient descent, too small - slow pace, too small - overshooting<br>2. <em>Batch Size:</em> batches allow us to use stochastic gradient descent, in case the datasets are large, if all the them are fed at once the computation would be very expensive. Too small - less representative of data, too large - longer training time<br>3. <em>Second-Order Behavior of Gradient Descent:</em> adjust the learning rate based on the rate of descent(second-order behavior: derivative),large learning rate at the beginning, adjust to slower learning rate as it get closer. Methods: AdaGrad, RMSProp, Adam<br><br>Vanishing Gradients: when increasing the number of layers in a network, the layers towards the input will be affected less by the error calculation occuring at the output as going backwards throught the network. Initialization and Normalization will help to mitigate the issue.<br><br><strong>Backpropagation</strong> is to calculate the error contribution of each neuron after a batch of data is processed. It works by calculating the error at the output and then distributes back through the network layers. It belongs to supervised learning as it requires a known output for each input value. The mathematical detail is showed below from Andrew Ng’s Neural Networks and Deep Learning in Coursera.<br><center><br><img src="/images/2018/01/BP.png" width="70%" height="70%"><br></center><h2 id="Initialization-of-Weights"><a href="#Initialization-of-Weights" class="headerlink" title="Initialization of Weights"></a>Initialization of Weights</h2><h3 id="Zeros"><a href="#Zeros" class="headerlink" title="Zeros"></a>Zeros</h3><p>No randomness (too subjective) so not a good choice</p><h3 id="Random-Distribution"><a href="#Random-Distribution" class="headerlink" title="Random Distribution"></a>Random Distribution</h3><p>Random distribution near zero is not optimal and results in activaion function distorition (distorted to large values)</p><h3 id="Xavier-Glorot-Initialization"><a href="#Xavier-Glorot-Initialization" class="headerlink" title="Xavier (Glorot) Initialization"></a>Xavier (Glorot) Initialization</h3><p>The weights drawn from uniform or normal distribution, with <strong>zero mean</strong> and <strong>specific variance</strong> $\text{Var}(W) =\frac{1}{n_{in}} $:<br><span>$$\begin{align*}&amp;Y = W_1X_1 +W_2X_2 + ... + W_nX_n\\&amp;\text{Var}(W_iX_i) = E[X_i]^2\text{Var}(W_i) + E[W_i]^2\text{Var}(X_i) + \text{Var}(W_i)\text{Var}(X_i)\\ &amp;\text{Var}(W_iX_i) = \text{Var}(W_i)\text{Var}(X_i) \qquad (\because E[X_i] = 0)\\ &amp;\text{Var}(Y) = \text{Var}(W_1X_1 + W_2X_2 +... + W_nX_n)  = n  \text{Var}(W_i)\text{Var}(X_i) \\ &amp;\because \text{Variance of the output is equal to the variance of the input}\\ &amp;\therefore n\text{Var}(W_i) = 1\\&amp;\therefore \text{Var}(W_i) = \frac{1}{n} =  \frac{1}{n_{in}}  = \frac{2}{n_{in} + n_{out} }\end{align*}$$</span><!-- Has MathJax --></p><h2 id="Overfitting-Issue"><a href="#Overfitting-Issue" class="headerlink" title="Overfitting Issue"></a>Overfitting Issue</h2><p>With potentially hundreds of parameters in a deep learning neural network, the possibility of overfitting is very high. We can mitigate this issue by the following ways:</p><ul><li><strong>$L_1/L_2$ Regularization</strong><br>Add a penalty for a larger weights in the model (not unique to neural networks)</li><li><strong>Dropout</strong><br>Remove neurons during training randomly so that the network does not over rely on any particular neuron (unique to neural networks)</li><li><strong>Expanding Data</strong><br>Artificially expand data via adding noise, tilting images </li></ul><h1 id="Convolutional-Neural-Networks"><a href="#Convolutional-Neural-Networks" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h1><h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h2><p><strong>Tensor:</strong> N-dimensional arrays:<br>Scalar: 3<br>Vector: [3,4,5]<br>Matrix: [[3,4],[5,6],[7,8]]<br>Tensor: [[[1,2], [3,4]],  [[5,6], [7,8]]]<br>We use tensors to feed in sets of images into the model - (I,H,W,C)<br>I: Images<br>H: Height of Image in Pixels<br>W: Width of Image in Pixels<br>C: Color Channels: 1 - Grayscale, 3 - RGB</p><h2 id="DNN-vs-CNN"><a href="#DNN-vs-CNN" class="headerlink" title="DNN vs CNN"></a>DNN vs CNN</h2><h3 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h3><p>The left figure is <strong>densely connected layer</strong>, every neuron in the layer is directly connected to the every neuron in the next layer. While each unit in the <strong>convolutional layer</strong> is connected to a smaller number of nearby units in the next layer. </p><center><br><img class="left" src="/images/2018/01/DNNCNN.png" width="60%" height="50%"><br></center><br>The reason for the idea of CNN is that most images are at least 256 by 256 pixels or greater (MNIST only 28 by 29 pixels, 784 total). So there are too many parameters unscalable to new images. Another merit of CNN is for image processing, pixels nearby to each other are much more correlated to each other for image detection. Each CNN layer looks at an increasingly larger part of the image. And having units only connected to nearby units helps invariance. CNN helps limit the search of weights to the size of the convolution. <em>Convolutional layers are only connected to pixels in their respective fields.</em> By <strong>adding a padding of zeros</strong> around the image we can fix the issue for edge neurons where there may not be an input for them.<br>Take the example of 1-D Convolution, we treat the weights as a filter for edge detection, then expand one filters to multiple filters. Stride 1 means 1 unit at a time. The top circle means the zero padding added to include more edge pixels. Each filter detects a different feature.<br><center><br><img class="left" src="/images/2018/01/CNN12.png" width="70%" height="60%"><br></center><br>Now for simplicity, the sets of neurons are visualized as blocks.<br><center><br><img class="left" src="/images/2018/01/CNN34.png" width="80%" height="70%"><br></center><br>For 2-D Images and Color Images:<br><center><br><img class="left" src="/images/2018/01/2D.png" width="80%" height="70%"><br></center><p>More Info: <a href="http://setosa.io/ev/image-kernels/" target="_blank" rel="noopener">Image Kernals</a></p><h3 id="Subsampling"><a href="#Subsampling" class="headerlink" title="Subsampling"></a>Subsampling</h3><p>Except <strong>convolutional layers</strong>, there’s another kind of layers called <strong>pooling layers</strong>. Pooling layers will subsample the input image to reduce the memory use and computer load as well as reducing the number of parameters.<br>Take example of MNIST, only select max value to the next layer, and move over by stride. So the pooling layer will finally remove a lot of information.</p><center><br><img class="left" src="/images/2018/01/pooling.png" width="60%" height="50%"><br></center><p>Another technique is “Dropout”. Dropout is regarded as a form of regularization as during training, units are randomly dropped with their connection to help prevent overfitting. </p><center><br><img class="left" src="/images/2018/01/diagram.png" width="80%" height="70%"><br></center><h1 id="Recurrent-Neural-Networks"><a href="#Recurrent-Neural-Networks" class="headerlink" title="Recurrent Neural Networks"></a>Recurrent Neural Networks</h1><p>Common Neural Networks can handle classification and regression problems, but for sequence information, we need Recurrent Neural Networks.<br>Normal Neural Networks just aggregation of inputs and pass the activation function to get the output. Recurrent Neural Networks send output back to itself.<br>Here I have to mention my previous research work – Echo State Networks, also belong to RNN: <a href="https://qiuyiwu.github.io/ESN/"><strong>ESN</strong></a> </p><p><center><br><img class="left" src="/images/2018/01/RNN.png" width="90%" height="80%"><br></center><br>Cells that are a function of inputs from previous time steps are also know as memory cells. </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Here I’ll give the theory part of Neural Networks, sepecifically three kinds of NN: Normal Neural Networks, Convolutional Neural Networks, Recurrent Neural Networks. &lt;/p&gt;
&lt;h1 id=&quot;Neural-Networks&quot;&gt;&lt;a href=&quot;#Neural-Networks&quot; class=&quot;headerlink&quot; title=&quot;Neural Networks&quot;&gt;&lt;/a&gt;Neural Networks&lt;/h1&gt;&lt;h2 id=&quot;In-single-neuron&quot;&gt;&lt;a href=&quot;#In-single-neuron&quot; class=&quot;headerlink&quot; title=&quot;In single neuron:&quot;&gt;&lt;/a&gt;In single neuron:&lt;/h2&gt;&lt;span&gt;$$\begin{align*}
z &amp;amp;= Wx + b \\
a &amp;amp;= \sigma (z)
\end{align*}$$&lt;/span&gt;&lt;!-- Has MathJax --&gt;
&lt;h2 id=&quot;Activation-Function&quot;&gt;&lt;a href=&quot;#Activation-Function&quot; class=&quot;headerlink&quot; title=&quot;Activation Function&quot;&gt;&lt;/a&gt;Activation Function&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Perceptron:&lt;/strong&gt; binary classifier, small changes are not reflected.&lt;span&gt;$$\begin{align*}
f(x) = \begin{cases}
1 &amp;amp; \text{if } Wx+b&amp;gt;0\\ 
0 &amp;amp; \text{otherwise}
\end{cases}
\end{align*}$$&lt;/span&gt;&lt;!-- Has MathJax --&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="TensorFlow" scheme="https://qiuyiwu.github.io/categories/TensorFlow/"/>
    
    
      <category term="TensorFlow, DNN, CNN, RNN" scheme="https://qiuyiwu.github.io/tags/TensorFlow-DNN-CNN-RNN/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow for Deep Learning 1</title>
    <link href="https://qiuyiwu.github.io/2018/01/16/Tensorflow-for-Deep-Learning-1/"/>
    <id>https://qiuyiwu.github.io/2018/01/16/Tensorflow-for-Deep-Learning-1/</id>
    <published>2018-01-16T23:04:34.000Z</published>
    <updated>2018-12-21T04:54:55.082Z</updated>
    
    <content type="html"><![CDATA[<p>I’m learning how to use Google’s TensorFlow framework to create artificial neural networks for deep learning with Python from Udemy. I’m using Jupiter notebook to practice and blog my learning progress here.</p><p>TensorFlow is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. This architecture allows users to deploy computation to one or more CPUs or GPUs, in a desktop, server, or mobile device with a single API (Application programming interface).</p><a id="more"></a><h1 id="Install-TensorFlow-Environment"><a href="#Install-TensorFlow-Environment" class="headerlink" title="Install TensorFlow Environment"></a>Install TensorFlow Environment</h1><p><a href="https://www.anaconda.com/download/#macos" target="_blank" rel="noopener">Download Anaconda Distribution</a> for Python 3.6 Version. Tutorial comes in that page.<br>For MacOS: Open Terminal, use <code>cd</code> to the target directory<br>Create the environment file: run <code>conda env create -f tfdl_env.yml</code><br>Activate the file: run <code>source activate tfdeeplearning</code><br>Now you are in the virtual environment of tfdeeplearning<br>If you want to get out of this, run <code>source deactivate</code></p><p><strong>Note:</strong> <a href="https://stackoverflow.com/questions/42096280/how-is-anaconda-related-to-python#comment71363474_42096429" target="_blank" rel="noopener">How is Anaconda related to Python?</a><br>Anaconda is a python and R distribution. It aims to provide everything you need (python wise) for data science “out of the box”.<br>It includes:</p><ul><li>The core python language</li><li>100+ python “packages” (libraries)</li><li>Spyder (IDE/editor - like pycharm) and Jupyter</li><li><code>conda</code>, Anaconda’s own package manager, used for updating Anaconda and packages</li></ul><p>Also Anaconda is used majorly for the data science. which manipulates large datasets based on statistical methods. ie. Many statistical packages are already available in anaconda libraries(packages) </p><p>Vanilla python installed from python.org comew with <a href="https://docs.python.org/3/library/" target="_blank" rel="noopener">standard library</a> is okay, in which case using <a href="https://pypi.python.org/pypi/pip" target="_blank" rel="noopener">pip</a> to install manually (which comes with most python dists and you should have it if you downloaded from python.org).<br>Learn more: <a href="https://www.continuum.io/anaconda-overview" target="_blank" rel="noopener">Anaconda overview</a>; <a href="https://docs.python.org/3/tutorial/" target="_blank" rel="noopener">Python 3 tutorial</a></p><h1 id="TensorFlow-Basic-Syntax"><a href="#TensorFlow-Basic-Syntax" class="headerlink" title="TensorFlow Basic Syntax"></a>TensorFlow Basic Syntax</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">print(tf.__version__)</span><br></pre></td></tr></table></figure><p>Output: <code>1.3.0</code></p><p><strong>Create a tensor ($\approx$ n-dimension array), the basic one (constant)</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hello = tf.constant(<span class="string">'Hello'</span>)</span><br><span class="line">world = tf.constant(<span class="string">'World'</span>)</span><br><span class="line">type (hello)</span><br></pre></td></tr></table></figure></p><p>Output: <code>tensorflow.python.framework.ops.Tensor</code></p><p><strong>Run this operation inside of a session:</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    result = sess.run(hello + world)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure></p><p>Output: <code>b&#39;HelloWorld&#39;</code><br>Note:<br>Use <code>with</code> is to make sure we don’t close the session until we run a block of code then close the session.<br>The <code>b</code> character prefix signifies that <code>HelloWorld</code> is a <a href="https://stackoverflow.com/questions/6224052/what-is-the-difference-between-a-string-and-a-byte-string" target="_blank" rel="noopener">byte string</a>, use <code>result.decode(&#39;utf-8&#39;)</code> to convert byte str to str.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant(<span class="number">10</span>)</span><br><span class="line">b = tf.constant(<span class="number">20</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    c = sess.run( a + b )</span><br><span class="line">print(c)</span><br></pre></td></tr></table></figure><p>Output: <code>30</code></p><p><strong>Numpy Operations:</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">const = tf.constant(<span class="number">10</span>)        <span class="comment"># constant operation</span></span><br><span class="line">fill_mat = tf.fill((<span class="number">4</span>,<span class="number">4</span>),<span class="number">10</span>)   <span class="comment"># matrix operation 4x4</span></span><br><span class="line">myzeros = tf.zeros((<span class="number">4</span>,<span class="number">4</span>))      <span class="comment"># 4x4 zeors</span></span><br><span class="line">myones = tf.ones((<span class="number">4</span>,<span class="number">4</span>))        <span class="comment"># 4x4 ones</span></span><br><span class="line">myrandn = tf.random_normal((<span class="number">4</span>,<span class="number">4</span>), mean = <span class="number">0</span>, stddev = <span class="number">1.0</span>)         <span class="comment"># random normal distribution </span></span><br><span class="line">myrandu = tf.random_uniform((<span class="number">4</span>,<span class="number">4</span>), minval = <span class="number">0</span>, maxval = <span class="number">1.0</span>)      <span class="comment"># random uniform distribution </span></span><br><span class="line"></span><br><span class="line">my_ops = [const, fill_mat, myzeros, myones, myrandn, myrandu]</span><br><span class="line"></span><br><span class="line">sess = tf.InteractiveSession()      <span class="comment"># Interactive Session</span></span><br><span class="line"><span class="keyword">for</span> op <span class="keyword">in</span> my_ops:</span><br><span class="line">    print(sess.run(op),<span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure></p><p>Note:<br><code>Interactive Session</code> is particularly useful for Jupyter Notebook, it allows you to constantly call it through multiple cells.<br>In the Interactive Session, instead of using <code>sess.run(op)</code>, we can use <code>op.eval()</code>, it means “evaluate this operation”, and generates the same result.</p><p><strong>Matrix multiplication (common in neural networks)</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">10</span>],[<span class="number">100</span>]])</span><br><span class="line">result = tf.matmul(a,b)</span><br><span class="line">sess.run(result)</span><br></pre></td></tr></table></figure></p><span>$$\begin{align*}    \begin{bmatrix}        1 &amp; 2 \\        3 &amp; 4    \end{bmatrix} \times \begin{bmatrix}      10 \\ 100    \end{bmatrix} = \begin{bmatrix}      210 \\ 430    \end{bmatrix}\end{align*}$$</span><!-- Has MathJax --><p>Note: <code>sess.run(result)</code> can also use <code>result.eval()</code> instead.</p><h1 id="TensorFlow-Graphs"><a href="#TensorFlow-Graphs" class="headerlink" title="TensorFlow Graphs"></a>TensorFlow Graphs</h1><p>Graphs are sets of connected <em>nodes</em>(vertices), and the connections are called <em>edges</em>. In TensorFlow each node is an operation with possible inputs that can supply some outputs. When TenserFlow is started, a default graph is created.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(tf.get_default_graph())</span><br><span class="line">print(tf.Graph())</span><br></pre></td></tr></table></figure><p>Output:<br>fixed default graph: <code>&lt;tensorflow.python.framework.ops.Graph object at 0x11f4c92b0&gt;</code><br>Dynamic random graph: <code>&lt;tensorflow.python.framework.ops.Graph object at 0x11f707dd8&gt;</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">graph_one = tf.Graph()</span><br><span class="line">graph_two = tf.get_default_graph()</span><br><span class="line"><span class="keyword">with</span> graph_one.as_default():</span><br><span class="line">    print(graph_one <span class="keyword">is</span> tf.get_default_graph())</span><br></pre></td></tr></table></figure><p>Output: <code>True</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(graph_one <span class="keyword">is</span> tf.get_default_graph())</span><br></pre></td></tr></table></figure></p><p>Output: <code>False</code></p><h1 id="Variables-and-Placeholders"><a href="#Variables-and-Placeholders" class="headerlink" title="Variables and Placeholders"></a>Variables and Placeholders</h1><p><em>Variables</em> and <em>Placeholders</em> are two main types of tensor objects in a Graph. During the optimization process, TensorFlow tunes the parameters of the model. <em>Variables</em> can hold the values of weights and biases throughout the session. <strong><em>Variables</em> need to be initialized</strong>. <em>Placeholders</em> are initially empty and are used to feed in the actual training examples. However they do need a declared expected data type (tf.float32) with an optiional shape argument.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### Variable ###</span></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">my_tensor = tf.random_uniform((<span class="number">4</span>,<span class="number">4</span>), <span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">my_var = tf.Variable(initial_value = my_tensor)  <span class="comment"># give value to variable </span></span><br><span class="line"><span class="comment"># sess.run(my_var) cannot be directly run here, need initialized first!</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line">sess.run(my_var) <span class="comment"># Now it's the time!</span></span><br></pre></td></tr></table></figure></p><p>Output: <code>array([[ 0.27756679,  0.82726526,  0.80544853,  0.43891859],        [ 0.56279469,  0.57444489,  0.82595968,  0.63165414],        [ 0.16034544,  0.86095798,  0.74416387,  0.17536163],        [ 0.44427669,  0.69035304,  0.55842543,  0.00723565]], dtype=float32)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### Placeholder ###</span></span><br><span class="line">ph = tf.placeholder(tf.float32, shape = (<span class="number">4</span>,<span class="number">4</span>)) </span><br><span class="line">ph = tf.placeholder(tf.float32, shape = (<span class="keyword">None</span>,<span class="number">5</span>)) <span class="comment"># can be fed in the actually number of sampes</span></span><br><span class="line">ph = tf.placeholder(tf.float32) <span class="comment"># no shape</span></span><br></pre></td></tr></table></figure><h1 id="TensorFlow-Neural-Network"><a href="#TensorFlow-Neural-Network" class="headerlink" title="TensorFlow Neural Network"></a>TensorFlow Neural Network</h1><p>Create a neuron that performs a simple linear fit to some 2-D data. Graph of $wx+b = z$</p><ol><li>Build a Graph</li><li>Initiate the Session</li><li>Feed Data In and get Output</li><li>Add in the cost function in order to train the network to optimize the parameters<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">### Build a simple Graph ###</span></span><br><span class="line">np.random.seed(<span class="number">101</span>)</span><br><span class="line">tf.set_random_seed(<span class="number">101</span>)</span><br><span class="line"></span><br><span class="line">rand_a = np.random.uniform(<span class="number">0</span>,<span class="number">100</span>, (<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">rand_b = np.random.uniform(<span class="number">0</span>,<span class="number">100</span>, (<span class="number">5</span>,<span class="number">1</span>))</span><br><span class="line">a = tf.placeholder(tf.float32) <span class="comment"># default shape</span></span><br><span class="line">b = tf.placeholder(tf.float32) <span class="comment"># default shape</span></span><br><span class="line"><span class="comment"># Two ways to do the operation: </span></span><br><span class="line"><span class="comment">#tf.add(a, b)  |  tf.multiply(a, b)  |  tf.matmul(a, b) </span></span><br><span class="line">add_op = a + b</span><br><span class="line">mul_op = a * b</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    add_result = sess.run(add_op, feed_dict = &#123; a:rand_a, b:rand_b&#125;) </span><br><span class="line">    print(add_result, <span class="string">'\n'</span>)</span><br><span class="line">    mul_result = sess.run(mul_op, feed_dict = &#123; a:rand_a, b:rand_b&#125;)</span><br><span class="line">    print(mul_result)</span><br></pre></td></tr></table></figure></li></ol><p>Output:<br>`[[ 151.07165527  156.49855042  102.27921295  116.58396149  167.95948792]<br> [ 135.45622253   82.76316071  141.42784119  124.22093201   71.06043243]<br> [ 113.30171204   93.09214783   76.06819153  136.43911743  154.42727661]<br> [  96.7172699    81.83804321  133.83674622  146.38117981  101.10578918]<br> [ 122.72680664  105.98292542   59.04463196   67.98310089   72.89292145]] </p><p>[[ 5134.64404297  5674.25         283.12432861  1705.47070312  6813.83154297]<br> [ 4341.8125      1598.26696777  4652.73388672  3756.8293457    988.9463501 ]<br> [ 3207.8112793   2038.10290527  1052.77416992  4546.98046875  5588.11572266]<br> [ 1707.37902832   614.02526855  4434.98876953  5356.77734375  2029.85546875]<br> [ 3714.09838867  2806.64379883   262.76763916   747.19854736 1013.29199219]]`</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### Build a Neural Network ### </span></span><br><span class="line">n_features = <span class="number">10</span>      <span class="comment"># input layer</span></span><br><span class="line">n_dense_neurons = <span class="number">3</span>  <span class="comment"># hidden layer</span></span><br><span class="line">x = tf.placeholder(tf.float32, (<span class="keyword">None</span>, n_features))</span><br><span class="line">W = tf.Variable(tf.random_normal([n_features, n_dense_neurons]))</span><br><span class="line">b = tf.Variable(tf.ones([n_dense_neurons]))</span><br><span class="line"></span><br><span class="line">xW = tf.matmul(x, W) </span><br><span class="line">z = tf.add(xW, b)</span><br><span class="line"><span class="comment"># a = tf.nn.relu | tf.tahn</span></span><br><span class="line">a = tf.sigmoid(z)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)        <span class="comment"># DON'T FORGET!</span></span><br><span class="line">    layer_out = sess.run(a, feed_dict = &#123; x: np.random.random([<span class="number">1</span>,n_features])&#125;)</span><br><span class="line">print(layer_out)</span><br></pre></td></tr></table></figure><p>Output: <code>[[ 0.81314439  0.98195159  0.73793817]]</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### Simple Regression Example ###</span></span><br><span class="line">x_data = np.linspace(<span class="number">0</span>,<span class="number">10</span>,<span class="number">10</span>) + np.random.uniform(<span class="number">-1.5</span>,<span class="number">1.5</span>,<span class="number">10</span>)  <span class="comment"># with noise</span></span><br><span class="line">y_label = np.linspace(<span class="number">0</span>,<span class="number">10</span>,<span class="number">10</span>) + np.random.uniform(<span class="number">-1.5</span>,<span class="number">1.5</span>,<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline     </span><br><span class="line"><span class="comment"># %matplotlib inline    just for Jupyter notebook   </span></span><br><span class="line">plt.plot(x_data, y_label, <span class="string">'*'</span>)</span><br></pre></td></tr></table></figure><p><img src="/images/2018/01/TF1.png" alt="Sample Image Added via Markdown"><br>Now I want the neural network to solve $ y = mx + b$ with 10 training steps:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create two random variables</span></span><br><span class="line">m = tf.Variable(<span class="number">0.44</span>)</span><br><span class="line">b = tf.Variable(<span class="number">0.87</span>)</span><br><span class="line"></span><br><span class="line">error = <span class="number">0</span>  <span class="comment"># create an error starting from 0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> zip(x_data, y_label):   <span class="comment"># zip makes a list of x,y</span></span><br><span class="line">    y_hat = m * x + b</span><br><span class="line">    error += (y - y_hat) ** <span class="number">2</span>       <span class="comment"># cost function</span></span><br><span class="line"></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate = <span class="number">0.001</span>)</span><br><span class="line">train = optimizer.minimize(error)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    training_steps = <span class="number">10</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(training_steps):</span><br><span class="line">        sess.run(train)</span><br><span class="line">    final_slope, final_intercept = sess.run([m,b])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test </span></span><br><span class="line">x_test = np.linspace(<span class="number">-1</span>,<span class="number">11</span>,<span class="number">10</span>)</span><br><span class="line">y_pred = final_slope * x_test + final_intercept </span><br><span class="line"></span><br><span class="line">plt.plot(x_test, y_pred, <span class="string">'r'</span>)</span><br><span class="line">plt.plot(x_test, y_label,<span class="string">'*'</span>)</span><br></pre></td></tr></table></figure></p><p><img src="/images/2018/01/TF2.png" alt="Sample Image Added via Markdown"></p><h1 id="TensorFlow-Regression"><a href="#TensorFlow-Regression" class="headerlink" title="TensorFlow Regression"></a>TensorFlow Regression</h1><p>Regression example with huge dataset:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">x_data = np.linspace(<span class="number">0.0</span>,<span class="number">10.0</span>,<span class="number">1000000</span>)  <span class="comment"># huge dataset</span></span><br><span class="line">noise = np.random.randn(len(x_data))    <span class="comment"># add noise</span></span><br><span class="line"><span class="comment"># y = mx + b | b = 5, m = 0.5</span></span><br><span class="line">y_true = (<span class="number">0.5</span> * x_data) + <span class="number">5</span> + noise</span><br><span class="line"></span><br><span class="line">x_df = pd.DataFrame(data = x_data, columns = [<span class="string">'X Data'</span>])</span><br><span class="line">y_df = pd.DataFrame(data = y_true, columns = [<span class="string">'Y'</span>])</span><br><span class="line">my_data = pd.concat([x_df, y_df], axis = <span class="number">1</span>)  <span class="comment"># axis = 1 column concatenate</span></span><br><span class="line">my_data.sample(n = <span class="number">25</span>) <span class="comment"># random 25 samples</span></span><br><span class="line">my_data.sample(n=<span class="number">250</span>).plot(kind = <span class="string">'scatter'</span>, x = <span class="string">'X Data'</span>, y = <span class="string">'Y'</span>)</span><br></pre></td></tr></table></figure></p><p><img src="/images/2018/01/TF3.png" alt="Sample Image Added via Markdown"><br>For real cases, usually the datasets are humongous and we do not use all of them at once. Instead, we use batch method to feed the data batch by batch into the network to save the time. The number of batches depends on the size of the data. Here we feed 1000 batches of data, with each batch has 8 corresponding data points (x data points with corresponding y labels). To make batches useful, we grab 8 random data points via <code>rand_ind = np.random.randint(len(x_data), size = batch_size)</code>, then train the optimizer.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">8</span></span><br><span class="line">m = tf.Variable(<span class="number">0.81</span>)</span><br><span class="line">b = tf.Variable(<span class="number">0.17</span>)</span><br><span class="line"></span><br><span class="line">xph = tf.placeholder(tf.float32,[batch_size])</span><br><span class="line">yph = tf.placeholder(tf.float32,[batch_size])</span><br><span class="line"></span><br><span class="line">y_model = m * xph + b    <span class="comment"># the Graph I want to compute</span></span><br><span class="line"><span class="comment"># Loss function</span></span><br><span class="line">error = tf.reduce_sum(tf.square(yph - y_model)) <span class="comment"># tf.reduce_sum: Computes the sum of elements across dimensions of a tensor.</span></span><br><span class="line"><span class="comment"># Optimizer</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate = <span class="number">0.001</span>)     <span class="comment"># This is gradient decent optimizer, there are many other optimizers</span></span><br><span class="line">train = optimizer.minimize(error)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    batches = <span class="number">1000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(batches):</span><br><span class="line">        rand_ind = np.random.randint(len(x_data), size = batch_size)    <span class="comment"># random index</span></span><br><span class="line">        feed = &#123;xph: x_data[rand_ind], yph: y_true[rand_ind]&#125;</span><br><span class="line">        sess.run(train, feed_dict = feed)</span><br><span class="line">    model_m, model_b = sess.run([m, b])</span><br><span class="line"></span><br><span class="line">y_hat = x_data * model_m + model_b</span><br><span class="line">my_data.sample(<span class="number">250</span>).plot(kind = <span class="string">'scatter'</span>, x = <span class="string">'X Data'</span>, y = <span class="string">'Y'</span>)</span><br><span class="line">plt.plot(x_data, y_hat, <span class="string">'r'</span>)</span><br></pre></td></tr></table></figure></p><p><img src="/images/2018/01/TF4.png" alt="Sample Image Added via Markdown"></p><h1 id="TensorFlow-Estimator-API"><a href="#TensorFlow-Estimator-API" class="headerlink" title="TensorFlow Estimator API"></a>TensorFlow Estimator API</h1><p>This section is to solve the regression task with TensorFlow estimator API. Wait a minute, what is API?  Technically, API stands for <em>Application Programming Interface</em>, but still, what is that? Basically, it is part of the server, that can be distinctively separated from its environment, that receives requests and sends responses. Here is an excellent article from Petr Gazarov explaining <a href="https://medium.freecodecamp.org/what-is-an-api-in-english-please-b880a3214a82" target="_blank" rel="noopener"><strong>API</strong></a>.<br>There are many other higher levels of APIs (Keras, Layers…). The tf.estimator API has several model types to choose from:</p><ul><li>tf.estimator.LinearClassifier: Constructs a linear classification model</li><li>tf.estimator.LinearRegressor: Constructs a linear regression model</li><li>tf.estimator.DNNClassifier: Constructs a neural network classification model</li><li>tf.estimator.DNNRegressor: Constructs a neural network regression model</li><li>tf.estimator.DNNLinearCombinedRegressor: Constructs a neural network and linear combined regression model</li></ul><p><br><br>Steps for using the Estimator API:</p><ul><li>Define a list of feature columns</li><li>Create the Estimator Model</li><li>Create a Data Input Function</li><li>Call train, evaluate, and predict methods on the estimator object</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">feat_cols = [tf.feature_column.numeric_column(<span class="string">'x'</span>, shape = <span class="number">1</span>)]  <span class="comment"># set all feature columns</span></span><br><span class="line">estimator = tf.estimator.LinearRegressor(feature_columns = feat_cols)  <span class="comment"># set estimator</span></span><br><span class="line"><span class="comment"># Split the data</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x_data, y_true, test_size = <span class="number">0.3</span>, random_state = <span class="number">101</span>)</span><br><span class="line"><span class="comment"># Set up Estimator Inputs</span></span><br><span class="line">input_func = tf.estimator.inputs.numpy_input_fn(&#123;<span class="string">'x'</span>: x_train&#125;, y_train, batch_size = <span class="number">8</span>, num_epochs = <span class="keyword">None</span>, shuffle = <span class="keyword">True</span>)     <span class="comment"># we can also use "tf.estimator.inputs.pandas_input_fn"</span></span><br><span class="line">train_input_func = tf.estimator.inputs.numpy_input_fn(&#123;<span class="string">'x'</span>: x_train&#125;, y_train, batch_size = <span class="number">8</span>, num_epochs = <span class="number">1000</span>, shuffle = <span class="keyword">False</span>)     <span class="comment"># change num_epochs and shuffle</span></span><br><span class="line">test_input_func = tf.estimator.inputs.numpy_input_fn(&#123;<span class="string">'x'</span>: x_test&#125;, y_test, batch_size = <span class="number">8</span>, num_epochs = <span class="number">1000</span>, shuffle = <span class="keyword">False</span>)     <span class="comment"># change num_epochs and shuffle, and the dataset</span></span><br><span class="line"><span class="comment"># Train the estimator</span></span><br><span class="line">estimator.train(input_fn = input_func, steps = <span class="number">1000</span>)</span><br><span class="line"><span class="comment"># Evaluation</span></span><br><span class="line">train_matrics = estimator.evaluate(input_fn = train_input_func, steps = <span class="number">1000</span>)</span><br><span class="line">test_matrics = estimator.evaluate(input_fn = test_input_func, steps = <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Training data matrics'</span>)</span><br><span class="line">print(train_matrics)</span><br><span class="line">print(<span class="string">'Test data matrics'</span>)</span><br><span class="line">print(test_matrics)</span><br></pre></td></tr></table></figure><p>Output:<br><code>Training data matrics{&#39;loss&#39;: 8.7310658, &#39;average_loss&#39;: 1.0913832, &#39;global_step&#39;: 1000}Test data matrics{&#39;loss&#39;: 8.6690454, &#39;average_loss&#39;: 1.0836307, &#39;global_step&#39;: 1000}</code><br>This is a good way to check if the model is overfitting (very low loss on training data but very high loss on test data). We want the loss of training data and test data are very close to each other.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get predict value</span></span><br><span class="line">brand_new_data = np.linspace(<span class="number">0</span>,<span class="number">10</span>,<span class="number">10</span>)</span><br><span class="line">input_fn_predict = tf.estimator.inputs.numpy_input_fn(&#123;<span class="string">'x'</span>: brand_new_data&#125;, shuffle = <span class="keyword">False</span>)</span><br><span class="line">estimator.predict(input_fn = input_fn_predict)</span><br><span class="line">list(estimator.predict(input_fn = input_fn_predict))</span><br></pre></td></tr></table></figure></p><p>Output:<br><code>[{&#39;predictions&#39;: array([ 4.43396044], dtype=float32)}, {&#39;predictions&#39;: array([ 5.06833887], dtype=float32)}, {&#39;predictions&#39;: array([ 5.7027173], dtype=float32)}, {&#39;predictions&#39;: array([ 6.33709526], dtype=float32)}, {&#39;predictions&#39;: array([ 6.97147369], dtype=float32)}, {&#39;predictions&#39;: array([ 7.60585213], dtype=float32)}, {&#39;predictions&#39;: array([ 8.24023056], dtype=float32)}, {&#39;predictions&#39;: array([ 8.87460899], dtype=float32)}, {&#39;predictions&#39;: array([ 9.50898743], dtype=float32)}, {&#39;predictions&#39;: array([ 10.14336586], dtype=float32)}]</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plot the prediction</span></span><br><span class="line">predictions = []</span><br><span class="line"><span class="keyword">for</span> pred <span class="keyword">in</span> estimator.predict(input_fn = input_fn_predict):</span><br><span class="line">    predictions.append(pred[<span class="string">'predictions'</span>])</span><br><span class="line">my_data.sample(<span class="number">250</span>).plot(kind = <span class="string">'scatter'</span>, x = <span class="string">'X Data'</span>, y = <span class="string">'Y'</span>)</span><br><span class="line">plt.plot(brand_new_data, predictions, <span class="string">'r'</span>)</span><br></pre></td></tr></table></figure></p><p><img src="/images/2018/01/TF5.png" alt="Sample Image Added via Markdown"></p><h1 id="TensorFlow-Classification"><a href="#TensorFlow-Classification" class="headerlink" title="TensorFlow Classification"></a>TensorFlow Classification</h1><p>Use real dataset “Pima Indians Diabetes Dataset”, including both categorical and continuous features, to use tf.estimator switching models – from linear classifier to dense neural network classifier. </p><h2 id="Linear-Classifier"><a href="#Linear-Classifier" class="headerlink" title="Linear Classifier"></a>Linear Classifier</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">diabetes = pd.read_csv(<span class="string">'pima-indians-diabetes.csv'</span>)</span><br><span class="line">diabetes.head()</span><br></pre></td></tr></table></figure><p><img src="/images/2018/01/TF6.png" alt="Sample Image Added via Markdown"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">diabetes.columns</span><br></pre></td></tr></table></figure></p><p>Output: <code>Index([&#39;Number_pregnant&#39;, &#39;Glucose_concentration&#39;, &#39;Blood_pressure&#39;, &#39;Triceps&#39;,       &#39;Insulin&#39;, &#39;BMI&#39;, &#39;Pedigree&#39;, &#39;Age&#39;, &#39;Class&#39;, &#39;Group&#39;],      dtype=&#39;object&#39;)</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># normalize the data except the catagorical part (Age, Class, Group)</span></span><br><span class="line">cols_to_norm = [<span class="string">'Number_pregnant'</span>, <span class="string">'Glucose_concentration'</span>, <span class="string">'Blood_pressure'</span>, <span class="string">'Triceps'</span>,</span><br><span class="line">       <span class="string">'Insulin'</span>, <span class="string">'BMI'</span>, <span class="string">'Pedigree'</span>]</span><br><span class="line">diabetes[cols_to_norm] = diabetes[cols_to_norm].apply(<span class="keyword">lambda</span> x: (x - x.min())/(x.max() - x.min()))</span><br><span class="line">diabetes.head()</span><br></pre></td></tr></table></figure></p><p><img src="/images/2018/01/TF7.png" alt="Sample Image Added via Markdown"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create feature columns</span></span><br><span class="line"><span class="comment"># continuous value</span></span><br><span class="line">num_preg = tf.feature_column.numeric_column(<span class="string">'Number_pregnant'</span>)</span><br><span class="line">plasma_gluc = tf.feature_column.numeric_column(<span class="string">'Glucose_concentration'</span>)</span><br><span class="line">dias_press = tf.feature_column.numeric_column(<span class="string">'Blood_pressure'</span>)</span><br><span class="line">tricep = tf.feature_column.numeric_column(<span class="string">'Triceps'</span>)</span><br><span class="line">insulin = tf.feature_column.numeric_column(<span class="string">'Insulin'</span>)</span><br><span class="line">bmi = tf.feature_column.numeric_column(<span class="string">'BMI'</span>)</span><br><span class="line">diabetes_pedigree = tf.feature_column.numeric_column(<span class="string">'Pedigree'</span>)</span><br><span class="line">age = tf.feature_column.numeric_column(<span class="string">'Age'</span>)</span><br><span class="line"><span class="comment"># catagorical value</span></span><br><span class="line">assigned_group = tf.feature_column.categorical_column_with_vocabulary_list(<span class="string">'Group'</span>, [<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'C'</span>,<span class="string">'D'</span>])     </span><br><span class="line"><span class="comment"># assigned_group = tf.feature_column.categorical_column_with_hash_bucket('Group', hash_bucket_size = 10)  </span></span><br><span class="line"><span class="comment"># For many groups scenario we can use hash_bucket, as long as we set the hash_bucket_size &gt; # of groups</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line">diabetes[<span class="string">'Age'</span>].hist(bins = <span class="number">20</span>)</span><br></pre></td></tr></table></figure></p><p><img src="/images/2018/01/TF8.png" alt="Sample Image Added via Markdown"><br>The distribution of Age from the histgram shows that most of the people are 20s. And instead of treating this variable as a continuous variable, we can bucket these values together, making boundary for each decade or so. <strong>Basically we use bucket system to transform continuous variable into categorial variable.</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">age_bucket = tf.feature_column.bucketized_column(age, boundaries = [<span class="number">20</span>,<span class="number">30</span>,<span class="number">40</span>,<span class="number">50</span>,<span class="number">60</span>,<span class="number">70</span>,<span class="number">80</span>])</span><br><span class="line"><span class="comment"># feature columns</span></span><br><span class="line">feat_cols = [num_preg ,plasma_gluc,dias_press ,tricep ,insulin,bmi,diabetes_pedigree ,assigned_group, age_bucket]</span><br><span class="line"><span class="comment"># train test split</span></span><br><span class="line">x_data = diabetes.drop(<span class="string">'Class'</span>, axis = <span class="number">1</span>)   <span class="comment"># Class column is Y</span></span><br><span class="line">labels = diabetes[<span class="string">'Class'</span>]</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(x_data, labels, test_size = <span class="number">0.3</span>, random_state = <span class="number">101</span>)</span><br></pre></td></tr></table></figure></p><p>Create the model!<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Create the model</span></span><br><span class="line">input_func = tf.estimator.inputs.pandas_input_fn(x = X_train, y = y_train, batch_size = <span class="number">10</span>, num_epochs = <span class="number">1000</span>, shuffle = <span class="keyword">True</span>)</span><br><span class="line">model = tf.estimator.LinearClassifier(feature_columns = feat_cols, n_classes = <span class="number">2</span>)</span><br><span class="line">model.train(input_fn = input_func, steps = <span class="number">1000</span>)</span><br><span class="line"><span class="comment">## Evaluate the model</span></span><br><span class="line">eval_input_func = tf.estimator.inputs.pandas_input_fn(x = X_test, y = y_test, batch_size = <span class="number">10</span>, num_epochs = <span class="number">1</span>, shuffle = <span class="keyword">False</span>)</span><br><span class="line">results = model.evaluate(eval_input_func)</span><br><span class="line">results</span><br></pre></td></tr></table></figure></p><p>Output:<br><code>{&#39;accuracy&#39;: 0.73160172, &#39;accuracy_baseline&#39;: 0.64935064, &#39;auc&#39;: 0.79658431, &#39;auc_precision_recall&#39;: 0.6441071, &#39;average_loss&#39;: 0.52852213, &#39;global_step&#39;: 1000, &#39;label/mean&#39;: 0.35064936, &#39;loss&#39;: 5.0870256, &#39;prediction/mean&#39;: 0.35784912}</code><br>The accuracy is 73.16%.</p><p> Prediction!<br> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># No y value in prediction, put new data set in x</span></span><br><span class="line">pred_input_func = tf.estimator.inputs.pandas_input_fn(x = X_test, batch_size = <span class="number">10</span>, num_epochs = <span class="number">1</span>, shuffle = <span class="keyword">False</span>)</span><br><span class="line">predictions = model.predict(pred_input_func)</span><br><span class="line">my_pred = list(predictions)</span><br><span class="line">my_pred</span><br></pre></td></tr></table></figure></p><p><img src="/images/2018/01/TF9.png" alt="Sample Image Added via Markdown"></p><h2 id="Dense-Neural-Network-Classifier"><a href="#Dense-Neural-Network-Classifier" class="headerlink" title="Dense Neural Network Classifier"></a>Dense Neural Network Classifier</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a three-layer neural network, with 10 neurons in each layer </span></span><br><span class="line">dnn_model = tf.estimator.DNNClassifier(hidden_units = [<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>], feature_columns = feat_cols, n_classes = <span class="number">2</span>)</span><br><span class="line"><span class="comment">### If use the same of the previous step error will show up -- because of the categorial columns, so we need to use embeded group </span></span><br><span class="line">embedded_group_col = tf.feature_column.embedding_column(assigned_group, dimension = <span class="number">4</span>)</span><br><span class="line"><span class="comment">### Reset the feature columns</span></span><br><span class="line">feat_cols = [num_preg ,plasma_gluc,dias_press ,tricep ,insulin,bmi,diabetes_pedigree , embedded_group_col, age_bucket]</span><br><span class="line"><span class="comment">### Now do as the previous -- train the model</span></span><br><span class="line">input_func = tf.estimator.input.pandas_inputs_fn(X_train, y_train, batch_size = <span class="number">10</span>, num_epochs = <span class="number">1000</span>, shuffle = <span class="keyword">True</span>)</span><br><span class="line">dnn_model = tf.estimator.DNNClassifier(hidden_units = [<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>], feature_columns = feat_cols, n_classes = <span class="number">2</span>)</span><br><span class="line">dnn_model.train(input_fn = input_func, steps = <span class="number">1000</span>)</span><br><span class="line"><span class="comment"># Evaluate the model</span></span><br><span class="line">eval_input_func = tf.estimator.inputs.pandas_input_fn(x = X_test, y = y_test, batch_size = <span class="number">10</span>, num_epochs = <span class="number">1</span>, shuffle = <span class="keyword">False</span>)</span><br><span class="line">dnn_model.evaluate(eval_input_func)</span><br></pre></td></tr></table></figure><p>Output:<br><code>{&#39;accuracy&#39;: 0.75757575, &#39;accuracy_baseline&#39;: 0.64935064, &#39;auc&#39;: 0.82814813, &#39;auc_precision_recall&#39;: 0.67539084, &#39;average_loss&#39;: 0.49014509, &#39;global_step&#39;: 1000, &#39;label/mean&#39;: 0.35064936, &#39;loss&#39;: 4.7176466, &#39;prediction/mean&#39;: 0.36287409}</code></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I’m learning how to use Google’s TensorFlow framework to create artificial neural networks for deep learning with Python from Udemy. I’m using Jupiter notebook to practice and blog my learning progress here.&lt;/p&gt;
&lt;p&gt;TensorFlow is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. This architecture allows users to deploy computation to one or more CPUs or GPUs, in a desktop, server, or mobile device with a single API (Application programming interface).&lt;/p&gt;
    
    </summary>
    
      <category term="TensorFlow" scheme="https://qiuyiwu.github.io/categories/TensorFlow/"/>
    
    
      <category term="TensorFlow, Deep Learning, Python" scheme="https://qiuyiwu.github.io/tags/TensorFlow-Deep-Learning-Python/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode in Python3 - Two Sum</title>
    <link href="https://qiuyiwu.github.io/2018/01/09/LeetCode1/"/>
    <id>https://qiuyiwu.github.io/2018/01/09/LeetCode1/</id>
    <published>2018-01-09T21:48:51.000Z</published>
    <updated>2018-12-19T19:47:00.598Z</updated>
    
    <content type="html"><![CDATA[<p>As a graduate student in Statistics, I profoundly believe the power of programming skills to Statistics/ Data Science. I used R or Matlab as my default programming tools and they are pretty powerful to help me solve most mathematical related tasks. However I realize in Artificial Intelligence/ Machine Learning realm, Python is more prevelent. In order to better gain the gist of this mighty programming language, in the process of learning it, I will show my LeetCode practice in Python3 with relative explanations as well as pitfalls I encounter.<br><a id="more"></a></p><h3 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h3><p>Given an array of integers, return indices of the two numbers such that they add up to a specific target. You may assume that each input would have exactly one solution, and you may not use the same element twice.</p><h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><p>Given nums = [2, 7, 11, 15], target = 9,<br>Because nums[0] + nums[1] = 2 + 7 = 9,<br>return [0, 1].</p><h3 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">twoSum</span><span class="params">(self, nums, target)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)):</span><br><span class="line">            <span class="keyword">if</span> (target - nums[i])  <span class="keyword">in</span> nums[i+<span class="number">1</span>:]:</span><br><span class="line">                <span class="keyword">return</span> (i, nums.index(target - nums[i], i+<span class="number">1</span>) )</span><br></pre></td></tr></table></figure><h3 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h3><ul><li><p>It is easily to ignore the condition “not use the same element twice” in the question. Initially I used<br><span style="color:blue"> <strong>return (i, nums.index(target - nums[i] )</strong></span> instead of<br><span style="color:blue"> <strong>return (i, nums.index(target - nums[i], i+1 )</strong></span>,<br>it works for most cases, but for example such as <strong>Given nums = [3,3], target = 6</strong>, it failed to give me [0,1], it returned [0,0]. So basically <strong>.index(A,B)</strong> is to find the position of A after Bth index.</p></li><li><p>Another point worthy of mentioning is <span style="color:red"> <strong>self</strong></span> in the definition part <strong>def twoSum( <span style="color:red"> self</span>, nums, target)</strong>.  <span style="color:red"> <strong>self</strong></span> is used to refer class in python. In this case the class is  <span style="color:blue"> <strong>Solution</strong></span> </p></li></ul><h3 id="Difficulty"><a href="#Difficulty" class="headerlink" title="Difficulty"></a>Difficulty</h3><p>Easy</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;As a graduate student in Statistics, I profoundly believe the power of programming skills to Statistics/ Data Science. I used R or Matlab as my default programming tools and they are pretty powerful to help me solve most mathematical related tasks. However I realize in Artificial Intelligence/ Machine Learning realm, Python is more prevelent. In order to better gain the gist of this mighty programming language, in the process of learning it, I will show my LeetCode practice in Python3 with relative explanations as well as pitfalls I encounter.&lt;br&gt;
    
    </summary>
    
      <category term="LeetCode" scheme="https://qiuyiwu.github.io/categories/LeetCode/"/>
    
    
      <category term="TwoSum" scheme="https://qiuyiwu.github.io/tags/TwoSum/"/>
    
  </entry>
  
  <entry>
    <title>Data Mining Note 4 - Discriminant Analysis and Naive Bayes</title>
    <link href="https://qiuyiwu.github.io/2018/01/07/Data-Mining-Note4/"/>
    <id>https://qiuyiwu.github.io/2018/01/07/Data-Mining-Note4/</id>
    <published>2018-01-07T19:11:53.000Z</published>
    <updated>2018-12-19T19:47:00.595Z</updated>
    
    <content type="html"><![CDATA[<p>The intuitive motivation of discriminant analysis is to estimate a function based on a given dataset. However, unless a funtion is derived as a result of optimizing the expected loss of interest, more has to be done to find out if that function is good with respect to the standards set by the loss funtion. Ideally, one would like to build funtions as solutions to the criterion of optimality. Unfortunately, that can be hard in both classification (and even regression). Hence it is common for researchers to come up with a decent (reasonable) function, and then compute the estimated average loss to find out if what the discovered is a pearl.</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The intuitive motivation of discriminant analysis is to estimate a function based on a given dataset. However, unless a funtion is derived as a result of optimizing the expected loss of interest, more has to be done to find out if that function is good with respect to the standards set by the loss funtion. Ideally, one would like to build funtions as solutions to the criterion of optimality. Unfortunately, that can be hard in both classification (and even regression). Hence it is common for researchers to come up with a decent (reasonable) function, and then compute the estimated average loss to find out if what the discovered is a pearl.&lt;/p&gt;
    
    </summary>
    
      <category term="Data Mining" scheme="https://qiuyiwu.github.io/categories/Data-Mining/"/>
    
    
      <category term="LDA, QDA, Navie Bayes" scheme="https://qiuyiwu.github.io/tags/LDA-QDA-Navie-Bayes/"/>
    
  </entry>
  
  <entry>
    <title>Data Mining Note 3 - K Nearest Neighbors</title>
    <link href="https://qiuyiwu.github.io/2018/01/06/Data-Mining-Note3/"/>
    <id>https://qiuyiwu.github.io/2018/01/06/Data-Mining-Note3/</id>
    <published>2018-01-06T16:37:58.000Z</published>
    <updated>2018-12-21T04:50:53.347Z</updated>
    
    <content type="html"><![CDATA[<p>This chapter talks about k Nearest Neighbors (kNN) as a metholodogy for both classification and regression problems. The kNN method serves a basic and easy to understand foundational machine learning and data mining technique. The kNN method is an excellent baseline machine learning technique, and also allows many extensions. It usually performs reasonable well or sometimes very well when compared to more sophisticated techniques.</p><p>kNN classification basically means the estimated class of a vector $\mathbf{x}$ is the most frequent class label in the neighborhood of $\mathbf{x}$. kNN classifiers are inherently naturally multi-class, and are used extensively in applications such as image processing, character recognition and general pattern recognition tasks. For kNN regression, the estimated response value of a vector $\mathbf{x}$ is the average of the response values in the neighborhood of $\mathbf{x}$.<br><a id="more"></a></p><p><strong>Principle for kNN Classification</strong>: The reasonable class/category for a given object is the most prevalent class among its nearest neighbors.<br><strong>Principle k-Nearest Neighbor Regression</strong>: The reasonable prediction of the response value for a given object is the average of the response values of its nearest neighbors.</p><p><strong>General Steps for kNN</strong><br>Comparison of Classification and Regression</p><ol><li>Choose a distance for measuring how far a given point is from another</li><li>Set the <strong>size</strong> of the neighborhood k</li><li>Compute the distance from each existing point to the new point</li><li>Identify the class labels of the k points closest/nearest to the new point (classification)<br>Identify the response values of the k points closest/nearest to the new point (regression)</li><li>Assign the most frequent label to the new point (classification)<br>Compute the average of the response values of those k neighbors as the best estimate of the new point (regression)</li></ol><p><br></p><h3 id="Distance"><a href="#Distance" class="headerlink" title="Distance"></a>Distance</h3><p>First introduce some most commonly used distance below, which would be applied later in kNN.</p><ul><li><p>Euclidean distance: also known as the $l_2$ distance</p><span>$$\begin{align*}d(\mathbf{x}_i,\mathbf{x}_j)  = \sqrt{\sum_{l=1}^q(x_{il}-x_{jl})^2} = ||\mathbf{x}_i-\mathbf{x}_j||_2 \end{align*}$$</span><!-- Has MathJax --></li><li><p>Manhattan distance (city block): also known as $l_2$ distance</p><span>$$\begin{align*}d(\mathbf{x}_i,\mathbf{x}_j)  = \sum_{l=1}^q|x_{il}-x_{jl}| = ||\mathbf{x}_i-\mathbf{x}_j||_1 \end{align*}$$</span><!-- Has MathJax --></li><li><p>Maximum distance: also known as the infinity distance</p><span>$$\begin{align*}d(\mathbf{x}_i,\mathbf{x}_j)  = \underset{l=1,...,q}{\text{max}}|x_{il}-x_{jl}| = ||\mathbf{x}_i-\mathbf{x}_j||_\infty\end{align*}$$</span><!-- Has MathJax --></li><li><p>Minkowski distance: also known as $l_p$ distance</p><span>$$\begin{align*}d(\mathbf{x}_i,\mathbf{x}_j)  = \Big\{\sum_{l=1}^q|x_{il}-x_{jl}| \Big\}^{1/p}\end{align*}$$</span><!-- Has MathJax --></li><li><p>Canberra distance:</p><span>$$\begin{align*}d(\mathbf{x}_i,\mathbf{x}_j)  =\sum_{l=1}^q \frac{ |x_{il}-x_{jl}| }{ |x_{il}+x_{jl}| }  \end{align*}$$</span><!-- Has MathJax --></li><li><p>Jaccard/Tanimoto distance: For binary vectors ie $\mathbf{x}_i\in {0,1}^q$</p><span>$$\begin{align*}d(\mathbf{x}_i,\mathbf{x}_j)  =1- \frac{ \mathbf{x}_i\cdot \mathbf{x}_j }{ |\mathbf{x}_i|^2 + |\mathbf{x}_j|^2  - \mathbf{x}_i\cdot \mathbf{x}_j}  \end{align*}$$</span><!-- Has MathJax --></li></ul><p><br></p><h3 id="kNN-Classification"><a href="#kNN-Classification" class="headerlink" title="kNN Classification"></a>kNN Classification</h3><h4 id="Detailed-Steps-for-kNN-Classification"><a href="#Detailed-Steps-for-kNN-Classification" class="headerlink" title="Detailed Steps for kNN Classification"></a>Detailed Steps for kNN Classification</h4><p>$\mathcal{D} = \Big\{(x_1, Y_1),…, (x_n, Y_n) \Big\}$, with $x_i\in \mathcal{X}^q, Y_i\in \{1,…,g\}$</p><ol><li>Choose the value of k and the distance to be used</li><li>Let $\mathbf{x}^*$ be a new point. Compute  $d(\mathbf{x}^*,\mathbf{x}_i)  \quad i=1,2,…n $</li><li>Rank all the distances $d^∗_i$ in increasing order: <span>$$\begin{align*} d^&lowast;_{(1)} \leq  d^&lowast;_{(2)} \leq ...\leq  d^&lowast;_{(k)} \leq  d^&lowast;_{(k+1)} \leq ... d^&lowast;_{(n)} \end{align*}$$</span><!-- Has MathJax --></li><li>Form $\mathcal{V}_k(\mathbf{x}^∗)$, the k-Neighborhood of $\mathbf{x}^∗$<span>$$\begin{align*} \mathcal{V}_k(\mathbf{x}^&lowast;) = \Big\{ \mathbf{x}_i:  d(\mathbf{x}^*,\mathbf{x}_i)\leq d^*_{(k)}  \Big\}\end{align*}$$</span><!-- Has MathJax --></li><li>Compute the predicted response $\hat{Y}^*$ as <span>$$\begin{align*}\hat{Y}^*_{kNN} &amp;= \text{Most frequent label in } \mathcal{V}_k(\mathbf{x}^&lowast;) \\  &amp;=  \hat{f}^*_{kNN}(\mathbf{x}^*) = \underset{j\in\{1,...,g\}}{\texttt{argmax}} \Big\{  p_j^{(k)}(\mathbf{x}^*)  \Big\}\\ \text{where }  p_j^{(k)}(\mathbf{x}^*)= \frac{1}{k}\sum_{\mathbf{x}^* \in  \mathcal{V}_k(\mathbf{x}^&lowast;) } I(Y_i=j) &amp;\text{ estimates the probability that } \mathbf{x}^* \text{ belongs to class j based on} \mathcal{V}_k(\mathbf{x}^&lowast;)\end{align*}$$</span><!-- Has MathJax --></li></ol><ul><li>Note: <strong>[Posterior probability estimate]</strong> $ \frac{1}{k}\sum_{\mathbf{x}^* \in  \mathcal{V}_k(\mathbf{x}^∗) } I(Y_i=j) $ can be regarded as a rough estimate of $ \pi_j (\mathbf{x}^*) = Pr[Y^*=j| \mathbf{x}^* ] $ , the posterior probability of class membership of $ \mathbf{x}^* $</li></ul><h4 id="Comments"><a href="#Comments" class="headerlink" title="Comments:"></a>Comments:</h4><ul><li>kNearest Neighbors (kNN) essentially performs classification by voting for the most popular response among the k nearest neighbors of $\mathbf{x}^*$.</li><li>kNN provides the most basic form of nonparametric classification.</li><li>Since the fundamental building block of kNN is the distance measure, one can easily perform classification beyond the traditional setting where the predictors are numeric. For instance, classification with kNN can be readily performed on indicator attributes<br>$$  \mathbf{x}^* = (x_{i1},…, x_{ip}  )^T \in \{ 0,1\}^p $$</li><li>kNN classifiers are inherently naturally <strong>multi-class</strong>, and are used in many applications.</li></ul><p><br></p><h3 id="kNN-Regression"><a href="#kNN-Regression" class="headerlink" title="kNN Regression"></a>kNN Regression</h3><h4 id="Detailed-Steps-for-kNN-Regression"><a href="#Detailed-Steps-for-kNN-Regression" class="headerlink" title="Detailed Steps for kNN Regression"></a>Detailed Steps for kNN Regression</h4><p>$\mathcal{D} = \Big\{(x_1, Y_1),…, (x_n, Y_n) \Big\}$, with $x_i\in \mathcal{X}^q, Y_i\in \mathbb{R}$</p><ol><li>Choose the value of k and the distance to be used</li><li>Let $\mathbf{x}^*$ be a new point. Compute  $d(\mathbf{x}^*,\mathbf{x}_i)  \quad i=1,2,…n $</li><li>Rank all the distances $d^∗_i$ in increasing order: <span>$$\begin{align*} d^&lowast;_{(1)} \leq  d^&lowast;_{(2)} \leq ...\leq  d^&lowast;_{(k)} \leq  d^&lowast;_{(k+1)} \leq ... d^&lowast;_{(n)} \end{align*}$$</span><!-- Has MathJax --></li><li>Form $\mathcal{V}_k(\mathbf{x}^∗)$, the k-Neighborhood of $\mathbf{x}^∗$<span>$$\begin{align*} \mathcal{V}_k(\mathbf{x}^&lowast;) = \Big\{ \mathbf{x}_i:  d(\mathbf{x}^*,\mathbf{x}_i)\leq d^*_{(k)}  \Big\}\end{align*}$$</span><!-- Has MathJax --></li><li>Compute the predicted response $\hat{Y}^*$ as <span>$$\begin{align*}\hat{Y}^*_{kNN} &amp;=  \hat{f}^*_{kNN}(\mathbf{x}^*)\\ &amp;= \frac{1}{k}\sum_{\mathbf{x}^* \in  \mathcal{V}_k(\mathbf{x}^&lowast;) } Y_i \\  &amp;=  \frac{1}{k}\sum_{i=1}^n Y_i I( \mathbf{x}^* \in \mathcal{V}_k(\mathbf{x}^&lowast;) )\end{align*}$$</span><!-- Has MathJax --></li></ol><h4 id="Comments-1"><a href="#Comments-1" class="headerlink" title="Comments:"></a>Comments:</h4><ul><li>kNearest Neighbors (kNN) essentially performs regression by averaging the responses of the nearest neighbors of $\mathbf{x}^*$.</li><li>kNN provides the most basic form of nonparametric regression</li><li>Since the fundamental building block of kNN is the distance measure, one can easily perform regression beyond the traditional setting where the predictors are numeric. For instance, Regression vectors of binary with kNN can be readily performed on indicator attributes<br>$$  \mathbf{x}^* = (x_{i1},…, x_{ip}  )^T \in \{ 0,1\}^p $$ </li><li>kNN somewhat performs smoothing (filtering).</li><li>The estimated response kNN for $\mathbf{x}^*$ is estimator of the average response which is the <strong>conditional expectation of Y given $\mathbf{x}^*$</strong><br>$$ \hat{Y}^*_{kNN} =\mathbb{E}\widehat{[Y^*|\mathbf{x}^*]}  $$</li></ul><p><br></p><h3 id="Basic-kNN-amp-Weighted-kNN"><a href="#Basic-kNN-amp-Weighted-kNN" class="headerlink" title="Basic kNN &amp; Weighted kNN"></a>Basic kNN &amp; Weighted kNN</h3><h4 id="Limitation-of-Basic-kNN"><a href="#Limitation-of-Basic-kNN" class="headerlink" title="Limitation of Basic kNN"></a>Limitation of Basic kNN</h4><ul><li><p><strong>Equidistance</strong>: All neighbors are given the same contribution to the estimate of the response; In the estimated probability</p><span>$$\begin{align*} p_j^{(k)}(\mathbf{x}^*) &amp; = \frac{1}{k}\sum_{\mathbf{x}^* \in  \mathcal{V}_k(\mathbf{x}^&lowast;) } I(Y_i=j) =\sum_{\mathbf{x}^* \in  \mathcal{V}_k(\mathbf{x}^&lowast;) } w_i I(Y_i=j) \\ \text{the weight } w_i = \frac{1}{k}=\texttt{constant }&amp;\text{for all points in }  \mathcal{V}_k(\mathbf{x}^&lowast;) \text{ regardlesslyof how far they are from } \mathbf{x}^&lowast;\end{align*}$$</span><!-- Has MathJax --></li><li><p><strong>No model, weak interpretability</strong>: There is no underlying model, therefore no interpretation of the response relative to the predictor variables. There is no training set, since all happens at prediction. For this reason, kNN is referred to as <strong>lazy method</strong>.</p></li><li><p><strong>Computationally intensive</strong>: Predictions are computationally very intensive, due to the fact that for each new observation, the whole dataset must be traversed to compute the response</p></li></ul><p><br></p><h4 id="Extension-Weighted-kNN"><a href="#Extension-Weighted-kNN" class="headerlink" title="Extension: Weighted kNN"></a>Extension: Weighted kNN</h4><p>kNN classification can be improved by weighting the votes as a function of the distance from $\mathbf{x}^∗$. The weights are defined so as to preserve convexity $ \sum_{i=1}^k w_i = 1$. Some of the common weighting schemes include:</p><ul><li>Exponential Decay:<span>$$\begin{align*}w_i = \frac{e^{-d_i^*}}{\sum_{l=1}^k e^{-d_l^*}  } \end{align*}$$</span><!-- Has MathJax --></li><li>Inverse Distance:<span>$$\begin{align*}w_i = \frac{ \frac{1}{1+d_i^*}}{\sum_{l=1}^k  \frac{1}{1+d_l^*} } \end{align*}$$</span><!-- Has MathJax --></li></ul><p><br></p><h3 id="Effect-of-kNN"><a href="#Effect-of-kNN" class="headerlink" title="Effect of kNN"></a>Effect of kNN</h3><h4 id="Effect-of-k"><a href="#Effect-of-k" class="headerlink" title="Effect of k"></a>Effect of k</h4><ul><li><p>k controls the complexity of the underlying classifier, with small k yielding very complex classifiers and large k yielding rather simple ones.</p></li><li><p>If k is <strong>small</strong>, the estimated class is determined based on very few neighbors, the resulting kNN classifier will have very <strong>low bias</strong>, but very <strong>high variance</strong>. Take the example of k=1, the decision boundary will perfectly separate the classes on the training set, but will perform poorly on the test set.</p></li><li><p>If k is <strong>large</strong>, the estimated class is determined based on very many neighbors from far and wide, the resulting kNN classifier will<br>have very <strong>large bias</strong>, but very <strong>low variance</strong>. In fact, for truly large k, the decision boundary will be a constant hyperplane.</p></li><li><p>The determination of an optimal k desires the trade-off between bias and variance:<br>Determine k by cross validation<br>Determine k by direct minimization of the estimated prediction error via a suitably chosen test set</p><h4 id="Effect-of-n"><a href="#Effect-of-n" class="headerlink" title="Effect of n"></a>Effect of n</h4><p>As stated before, kNN is a lazy method, everything happens at prediction step. Thus the sample size n plays a crucial role, and <strong>large n</strong> would lead to <strong>intense</strong> prediction.</p><h4 id="Effect-of-p"><a href="#Effect-of-p" class="headerlink" title="Effect of p"></a>Effect of p</h4><p>The dimensionality p of the input space is <strong>only</strong> felt by the function that computes the <strong>distances</strong>. If the function is optimized, kNN should be <strong>unaffected</strong> by this dimensionality</p><h4 id="Effect-of-distance"><a href="#Effect-of-distance" class="headerlink" title="Effect of distance"></a>Effect of distance</h4><p>Some distances are more robust to extreme observations.</p></li></ul><h3 id="Pros-amp-Cons-of-kNN"><a href="#Pros-amp-Cons-of-kNN" class="headerlink" title="Pros &amp; Cons of kNN"></a>Pros &amp; Cons of kNN</h3><h4 id="Strength"><a href="#Strength" class="headerlink" title="Strength"></a>Strength</h4><ul><li>The kNN method is intuitively appealing and very easy to understand, explain, program/code and interpret</li><li>The kNN method provides a decent estimate of $Pr[Y = j|x]$, the posterior probability of class membership</li><li><span style="color:red"> <em>The kNN method easily handles missing values (by restricting distance calculations to subspace)</em></span></li><li><span style="color:red"> <em>As the number of training samples grows larger, the asymptotic misclassification error rate is bounded by twice the Bayes risk.</em></span><br>$$ \underset{n\rightarrow \infty}{lim} R(\hat{f}_n^{(kNN)}) \leq 2R^* $$</li><li><span style="color:red"> <em>The kNN method is naturally suitable for sequential/incremental machine learning.</em></span></li><li><span style="color:red"> <em>The kNN method is also suitable where the hypothesis space is variable in size.</em></span></li><li>The kNN method can handle <strong>non-numeric</strong> data as long as the <strong>distance</strong> can be defined.</li><li>The kNN methods can handle <strong>mixed types</strong> of data as long as the <strong>distance</strong> are computed as <strong>hybrid or combinations</strong>.</li><li>The kNN method is inherently multi-class. This is very important because for some other methods, going beyond binary classification requires some sophisticated mathematics. It also handles very flexible decision boundaries.</li></ul><h4 id="Weakness"><a href="#Weakness" class="headerlink" title="Weakness"></a>Weakness</h4><ul><li>The <strong>computational complexity</strong> of kNN is very high in prediction. Specifically, it is $\mathcal{O}(nmp)$ where n is the training set size, m is the test set size and p is the number of predictor variables. This means that kNN requires <strong>large amount of memory</strong>, and therefore does NOT scale well. This <strong>failure in scalability</strong> is addressed using various heuristics and strategies.</li><li>kNN methods <strong>suffer from the Curse of Dimensionality (COD)</strong>. When p is large and n is small (short fat data/ dimension of the space very high), the concept of nearness becomes meaningless to the point of being ill-defined, because the ”neighborhood” becomes very large. Thus, the nearest neighbor could be very far when the space is high dimensional and there are very few observations.</li><li>The kNN method does not yield a model, and therefore no parameter to help explain why the method performs as it does.</li><li>The kNN method is heavily affected by the local structure, and it is very sensitive to both irrelevant and correlated features. Unless the distance is well chosen and properly calibrated, kNN methods will be sensitive to outliers and all sorts of noise in the data.</li><li>Unless the distance is used in some way to weight the neighbors, more <strong>frequent classes will dominate</strong> in the determination of the estimated label. This means one has to be careful with kNN when one class has a far larger proportion of observations than the others.</li><li>The measurement scale of each variable affect the kNN method more than most methods. (measurement scale: standardizing, unitizing or cubizing/squeezing the data).</li></ul><p><br></p><h3 id="Application-of-kNN"><a href="#Application-of-kNN" class="headerlink" title="Application of kNN"></a>Application of kNN</h3><ul><li><strong>Handwritten Digit Recognition</strong> is usually the first task in some Data Analytics competitions.</li><li><strong>Text Mining</strong> and specific topic of text categorization/classification has made successful use of kNearest Neighbors approach</li><li><strong>Credit Scoring</strong> is another application that has been connected with k Nearest Neighbors Classification</li><li><strong>Disease Diagnostics</strong> also has been tackled using k Nearest Neighbors Classifiers</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This chapter talks about k Nearest Neighbors (kNN) as a metholodogy for both classification and regression problems. The kNN method serves a basic and easy to understand foundational machine learning and data mining technique. The kNN method is an excellent baseline machine learning technique, and also allows many extensions. It usually performs reasonable well or sometimes very well when compared to more sophisticated techniques.&lt;/p&gt;
&lt;p&gt;kNN classification basically means the estimated class of a vector $\mathbf{x}$ is the most frequent class label in the neighborhood of $\mathbf{x}$. kNN classifiers are inherently naturally multi-class, and are used extensively in applications such as image processing, character recognition and general pattern recognition tasks. For kNN regression, the estimated response value of a vector $\mathbf{x}$ is the average of the response values in the neighborhood of $\mathbf{x}$.&lt;br&gt;
    
    </summary>
    
      <category term="Data Mining" scheme="https://qiuyiwu.github.io/categories/Data-Mining/"/>
    
    
      <category term="kNN, Classification, Regression, Distance" scheme="https://qiuyiwu.github.io/tags/kNN-Classification-Regression-Distance/"/>
    
  </entry>
  
  <entry>
    <title>Data Mining Note 2 - Binary Classification</title>
    <link href="https://qiuyiwu.github.io/2018/01/05/Data-Mining-Note2/"/>
    <id>https://qiuyiwu.github.io/2018/01/05/Data-Mining-Note2/</id>
    <published>2018-01-06T01:07:53.000Z</published>
    <updated>2018-12-21T04:51:39.772Z</updated>
    
    <content type="html"><![CDATA[<p>This chapter gives basic introduction of Vapnik-Chervonenkis Theory on Binary Classification. When it comes to binary classification task, we always want to know the functional relationship between $\mathbf{x}$ and $y$, or how to determine the “best” approach to determining from the available observaions such that given a new observation $\mathbf{x}^{new}$, we can predict its class $y^{new}$ as accurately as possible.</p><h3 id="Universal-Best"><a href="#Universal-Best" class="headerlink" title="Universal Best"></a>Universal Best</h3><p>Constructing a classification rule that puts all the points in their corresponding classes could be dangerous for classifying new observations not present in the current collection of observations. So to find an classification rule that achieves the absolute very best on the present data is not enough since infinitely many more observations can be generated. <strong>Even the universally best classifier will make mistakes.</strong><br><a id="more"></a></p><p>Of all the functions in $\mathcal{Y}^{\mathcal{X}}$, assume that there is a function $f^*$ that maps any $\mathbf{x}\in\mathcal{X}$ to the corresponding $y\in\mathcal{Y}$ with the minimum number of mistakes. $f^*$ is denoted as <strong>universal best</strong><br><span>$$\begin{align*}f^* :\quad &amp; \mathcal{X} \rightarrow \mathcal{Y}\\     &amp;  \mathbf{x} \rightarrow f^*(\mathbf{x})\end{align*}$$</span><!-- Has MathJax --></p><p>$f$ denote any generic function mapping an element $\mathbf{x}$ of $\mathcal{X}$ to its corresponding image $f(\mathbf{x})$ in $\mathcal{Y}$. Each time $\mathbf{x}$ is drawn from $\mathbb{P}(\mathbf{x})$, the disagreement between the image $f(\mathbf{x})$ and the true image y is called the loss, denoted by $l(y, f(\mathbf{x}))$. The expected value of this loss function with respect to the distribution $\mathbb{P}(\mathbf{x})$ is called <strong>the risk functional</strong> of $f$, denoted as $R(f)$.<br>$$ R(f) = \mathbb{E}[l(Y, f(\mathbf{X}))] = \int l(y, f(\mathbf{x})) d \mathbb{P}(\mathbf{x})  $$<br>The best function $f^*$ over the space $\mathcal{Y}^{\mathcal{X}}$ of all measurable functions from $\mathcal{X}$ to $\mathcal{Y}$ is therefore<br><span>$$\begin{align*}f^* &amp;=\texttt{arg} \underset{f}{inf} R(f)  \\R(f^*) &amp;= R^* = \underset{f}{inf} R(f)\end{align*}$$</span><!-- Has MathJax --><br>Note: Finding $f^*$ without the knowledge of $\mathbb{P}(\mathbf{x})$ means having to search the infinite dimensional function space $\mathcal{Y}^{\mathcal{X}}$ of all mappings from $\mathcal{X}$ to $\mathcal{Y}$, which is an ill-posed and computationally nasty problem (No way to get the universal best).</p><p>Thus, we seek a more reasonable way to solve the problem via choosing from a function space $F\subset \mathcal{Y}^{\mathcal{X}} $ , a function $f^+ \in F$ that best estimates the dependencies between $\mathbf{x}$ and $y$. The <strong>goal</strong> of statistical function estimation is to devise a technique (strategy) that chooses from the function class $\mathcal{F}$, the one function whose true risk is as close as possible to the lowest risk in class $\mathcal{F}$. So it is crucial to define what is meant to be best estimates. Hence we need to know what is loss function and risk functional.</p><h3 id="Loss-Functioin-and-Risk-Functional"><a href="#Loss-Functioin-and-Risk-Functional" class="headerlink" title="Loss Functioin and Risk Functional"></a>Loss Functioin and Risk Functional</h3><p>For classification task, the so-called 0-1 loss function defined below is used.<br>$$ l(y, f(\mathbf{x})) = \mathbb{1}_{Y\neq f(X)} = \begin{cases} 0, &amp;\text{if } y = f(\mathbf{x}) \\  1, &amp;\text{if } y \neq f(\mathbf{x}) \end{cases} $$</p><p>And the corresponding risk functional, also named cost function showed below confirms the intuition because it is estimated in practice by simply computng the proportion of misclassfied entities.<br>$$ R(f)= \int l(y, f(\mathbf{x})) d \mathbb{P}(\mathbf{x})   = \mathbb{E}[\mathbb{1}_{Y\neq f(X)}] = \underset{(X,Y)\sim \mathbb{P}}{Pr}[Y\neq f(X)]$$</p><p>The minimizer of the 0-1 risk functional over all possible classifiers is the so-called Bayes classifier which we shall denote here by $f^*$. It minimizes the rate of misclassifications.<br>$$  f^* =\texttt{arg} \underset{f}{inf} R(f) =  \texttt{arg} \underset{f}{inf} [\underset{(X,Y)\sim \mathbb{P}}{Pr}[Y\neq f(X)]  ]$$</p><p>Specifically, the Bayes’ classifier $f^*$ is given by the posterior probability of class membership, namely<br>$$  f^* =\texttt{arg} \underset{y\in \mathcal{Y}}{max} [Pr[Y=y | \mathbf{x}]  ]$$</p><p>Because it is impossible to find the universal best $f^*$, We need to select a reasonable function space $F\subset \mathcal{Y}^{\mathcal{X}} $, and then choose the best estimator $f^+$ from $\mathcal{F}$. For the binary pattern recognition problem, one may consider finding the best linear separating hyperplane.</p><p>For empirical risk minimization,<br><span>$$\begin{align*} \hat{R}(f) &amp;=  \frac{1}{n}\sum_{i=1}^n \mathbb{1}_{Y\neq f(X)}\\  \hat{f} &amp;=     \underset{f\in \mathcal{F}}{\texttt{argmin}} [  \frac{1}{n}\sum_{i=1}^n \mathbb{1}_{Y\neq f(X)} ]\end{align*}$$</span><!-- Has MathJax --></p><h3 id="Bias-Variance-Trade-Off"><a href="#Bias-Variance-Trade-Off" class="headerlink" title="Bias-Variance Trade-Off"></a>Bias-Variance Trade-Off</h3><p>In statistical estimation, the blow important issues needs to be taken into account:</p><ul><li>Bias of the estimator</li><li>Variance of the estimator</li><li>Consistency of the estimator<br>In point estimation, given $\theta$ as the true value of the parameter, $\hat{\theta}$ is a point estimator of $\theta$, then the total error can be decomposed as: <span>$$\begin{align*}  \hat{\theta}-\theta = \underbrace{ \hat{\theta}- \mathbb{E}[ \hat{\theta}]}_\text{Estimation error}  + \underbrace{\mathbb{E}[ \hat{\theta}]-\theta}_\text{Bias} \end{align*}$$</span><!-- Has MathJax -->Under the squared error loss, one seeks $\hat{\theta}$ that minimizes the mean squared error, rather than trying to find the minimum variance unbiased estimator (MVUE).<span>$$\begin{align*}  \hat{\theta}=\underset{\theta\in \Theta}{\texttt{argmin}}\mathbb{E}[ (\hat{\theta}-\theta)^2] = \underset{\theta\in \Theta}{\texttt{argmin}}MSE(\hat{\theta})\end{align*}$$</span><!-- Has MathJax -->Actually, the traditional bias-variance decomposition of the MSE reveals the bias-variance trade-off: <span>$$\begin{align*} MSE(\hat{\theta}) &amp;= \mathbb{E}[ (\hat{\theta}-\theta)^2] \\  &amp;= \mathbb{E}[ (\hat{\theta}- \mathbb{E}[ \hat{\theta}])^2] + \mathbb{E}[(\mathbb{E}[ \hat{\theta}]-\theta)^2 ] \\  &amp;= \text{Variance} + \text{Bias}^2\end{align*}$$</span><!-- Has MathJax -->Again becasue of the previous statement that we cannot get the true value of $\theta$, MVUE will not help, which means the estimation we get contains bias. If the bias is too small, variance would be very large. Vice versa. The best compromise is then to trade-off bias and variance. In functional terms we call it trade-off between <strong>approximation error</strong> and <strong>estimation error</strong>.</li></ul><p><img src="/images/2018/01/BVTradeoff.png" alt="Sample Image Added via Markdown"></p><h3 id="Statistical-Consistency"><a href="#Statistical-Consistency" class="headerlink" title="Statistical Consistency"></a>Statistical Consistency</h3><p>$\hat{\theta}_n$ is a consistent estimator of $\theta$ if $\forall \epsilon &gt;0$<br>$$ \underset{n\rightarrow \infty}{lim}Pr[|\hat{\theta}_n-\theta|&gt;\epsilon] = 0 $$</p><p>It turns out that for unbiased estimators $\hat{\theta}_n$, consistency is straightforward as direct consequence of a basic probabilistic inequality like Chebyshev’s inequality. However, for biased estimators, one has to be more careful.<br>The <strong>ERM (Consistency of the Empirical Risk Minimization) principle</strong> is consistent if it provides a sequence of functions for which both the expected risk and the empirical risk converge to the minimal possible value of the risk in the function class under consideration.<br><span>$$\begin{align*}R(\hat{f}_n) &amp; \xrightarrow[n \rightarrow \infty]{P} \underset{f\in \mathcal{F}}{inf}R(f) = R(f^+) \\ \underset{n\rightarrow \infty}{lim} Pr&amp;[\underset{f\in\mathcal{F}}{sup}|R(f)-\hat{R}_n(f)|&gt;\epsilon] = 0\end{align*}$$</span><!-- Has MathJax --><br>As $\hat{R}_n(f) $ reveals the disagreement between classifier $f$ and the truth about the label $y$ of $\mathbf{x}$ based on information contained in the sample $\mathcal{D}$. So for a given (fixed) function (classifier) $f$,<br>$$ \mathcal{E}[\hat{R}_n(f)] = R(f) $$</p><p><strong>Key Question:</strong><br>Since one cannot calculate the true error, how can one devise a learning strategy for choosing classifiers based on it?<br><strong>Tentative Answer:</strong><br>At least devise strategies that yield functions for which the upper bound on the theoretical risk is as tight as possible:</p><p>With probability $1 − \delta$ over an i.i.d. draw of some sample according to the distribution $\mathcal{P}$, the expected future error rate of some classifier is bounded by a function $g$ ($\delta$, error rate on sample) of $\delta$ and the error rate on sample. $\Downarrow\Downarrow\Downarrow$<br><span>$$\begin{align*}\text{Pr}\Big\{ \texttt{TestError} \leq \texttt{TrainError} + \phi(n,\delta, \kappa (\mathcal{F}))  \Big\} \leq 1-\delta\end{align*}$$</span><!-- Has MathJax --></p><p><strong>Thorem: (Vapnik and Chervonenkis, 1971)</strong><br>Let $\mathcal{F}$ be a class of functions implementing so learning machines, and let $\xi = VCdim(\mathcal{F})$ be the VC dimension of F. Let the theoretical and the empirical risks be defined as earlier and consider any data distribution in the population of interest. Then $\forall f\in \mathcal{F}$, the prediction error (theoretical risk) is bounded by<br>$$  R(f) \leq \hat{R}_n(f) + \sqrt{  \frac{\zeta(\text{log}\frac{2n}{\zeta}+1)-\text{log}\frac{\eta}{4} }{n}} $$<br>with probability of at least $1-\eta$. or<br>$$\text{Pr} \Big( \texttt{TestError} \leq \texttt{TrainError} +     \sqrt{  \frac{\zeta(\text{log}\frac{2n}{\zeta}+1)-\text{log}\frac{\eta}{4} }{n}} \Big) \leq 1-\eta  $$</p><p><strong>VC Bound</strong></p><ul><li>From the expression of the VC Bound, to improve the predictive performance (reduce prediction error) of a class of machines is to achieve a<br>trade-off (compromise) between small VC dimension and minimization of the empirical risk.</li><li>One of the greatest appeals of the VC bound is that, though applicable to function classes of infinite dimension, it preserves the same intuitive form as the bound derived for finite dimensional $\mathcal{F}$</li><li>VC bound is acting in a way similar to the number of parameters, since it serves as a measure of the complexity of $\mathcal{F}$. </li><li>One should seek to construct a classifier that achieves the best trade-off between complexity of function class (measured by VC dimension) and fit to the training data (measured by empirical risk).</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This chapter gives basic introduction of Vapnik-Chervonenkis Theory on Binary Classification. When it comes to binary classification task, we always want to know the functional relationship between $\mathbf{x}$ and $y$, or how to determine the “best” approach to determining from the available observaions such that given a new observation $\mathbf{x}^{new}$, we can predict its class $y^{new}$ as accurately as possible.&lt;/p&gt;
&lt;h3 id=&quot;Universal-Best&quot;&gt;&lt;a href=&quot;#Universal-Best&quot; class=&quot;headerlink&quot; title=&quot;Universal Best&quot;&gt;&lt;/a&gt;Universal Best&lt;/h3&gt;&lt;p&gt;Constructing a classification rule that puts all the points in their corresponding classes could be dangerous for classifying new observations not present in the current collection of observations. So to find an classification rule that achieves the absolute very best on the present data is not enough since infinitely many more observations can be generated. &lt;strong&gt;Even the universally best classifier will make mistakes.&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Data Mining" scheme="https://qiuyiwu.github.io/categories/Data-Mining/"/>
    
    
      <category term="classification, Vapnik-Chervonenkis, binary" scheme="https://qiuyiwu.github.io/tags/classification-Vapnik-Chervonenkis-binary/"/>
    
  </entry>
  
  <entry>
    <title>Data Mining Note 1 - Introduction</title>
    <link href="https://qiuyiwu.github.io/2018/01/05/Data-Mining-Note1/"/>
    <id>https://qiuyiwu.github.io/2018/01/05/Data-Mining-Note1/</id>
    <published>2018-01-05T19:20:09.000Z</published>
    <updated>2018-12-21T04:52:19.014Z</updated>
    
    <content type="html"><![CDATA[<p>I took Dr. Ernest Fokoue’s course Data Mining (STAT 747) in my Master study in RIT and gained tremendous fascinating modern Statistical Machine Learning technique skills. I want to share the marvelous essence of this course and my self-learning and self-reflection towards this course.</p><p>This course covers topics such as clustering, classification and regression trees, multiple linear regression under various conditions, logistic regression, PCA and kernel PCA, model-based clustering via mixture of gaussians, spectral clustering, text mining, neural networks, support vector machines, multidimensional scaling, variable selection, model selection, k-means clustering, k-nearest neighbors classifiers, statistical tools for modern machine learning and data mining, naïve Bayes classifiers, variance reduction methods (bagging) and ensemble methods for predictive optimality.<br><a id="more"></a></p><p>I will show the roadmap of this note in this post and follow the order. Basically, each post contains one essential data mining technique and later I will show some relative examples and exercises based on these methods.</p><ul><li>Supervised Learning<br>   Classification<br>   Regression</li><li>Unsupervised Learning<br>  Clustering Analysis<br>  Factor Analysis<br>  Topic Modeling<br>  Recommender System</li></ul><h3 id="Application-in-Statistical-Machine-Learning"><a href="#Application-in-Statistical-Machine-Learning" class="headerlink" title="Application in Statistical Machine Learning"></a>Application in Statistical Machine Learning</h3><ul><li>Handwritten Digit Recognition (MNIST)</li><li>Text Mining</li><li>Credit Scoring</li><li>Disease Diagonostics</li><li>Audio Processing</li><li>Speaker Recognition &amp; Speaker Identification</li></ul><h3 id="Computing-Tools-in-R"><a href="#Computing-Tools-in-R" class="headerlink" title="Computing Tools in R"></a>Computing Tools in R</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(ctv)</span><br><span class="line"><span class="keyword">library</span>(MachineLearning)</span><br><span class="line"><span class="keyword">library</span>(HighPerformanceComputing)</span><br><span class="line"><span class="keyword">library</span>(TimeSeries)</span><br><span class="line"><span class="keyword">library</span>(Bayesian)</span><br><span class="line"><span class="keyword">library</span>(Robust)</span><br><span class="line"><span class="keyword">library</span>(biglm)</span><br><span class="line"><span class="keyword">library</span>(foreach)</span><br><span class="line"><span class="keyword">library</span>(glmnet)</span><br><span class="line"><span class="keyword">library</span>(kernlab)</span><br><span class="line"><span class="keyword">library</span>(randomForest)</span><br><span class="line"><span class="keyword">library</span>(ada)</span><br><span class="line"><span class="keyword">library</span>(audio)</span><br><span class="line"><span class="keyword">library</span>(rpart)</span><br><span class="line"><span class="keyword">library</span>(e1071)</span><br><span class="line"><span class="keyword">library</span>(MASS)</span><br><span class="line"><span class="keyword">library</span>(kernlab)</span><br></pre></td></tr></table></figure><h1 id="Important-Aspects-of-Machine-Learning"><a href="#Important-Aspects-of-Machine-Learning" class="headerlink" title="Important Aspects of Machine Learning"></a>Important Aspects of Machine Learning</h1><h5 id="Machines-Inherently-designed-to-handle-p-larger-than-n-problems"><a href="#Machines-Inherently-designed-to-handle-p-larger-than-n-problems" class="headerlink" title="Machines Inherently designed to handle p larger than n problems"></a>Machines Inherently designed to handle p larger than n problems</h5><ul><li>Classification and Regression Trees</li><li>Support Vector Machines</li><li>Relevance Vector Machines (n &lt; 500)</li><li>Gaussian Process Learning Machines (n &lt; 500)</li><li>k-Nearest Neighbors Learning Machines (Watch for the curse of dimensionality)</li><li>Kernel Machines in general<h5 id="Machines-can-handle-p-larger-than-n-problems-if-regularized-with-suitable-constraints"><a href="#Machines-can-handle-p-larger-than-n-problems-if-regularized-with-suitable-constraints" class="headerlink" title="Machines can handle p larger than n problems if regularized with suitable constraints"></a>Machines can handle p larger than n problems if regularized with suitable constraints</h5></li><li>Multiple Linear Regression Models</li><li>Generalized Linear Models</li><li>Discriminant Analysis<h5 id="Ensemble-Learning-Machines"><a href="#Ensemble-Learning-Machines" class="headerlink" title="Ensemble Learning Machines"></a>Ensemble Learning Machines</h5></li><li>Random Subspace Learning Ensembles (Random Forest)</li><li>Boosting and its extensions</li></ul><p><br><br> <span style="color:red"> <strong>Note:</strong> <em>Red parts in this Note Series remain questionable and I will update and add explanations for those parts as soon as I figure them out.</em></span></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I took Dr. Ernest Fokoue’s course Data Mining (STAT 747) in my Master study in RIT and gained tremendous fascinating modern Statistical Machine Learning technique skills. I want to share the marvelous essence of this course and my self-learning and self-reflection towards this course.&lt;/p&gt;
&lt;p&gt;This course covers topics such as clustering, classification and regression trees, multiple linear regression under various conditions, logistic regression, PCA and kernel PCA, model-based clustering via mixture of gaussians, spectral clustering, text mining, neural networks, support vector machines, multidimensional scaling, variable selection, model selection, k-means clustering, k-nearest neighbors classifiers, statistical tools for modern machine learning and data mining, naïve Bayes classifiers, variance reduction methods (bagging) and ensemble methods for predictive optimality.&lt;br&gt;
    
    </summary>
    
      <category term="Data Mining" scheme="https://qiuyiwu.github.io/categories/Data-Mining/"/>
    
    
      <category term="SVM, KNN, KMeans, Ensemble, NN" scheme="https://qiuyiwu.github.io/tags/SVM-KNN-KMeans-Ensemble-NN/"/>
    
  </entry>
  
</feed>
