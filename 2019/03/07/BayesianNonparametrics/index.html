<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  

  
  <title>Bayesian Nonparametrics Notes | Qiuyi&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="The note is partially based on the Bayesian Nonparametrics Machine Learning lectures by Yee Whye Teh at Max Planck Institute for Intelligent Systems in Tübingen, Germany.  Machine learning is all abou">
<meta name="keywords" content="Nonparametric,Chinese Restaurant,Indian Buffet,Statistics">
<meta property="og:type" content="article">
<meta property="og:title" content="Bayesian Nonparametrics Notes">
<meta property="og:url" content="https://qiuyiwu.github.io/2019/03/07/BayesianNonparametrics/index.html">
<meta property="og:site_name" content="Qiuyi&#39;s Blog">
<meta property="og:description" content="The note is partially based on the Bayesian Nonparametrics Machine Learning lectures by Yee Whye Teh at Max Planck Institute for Intelligent Systems in Tübingen, Germany.  Machine learning is all abou">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://qiuyiwu.github.io/images/2019/03/model.png">
<meta property="og:image" content="https://qiuyiwu.github.io/images/2019/03/HMM.png">
<meta property="og:image" content="https://qiuyiwu.github.io/images/2019/03/recommend.png">
<meta property="og:image" content="https://qiuyiwu.github.io/images/2019/03/dirichlet.png">
<meta property="og:image" content="https://qiuyiwu.github.io/images/2019/03/model.png">
<meta property="og:image" content="https://qiuyiwu.github.io/images/2019/03/crp.png">
<meta property="og:image" content="https://qiuyiwu.github.io/images/2019/03/crpmm.png">
<meta property="og:updated_time" content="2019-03-10T17:54:50.865Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bayesian Nonparametrics Notes">
<meta name="twitter:description" content="The note is partially based on the Bayesian Nonparametrics Machine Learning lectures by Yee Whye Teh at Max Planck Institute for Intelligent Systems in Tübingen, Germany.  Machine learning is all abou">
<meta name="twitter:image" content="https://qiuyiwu.github.io/images/2019/03/model.png">
  
    <link rel="alternate" href="/atom.xml" title="Qiuyi&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
</html>
<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Qiuyi&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Researcher✨Qiuyi Wu</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://qiuyiwu.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-BayesianNonparametrics" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/03/07/BayesianNonparametrics/" class="article-date">
  <time datetime="2019-03-07T16:36:37.000Z" itemprop="datePublished">2019-03-07</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Statistics/">Statistics</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Bayesian Nonparametrics Notes
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>The note is partially based on the Bayesian Nonparametrics Machine Learning lectures by Yee Whye Teh at Max Planck Institute for Intelligent Systems in Tübingen, Germany. </p>
<p>Machine learning is all about data, and the uncertainty and complex process in the data. Probability theory is a rich language to express uncertainties. Graphical tool and complex models are develped to help visualize and derive algorithmic solutions.<br><a id="more"></a></p>
<h1 id="Probabilistic-Modelling"><a href="#Probabilistic-Modelling" class="headerlink" title="Probabilistic Modelling"></a>Probabilistic Modelling</h1><ul>
<li>Data: $x_1, x_2, …, x_n$</li>
<li>Latent variables: $y_1, y_2, …, y_n$</li>
<li>Parameter: $\theta$</li>
<li>Probabilistic model (generative model): parametrized joint distribution over variables<br>$P(x_1, x_2, …, x_n, y_1, y_2, …, y_n|\theta)$</li>
<li>Inference, of latent variables given observed data:<br>$P(y_1, y_2, …, y_n|x_1, x_2, …, x_n, \theta) = \frac{P(x_1, x_2, …, x_n, y_1, y_2, …, y_n|\theta)}{P(x_1, x_2, …, x_n|\theta)}$</li>
<li>Learning, of parameters by maximum likelihood:<br>$\theta^{ML} = \underset{\theta}{\text{argmax }}P(x_1, x_2, …, x_n|\theta) $</li>
<li>Prediction: $P( x_{n+1}, y_{n+1} |x_1, x_2, …, x_n,\theta)$</li>
<li>Classification: $\underset{c}{\text{argmax }}P(x_{n+1}|\theta^c) $</li>
</ul>
<h1 id="Bayesian-Modelling"><a href="#Bayesian-Modelling" class="headerlink" title="Bayesian Modelling"></a>Bayesian Modelling</h1><ul>
<li>Prior distribution: $P(\theta)$</li>
<li>Posterior distribution (both inference and learning):<br>$P(y_1, y_2, …, y_n,\theta |x_1, x_2, …, x_n) = \frac{P(x_1, x_2, …, x_n, y_1, y_2, …, y_n|\theta) P(\theta)  }{  P(x_1, x_2, …, x_n) }$</li>
<li>Prediction: $P( x_{n+1}| x_1, x_2, …, x_n) = \int P( x_{n+1}|\theta )P(\theta| x_1, x_2, …, x_n  )d\theta $</li>
<li>Classification: $P( x_{n+1}^c| x_1^c, x_2^c, …, x_n^c) = \int P( x_{n+1}|\theta^c )P(\theta^c| x_1^c, x_2^c, …, x_n^c  )d\theta^c $</li>
</ul>
<h2 id="Model-based-Clustering"><a href="#Model-based-Clustering" class="headerlink" title="Model-based Clustering"></a>Model-based Clustering</h2><p>Given data from heterogeneous unknown sources, and each cluster modelled using a parametric model:</p>
<ul>
<li>Data item:<br>$z_i|\pi\sim \text{Discrete}(\pi)$<br>$x_i|z_i,\theta_k \sim F(\theta))$</li>
<li>Mising proportions: $\pi = (\pi_1, \pi_2,…, \pi_K)|\alpha \sim \text{Dirichlet}(\alpha/K,…, \alpha/K)$</li>
<li>Cluster k: $\theta_k|\beta \sim \beta$<center><br><img class="left" src="/images/2019/03/model.png" width="50%" height="50%"><br></center>

</li>
</ul>
<h2 id="Hidden-Markov-Models"><a href="#Hidden-Markov-Models" class="headerlink" title="Hidden Markov Models"></a>Hidden Markov Models</h2><ul>
<li>Popular model for time series data (clustering over time)</li>
<li>Unobserved dynamic modelled using a Markov model</li>
<li>Observations modelled as independent conditioned on current state<center><br><img class="left" src="/images/2019/03/HMM.png" width="70%" height="70%"><br></center>

</li>
</ul>
<h2 id="Collaborative-Filtering"><a href="#Collaborative-Filtering" class="headerlink" title="Collaborative Filtering"></a>Collaborative Filtering</h2><p>A kind of recommendation system. e.g.: predict how much users would like products that they haven’t seen based on the previous data. </p>
<p>Data: Rating $R_{ij}$ of a user $\xi_i$ for a certain product $\eta_j$<br>$R_{ij}|\xi_i,\eta_j  \sim \mathcal{N}(\xi_i^\top \eta_j, \sigma^2) $</p>
<center><br><img class="left" src="/images/2019/03/recommend.png" width="40%" height="40%"><br></center>


<h1 id="Bayesian-Nonparametrics"><a href="#Bayesian-Nonparametrics" class="headerlink" title="Bayesian Nonparametrics"></a>Bayesian Nonparametrics</h1><p>As I discussed in the previous blog post (<a href="https://qiuyiwu.github.io/2019/02/19/GaussianProcess/">Why are Gaussian Process Models called Nonparametric?</a>), nonparametric model is a <strong>parametric</strong> model where the number of parameters increases with data. It just cannot be described by fixed number of parameters. And nonparametric models still make model assumptions, but they are less constrained. </p>
<p>Why do we want Bayesian nonparametrics?</p>
<ol>
<li>Model Selection:<br>Typically we use parametric model, say clustering, we need to decide the number of clusters. If we have too many clusters we would encounter overfitting issue, and underfitting if we have too little clusters. In Bayesian model we don’t do model optimization to find the maximum likelihood parameters for the optimal number of clusters, we just compute posterior distribution for the unknown part. And the Bayesian nonparametric model grows with amount of data, thus we can prevent overfitting and underfitting issues. </li>
<li>Large Function Spaces:<br>We would like to learn the large function space and infer the infinite dimensional objects themselves, and we want to learn the density of the input space.</li>
<li>Structural Learning:<br>We want to learn the structures of the data (e.g. hierarchical clustering), and we can use bayesian prior over combinatorial structures. Commonly, nonparametric priors alwayes lead to simpler learning algorithm than parametric priors because you don’t need to worry about complicated model selection (e.g. number of latent variables, the way of latent variables connecting to other variables etc.). </li>
<li>Novel &amp; Useful Properties<ul>
<li>Exchangeability: permute your dataset without effecting the analysis</li>
<li>Power laws behaviors: Pitman-Yor process, Indian Buffet process</li>
<li>Flexible to build complex models: hierarchical nonparametric models, dependent Dirichlet process</li>
</ul>
</li>
</ol>
<h1 id="Dirichlet-Process"><a href="#Dirichlet-Process" class="headerlink" title="Dirichlet Process"></a>Dirichlet Process</h1><p>Dirichlet process is the cornerstone of modern Bayesian nonparametrics. It is the infinite limit of finite mixture models. It was defined by Ferguson in 1973 as a distribution over measures.  </p>
<h2 id="Finite-Mixture-Models"><a href="#Finite-Mixture-Models" class="headerlink" title="Finite Mixture Models"></a>Finite Mixture Models</h2><p>Dirichlet distrbution on K-dimensional probability simplex $\{\pi | \sum_k \pi_k=1 \}$:<br>$P(d\pi| \alpha) = \frac{\Gamma (\alpha)}{\Pi_k\Gamma(\alpha/K)}\Pi_{k=1}^K \pi_k^{\alpha/K-1}d\pi \quad$     with $\Gamma (\alpha) = \int_0^\infty x^{\alpha -1} e^x dx$</p>
<center><br><img class="left" src="/images/2019/03/dirichlet.png" width="80%" height="80%"><br></center>

<p>See <strong>Model-based Clustering</strong> part in the previous section. Given the observations $\mathbf{x}$ we want to learn the latent variables $\mathbf{z}, \pi, \mathbf{\theta}$. We can use MCMC sampling to infer the unknown parameters. We are constructing a Markov chain such that we can do the inference in an iterative manner as the chain would converge at some point. </p>
<h3 id="Gibbs-Sampling"><a href="#Gibbs-Sampling" class="headerlink" title="Gibbs Sampling"></a>Gibbs Sampling</h3><center><br><img class="left" src="/images/2019/03/model.png" width="50%" height="50%"><br></center>

<ul>
<li>Calculate the conditional distribution of the latent variables given the parameters and the observations: $ p(z_i=k|\text{ others }) \propto \pi_k f(x_i |\theta_k^*) $<br>$ \pi|\text{ others } \sim \text{Dirichlet}(\frac{\alpha}{K}+n_1,…, \frac{\alpha}{K}+n_K) $</li>
<li>Calculate the conditional distribution of the parameters given the latent variables and the observations: $ p(\theta_k^*=\theta|\text{ others }) \propto B(\theta)\Pi_{j:z_j=k} f(x_j |\theta) $ </li>
<li>Iterate this process until they (latent variables and parameters) converge to the posterior distribution. </li>
</ul>
<h3 id="Collapsed-Gibbs-Sampling"><a href="#Collapsed-Gibbs-Sampling" class="headerlink" title="Collapsed Gibbs Sampling"></a>Collapsed Gibbs Sampling</h3><p>A more efficient approach of MCMC argorithm: integrate out $\pi, \theta^*$</p>
<ul>
<li>Marginalize out the parameters of the model </li>
<li>All we don’t know is the latent variable (the clusters), and now we have the reduced space to update.<br>$ p(z_i=k|\text{ others }) \propto  \frac{\frac{\alpha}{K} + n_k^{-i}}{\alpha+n-1}f(x_i|z_j=k)$<br>where $ f(x_i|z_j=k) \propto \int B(\theta)f(x_i|\theta)\Pi_{j\neq i: z_j = k} f(x_j|\theta)d\theta  $<br>The prior term $\frac{\frac{\alpha}{K} + n_k^{-i}}{\alpha+n-1}$ is interpretable, $n_k^{-i}$ denotes the number of other data iterms currently assigned to cluster $k$ besides the current item $i$ we are interested in. $\frac{\alpha}{K}$ means even no item belongs to cluster $k$, we still have some probability for cluster $k$. The likelihood $f(x_i|z_j=k)$ means the conditional probability of the observation given the observations that are currently assigned to cluster $k$</li>
<li>Conditional distributions can be easily computed if $F$ is conjugate to $B$.</li>
</ul>
<h3 id="Infinite-Limit-of-Collapsed-Gibbs-Sampling"><a href="#Infinite-Limit-of-Collapsed-Gibbs-Sampling" class="headerlink" title="Infinite Limit of Collapsed Gibbs Sampling"></a>Infinite Limit of Collapsed Gibbs Sampling</h3><p>Assume a very large varlue of $K \rightarrow \infty$, there are at most $n&lt;K$ occupied clusters (most components are empty). We can combine these empty components together:<br>$ p(z_i=k|\text{ others }) = \frac{n_k^{-i}+ \frac{\alpha}{K}}{n-1+\alpha}f(x_i|z_j=k)$<br>$ p(z_i=k_{empty}|\text{ others }) = \frac{\alpha\frac{K-K^*}{K}}{n-1+\alpha}f(x_i|\{\})$<br>When $K \rightarrow \infty$, we get:<br>$ p(z_i=k|\text{ others }) = \frac{n_k^{-i}}{n-1+\alpha}f(x_i|z_j=k)$<br>$ p(z_i=k_{empty}|\text{ others }) = \frac{\alpha}{n-1+\alpha}f(x_i|\{\})$</p>
<ul>
<li>The actual infinite limit of the finite mixture model has problems: any particular cluster will get a mixing proportion of 0.</li>
<li>Better way for the infinite limit thinking:<ol>
<li>Chinese restaurant process</li>
<li>Stick-breaking construction</li>
</ol>
</li>
<li>Both are infinite dimensional Dirichlet distributions of Dirichlet Process(DP) </li>
</ul>
<h3 id="Chinese-Restaurant-Process"><a href="#Chinese-Restaurant-Process" class="headerlink" title="Chinese Restaurant Process"></a>Chinese Restaurant Process</h3><h4 id="Partitions"><a href="#Partitions" class="headerlink" title="Partitions"></a>Partitions</h4><p>A partition $\varrho$ of a set $S$ is</p>
<ul>
<li>A disjoint family of non-empty of subsets $S$ whose union in $S$</li>
<li>e.g. $S = \{ A,B,C,D,E,F,G  \}$</li>
<li>e.g. $\varrho = \{ \{A\},\{B,C\},\{D,E,F,G\}  \}$</li>
<li>The set of all partitions of $S$ is $\mathcal{P}_S$</li>
<li><em>Random partitions</em> are random variables taking values in $\mathcal{P}_S$</li>
</ul>
<h4 id="Chinese-Restaurant-Process-1"><a href="#Chinese-Restaurant-Process-1" class="headerlink" title="Chinese Restaurant Process"></a>Chinese Restaurant Process</h4><ul>
<li>Each customer comes into restaurant and sits at a table<br>$P(\text{sit at table }c) = \frac{n_c}{\alpha+\sum_{c\in \varrho} n_c} $<br>$P(\text{sit at new table}) = \frac{\alpha}{\alpha+\sum_{c\in \varrho} n_c} $</li>
<li>Customers correspond to element $S$, tables correspond to clusters in $\varrho$</li>
<li>Rich-gets-richer: larger clusters are more likely to attract more customers</li>
<li>Multiplying conditional probabilities together, the overall probability of $\varrho$ (exchangeable partition probability function (EPPF)) is<br>$P(\varrho|\alpha)=\frac{\alpha^{|\varrho|}\Gamma(\alpha)}{\Gamma(n+\alpha)}\Pi_{c\in \varrho} \Gamma(|c|) $<br>The distribution over the partition does not relate to the order of the customers enter the restaurant, and it only depends on the number of the clusters and the size of the clusters. </li>
</ul>
<h5 id="Number-of-Clusters"><a href="#Number-of-Clusters" class="headerlink" title="Number of Clusters"></a>Number of Clusters</h5><p>The prior mean and variance of $K$ are:<br>$ E[ \rho| \alpha, n ] = \alpha(\psi(\alpha+n)-\psi(\alpha))\approx \alpha \text{ log}(1+\frac{n}{\alpha}) $<br>$ V[ \rho| \alpha, n ] = \alpha(\psi(\alpha+n)-\psi(\alpha)) + \alpha^2(\psi’(\alpha+n) - \psi’(\alpha)) \approx \alpha \text{ log}(1+\frac{n}{\alpha}) $<br>where $\psi(\alpha) = \frac{\partial }{\partial \alpha}\text{ log }\Gamma(\alpha)$<br><img src="/images/2019/03/crp.png" alt="Sample Image Added via Markdown"></p>
<h4 id="CRP-Mixture-Model"><a href="#CRP-Mixture-Model" class="headerlink" title="CRP Mixture Model"></a>CRP Mixture Model</h4><p>Given a dataset $S$, we want to partition it into clusters of similar items. And we can use CRP prior over partitioins $\varrho$.</p>
<ul>
<li>$\varrho\sim \text{CRP }(\alpha)$</li>
<li>$\theta^*|\varrho \sim H \text{ for } c\in \rho$</li>
<li><p>$x_i|\theta,\varrho \sim F(\theta_c^*) \text{ for }c\in \varrho \text{ with } i\in c$</p>
</li>
<li><p>CRP prior over $\varrho$, iid prior $H$ over cluster parameters</p>
<center><br><img class="left" src="/images/2019/03/crpmm.png" width="40%" height="40%"><br></center>
</li>
<li><p>CRP can thus induce distribution over partitions: </p>
<ul>
<li>$P(\mathbf{z}|\alpha) = \frac{\Gamma(\alpha)}{\Pi_k\Gamma(\alpha/K)}\frac{\Pi_k\Gamma(n_k+\alpha/K)}{\Gamma(\alpha+n)}$ describes a partition of the data set into clusters, and a labelling of<br>each cluster with a mixture component index.</li>
<li>Induces a distribution over partitions $\varrho$ (without labelling) of the data set:<br>$P(\varrho|\alpha) = [K]^k_{-1}\frac{\Gamma(\alpha)}{\Gamma(\alpha+n)} \Pi_{c\in\varrho} \frac{\Gamma(|c|+\alpha/K)}{\Gamma(\alpha/K)}$</li>
<li>Assume $K\rightarrow \infty$ (maximum number of clusters in our partitions),  we get a proper distribution over partitions without a limit on the number of clusters:<br>$P (\varrho|\alpha) \rightarrow \frac{\alpha^{|\varrho|}\Gamma(\alpha)}{\Gamma(\alpha+n)} \Pi_{c\in\varrho} \Gamma(|c|)$ </li>
<li>So the induced distribution over partitions approach Chinese Restaurant Process. </li>
</ul>
</li>
</ul>
<p>In Chinese Restaurant Process is the distribution over partitions of a collection of objects, which can be used to build nonparametric model-based clustering models. It is related to the infinite limit of finite mixture models (solve the limitation of the originial infinite models).</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1] <a href="https://kourouklides.fandom.com/wiki/Bayesian_Nonparametrics" target="_blank" rel="noopener">Bayesian Nonparametrics Fandom Page</a><br>[2] <a href="http://blog.bogatron.net/blog/2014/02/02/visualizing-dirichlet-distributions/" target="_blank" rel="noopener">Visualizing Dirichlet Distributions with Matplotlib</a><br>[3] <a href="https://www.youtube.com/watch?v=dNeW5zoNJ7g" target="_blank" rel="noopener">Bayesian Nonparametrics 1 - Yee Whye Teh - MLSS 2013 Tübingen</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qiuyiwu.github.io/2019/03/07/BayesianNonparametrics/" data-id="cjt39xdkj0001xly5850b5qlx" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Chinese-Restaurant/">Chinese Restaurant</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Indian-Buffet/">Indian Buffet</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Nonparametric/">Nonparametric</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Statistics/">Statistics</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2019/02/19/GaussianProcess/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Why are Gaussian Process Models called Nonparametric?</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Astrostatistics/">Astrostatistics</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Mining/">Data Mining</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/JUJUs/">JUJUs</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/LeetCode/">LeetCode</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Philosophy/">Philosophy</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Research/">Research</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Statistics/">Statistics</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/TensorFlow/">TensorFlow</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/API/">API</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Astronomy/">Astronomy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Astrostatistics/">Astrostatistics</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Birthday/">Birthday</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/">CNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Chinese-Restaurant/">Chinese Restaurant</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Classification/">Classification</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DNN/">DNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Fusion/">Data Fusion</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Mining/">Data Mining</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deep-Learning/">Deep Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Disqus/">Disqus</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Distance/">Distance</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ensemble/">Ensemble</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Firebase/">Firebase</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Firestore/">Firestore</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gaussian-Process/">Gaussian Process</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/">Hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Indian-Buffet/">Indian Buffet</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KMeans/">KMeans</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KNN/">KNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LeanCloud/">LeanCloud</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Network-Analysis/">Network Analysis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neural-Network/">Neural Network</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Nietzsche/">Nietzsche</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Nonparametric/">Nonparametric</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Philosophy/">Philosophy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/R/">R</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/">RNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Regression/">Regression</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/">SVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spatial-Statistics/">Spatial Statistics</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Statistics/">Statistics</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TensorFlow/">TensorFlow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Topic-Modeling/">Topic Modeling</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TwoSum/">TwoSum</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Valine/">Valine</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Vapnik-Chervonenkis/">Vapnik-Chervonenkis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/binary/">binary</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/blog/">blog</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/classification/">classification</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/comment/">comment</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hurrican-tracks/">hurrican tracks</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kNN/">kNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/local-search/">local search</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/music/">music</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/storm-surge/">storm surge</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/visitor-counts/">visitor counts</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/尼采/">尼采</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/API/" style="font-size: 10px;">API</a> <a href="/tags/Astronomy/" style="font-size: 10px;">Astronomy</a> <a href="/tags/Astrostatistics/" style="font-size: 10px;">Astrostatistics</a> <a href="/tags/Birthday/" style="font-size: 10px;">Birthday</a> <a href="/tags/CNN/" style="font-size: 15px;">CNN</a> <a href="/tags/Chinese-Restaurant/" style="font-size: 10px;">Chinese Restaurant</a> <a href="/tags/Classification/" style="font-size: 10px;">Classification</a> <a href="/tags/DNN/" style="font-size: 10px;">DNN</a> <a href="/tags/Data-Fusion/" style="font-size: 10px;">Data Fusion</a> <a href="/tags/Data-Mining/" style="font-size: 10px;">Data Mining</a> <a href="/tags/Deep-Learning/" style="font-size: 10px;">Deep Learning</a> <a href="/tags/Disqus/" style="font-size: 10px;">Disqus</a> <a href="/tags/Distance/" style="font-size: 10px;">Distance</a> <a href="/tags/Ensemble/" style="font-size: 10px;">Ensemble</a> <a href="/tags/Firebase/" style="font-size: 10px;">Firebase</a> <a href="/tags/Firestore/" style="font-size: 10px;">Firestore</a> <a href="/tags/Gaussian-Process/" style="font-size: 10px;">Gaussian Process</a> <a href="/tags/Hexo/" style="font-size: 20px;">Hexo</a> <a href="/tags/Indian-Buffet/" style="font-size: 10px;">Indian Buffet</a> <a href="/tags/KMeans/" style="font-size: 10px;">KMeans</a> <a href="/tags/KNN/" style="font-size: 10px;">KNN</a> <a href="/tags/LeanCloud/" style="font-size: 10px;">LeanCloud</a> <a href="/tags/Network-Analysis/" style="font-size: 10px;">Network Analysis</a> <a href="/tags/Neural-Network/" style="font-size: 10px;">Neural Network</a> <a href="/tags/Nietzsche/" style="font-size: 10px;">Nietzsche</a> <a href="/tags/Nonparametric/" style="font-size: 15px;">Nonparametric</a> <a href="/tags/Philosophy/" style="font-size: 10px;">Philosophy</a> <a href="/tags/Python/" style="font-size: 10px;">Python</a> <a href="/tags/R/" style="font-size: 10px;">R</a> <a href="/tags/RNN/" style="font-size: 15px;">RNN</a> <a href="/tags/Regression/" style="font-size: 10px;">Regression</a> <a href="/tags/SVM/" style="font-size: 10px;">SVM</a> <a href="/tags/Spatial-Statistics/" style="font-size: 10px;">Spatial Statistics</a> <a href="/tags/Statistics/" style="font-size: 20px;">Statistics</a> <a href="/tags/TensorFlow/" style="font-size: 15px;">TensorFlow</a> <a href="/tags/Topic-Modeling/" style="font-size: 15px;">Topic Modeling</a> <a href="/tags/TwoSum/" style="font-size: 10px;">TwoSum</a> <a href="/tags/Valine/" style="font-size: 10px;">Valine</a> <a href="/tags/Vapnik-Chervonenkis/" style="font-size: 10px;">Vapnik-Chervonenkis</a> <a href="/tags/binary/" style="font-size: 10px;">binary</a> <a href="/tags/blog/" style="font-size: 20px;">blog</a> <a href="/tags/classification/" style="font-size: 10px;">classification</a> <a href="/tags/comment/" style="font-size: 10px;">comment</a> <a href="/tags/hurrican-tracks/" style="font-size: 10px;">hurrican tracks</a> <a href="/tags/kNN/" style="font-size: 10px;">kNN</a> <a href="/tags/local-search/" style="font-size: 10px;">local search</a> <a href="/tags/music/" style="font-size: 10px;">music</a> <a href="/tags/storm-surge/" style="font-size: 10px;">storm surge</a> <a href="/tags/visitor-counts/" style="font-size: 10px;">visitor counts</a> <a href="/tags/尼采/" style="font-size: 10px;">尼采</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/03/07/BayesianNonparametrics/">Bayesian Nonparametrics Notes</a>
          </li>
        
          <li>
            <a href="/2019/02/19/GaussianProcess/">Why are Gaussian Process Models called Nonparametric?</a>
          </li>
        
          <li>
            <a href="/2019/01/29/Birthday/">Make a Birthday Cake in R</a>
          </li>
        
          <li>
            <a href="/2019/01/27/BlogTheme/">Blog Theme 日神 x 酒神</a>
          </li>
        
          <li>
            <a href="/2019/01/26/Hexo-View/">Add Article Views to Your Hexo Blog</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Qiuyi Wu<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>