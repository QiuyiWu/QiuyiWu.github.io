<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Reading Notes for Astrostatistics]]></title>
    <url>%2F2018%2F12%2F24%2FNoteAstro%2F</url>
    <content type="text"><![CDATA[I’m starting to read Feigelson &amp; Babu’s Modern Statistical Methods for Astronomy this Christmas and hope to finish it before Spring break in March, 2019. This book covers the fundamental statistics theories and methodologies in application on Astronomy. It also aims to help astronomers perceive megadatas from celestial objects via modern statistical analysis and interpret cosmic phenomena in advanced statistical language. It is the bible for Astrostatistics! I take notes and record here for myself better understanding this fantastic field. IntroductionCollaborations betweeen astronomers and statisticians:California–Harvard Astro-Statistical CollaborationInternational Computational Astrostatistics Group centered in PittsburghCenter for Astrostatistics at Penn State AstronomyTerm:Astronomy: the observational study of matter beyond Earth.Astrophysics: the study of intrinsic nature of astronomical bodies and the processes by which they binteract and evolve. Fields:Planetary Astronomer: study the Solar System and sxtra-solar planetary systemSolar Physicist: study the SunStellar Astronomer: study other starsGalactic Astronomer: study Milky Way GalaxyExtragalactic Astronomer: study other galaxiesCosmologist: study the Universe as a whole StatisticsLimitations of the spectrograph and observing conditions lead to uncertainty in the measured radial velocities. The uncertainty in our knowledge could be due to the current level of understanding of the phenomenon, which could be reduced in the future. The uncertainty of our knowledge could be due to future choices or events. Space and EventOutcome space/sample space: The set of all outcomes $\Omega$ of an experimentEvent: subset of a sample spaceDiscrete sample space: A finite (or countably infinite) sample space Astronomers deal with both countable space (such as the number of stars in the Galaxy, or the set of photons from a quasar arriving at a detector) and uncountable spaces (such as the variability characteristics of a quasar, or the background noise in an image constructed from interferometry observations). Axioms of ProbabilityProbability space: $(\Omega, \mathcal{F}, P)$ where $\Omega$ is sample space, $\mathcal{F}$ is a class of events, and $P$ is function that assigns probability to events in $\mathcal{F}$. Axiom 1: $0\leq P(A) \leq 1$, for all events $A$Axiom 2: $P(\Omega) = 1$Axiom 3: For mutually exclusive (pairwise disjoint) events $A_1, A_2,…,$ we have $P(A_1\cup A_2\cup A_3 …) = P(A_1)+P(A_2)+P(A_3)+…$$\qquad \qquad$ So if for all $i\neq j, A_i \cap A_j = \emptyset$, then $ P( \bigcup_{i=1}^\infty A_i ) = \sum_{i=1}^\infty P(A_i) $The generalization to $n$ events, $E_1,…,E_n$ below is called the inclusion-exclusion formula:$$\begin{align*} P(E_1\cup E_2\cup ...\cup E_n ) &amp;= \sum_{i=1}^\infty P(E_i) - \sum_{ i_1 &lt; i_2 } P(E_{i_1}\cap E_{i_2}) +... \\ &amp;+ (-1)^{r+1} \sum_{ i_1 &lt; i_2 &lt; ... &lt; i_r} P(E_{i_1}\cap E_{i_2} \cap ... \cap E_{i_r}) + ...\\ &amp;+ (-1)^{n+1} P(E_1\cap E_2 \cap ... \cap E_n) \end{align*}$$ Multiplication rule: $P(A_1\cap A_2\cap … \cap A_n ) = P(A_1)\times P(A_2|A_1)…P(A_{n-1}|A_1,…A_{n-2})\times P(A_{n}|A_1,…A_{n-1})$ Except for the rare circumstance when an entirely new phenomenon is discovered, astronomers are measuring properties of celestial bodies or populations for which some distinctive properties are already available. Consider, for example, a subpopulation of galaxies found to exhibit Seyfert-like spectra in the optical band (property A) that have already been examined for nonthermal lobes in the radio band (property B). Then the conditional probability that a galaxy has a Seyfert nucleus given that it also has radio lobes is given by Equation $P(A|B) = \frac{P(A\cap B)}{P(B)}$, and this probability can be estimated from careful study of galaxy samples. The composition of a Solar System minor body can be predominately ices or rock. Icy bodies are more common at large orbital distances and show spectral signatures of water (or other) ice rather than the spectral signatures of silicates. The probability that a given asteroid, comet or Kuiper Belt Object is mostly icy is then conditioned on its semi-major axis and spectral characteristics. To be continue…]]></content>
      <categories>
        <category>Astrostatistics</category>
      </categories>
      <tags>
        <tag>Astrostatistics, Astronomy, Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Data Fusion]]></title>
    <url>%2F2018%2F12%2F20%2FDataFusion%2F</url>
    <content type="text"><![CDATA[Ongoing resaerch projects in SAMSI Program on Model Uncertainty.To be announced in May, 2019. Summary:In fall 2018, I presented my previous work (collaborating with Ernest Fokoue) about music mining in the group meeting, and also submitted a paper [1] to introduce the idea of representing any given piece of music as a collection of “musical words” that we codenamed “muselets”, which are essentially musical words of various lengths. We specifically herein construct a naive dictionary featuring a corpus made up of African American, Chinese, Japanese and Arabic music, on which we perform both topic modelling and pattern recognition. Although some of the results based on the Naive Dictionary are reasonably good, we anticipate phenomenal predictive performances once we get around to actually building a full scale complete version of our intended dictionary of muselets. The idea of Data Fusion in this work is that we create uniform representation of music based on different sources and forms of musical data.In spring 2019, I will collaborate with Dr. Jong-Min Kim who extensively studies Mixture of D-vine copulas [2]. We plan to combine text mining and copula methods in the application of movie markets. Specifically, we study Chinese and American movie markets, to see the effects of different types of movies on the returns. For the most profitable movies, we study multiple effects that make those movies win the sales (potential factors: movie types, movie director, actors, actress etc.). [1] Wu, Qiuyi; Fokoue, Ernest. (2018) Naive Dictionary On Musical Corpora: From Knowledge Representation To Pattern Recognition. arXiv:1811.12802[2] Kim, D., Kim, J. M., Liao, S. M., &amp; Jung, Y. S. (2013). Mixture of D-vine copulas for modeling dependence. Computational Statistics &amp; Data Analysis, 64, 1-19. All rights reserved &copy; Copyright 2018, Qiuyi Wu.]]></content>
      <categories>
        <category>Research</category>
      </categories>
      <tags>
        <tag>Data Fusion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Music Mining]]></title>
    <url>%2F2018%2F12%2F20%2FMusicMining%2F</url>
    <content type="text"><![CDATA[Music and text are similar in the way that both of them can be regraded as information carrier and emotion deliverer. People get daily information from reading newspaper, magazines, blogs etc., and they can also write diary or personal journal to reflect on daily life, let out pent up emotions, record ideas and experience. Same power could come from music! Composers express their feelings through music with different combinations of notes, diverse tempo, and dynamics levels, as another version of language. All these similarities drive people to ask questions like: Could music deliver information tantamount to text? Can we efficiently use text mining approach in music field? Why music from diverse culture can bring people so many different feelings? What’s the similarity between music from different culture, or composers, or genres? To what extend do people grasp the meaning behind each piece of music expressed by the composer? Take the tragedy Titanic as an example, we learn the tragedy from the newspaper and feel anguished, but we can also get the mourning from the song My Heart Will Go On. The melody contains a lot of minor keys (e.g. $D\flat$, $F\sharp$, $A\flat$), which are more likely to trigger the dissonance via two closely spaced notes hitting the ear simultaneously and thus to make people feel sad. In this project we employ latent Dirichlet allocation model into the music concept. Assume an album, as a collection of songs, are the mixture of different topics (melodies). These topics are the distributions over a series of notes (left part of the figure). In each song, notes in every measure are chosen based on the topic assignments (colorful tokens), while the topic assignments are drawn from the document-topic distribution. More details can be found in my paper and thesis. All rights reserved &copy; Copyright 2018, Qiuyi Wu. Recommended Citation:[1] Wu, Qiuyi, and Ernest Fokoue. “Naive Dictionary On Musical Corpora: From Knowledge Representation To Pattern Recognition.” arXiv preprint arXiv:1811.12802 (2018). [2] Wu, Qiuyi, “Statistical Aspects of Music Mining: Naive Dictionary Representation” (2018). Thesis. Rochester Institute of Technology. Accessed from https://scholarworks.rit.edu/theses/9932]]></content>
      <categories>
        <category>Research</category>
      </categories>
      <tags>
        <tag>Music, Data Mining, Topic Modeling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm Surge]]></title>
    <url>%2F2018%2F12%2F19%2FStormSurge%2F</url>
    <content type="text"><![CDATA[Ongoing resaerch projects in SAMSI Program on Model Uncertainty.To be announced in May, 2019. Mid-term summary:In fall 2018, I mainly focused on input distribution subgroup demonstrated my initial exploratory analysis of KE synthetic storm tracks, and compared the simulated tracks with the real IBTrack storm data. One of the main discussions in this semester is how to improve the current practice with the technique of spatial statistics, such as using hierarchical model to improve the estimation of input distribution, or spatial-temporal point process modeling for the storm occurrence rate. Diverse existing projects giving by different researchers are shared and discussed in weekly group meetings.For spring 2019, we will (1.) Measure the model of storm evolution (e.g. the sudden change of the characteristics when the storms landed based on the coastline and how simulation data handle this); (2.) Think about how to impose some structure to improve the estimation concerning input distribution. All rights reserved &copy; Copyright 2018, Qiuyi Wu.]]></content>
      <categories>
        <category>Research</category>
      </categories>
      <tags>
        <tag>Spatial Statistics, Storm Surge</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Research on Public Policy Blogs]]></title>
    <url>%2F2018%2F06%2F05%2FPublic-Policy-Blogs%2F</url>
    <content type="text"><![CDATA[Use different Topic Modeling approaches on Political Blogs to see the performance of diverse methods. IntroductionTypes of Models in Comparison General LDA (R package) Supervised LDA (David M. Blei, Jon D. McAuliffe) Relational Topic Model (Jonathan Chang, David M. Blei) Topic Link Block Model (Derek Owens-Oas) Poisson Factor Modeling (Beta Negative Binomial Process Topic Model) Dynamic Text Network Model (Teague Henry, David Banks et al.) Key Values in Cleaned Blog PostsAfter preprocessing the text extracted from blog posts: dates: string of the given date in mm/dd/yy format domains: string of the blog website where post was found (remove “www.”) links: string of other websites occured in the post as hyperlinks (sorted alphabetically) words: filtered words from raw text in the blog posts (TFIDF variance threading used) rawText: direct content from blog posts (remove short posts and duplicate posts ) words_stem: stemmed words using Hunspell stemmer (e.g., apples -&gt; apple) Analysis via several Topic Modeling MethodsGeneral LDAGeneral LDA Model via Collapsed Gibbs Sampling Methods for Topic Models: Supervised LDAHere use Blog Site as labels. Relational Topic ModelRTM models the link as binary random variable that is conditioned on their text. The model can predict links between documents and predict words within them. The algorithm is based on variational EM algorithm. For each document $d$: Draw topic proportions $\theta_d|\alpha \sim \text{Dir}(\alpha)$ For each word $w_{d,n}$: Draw assignment $z_{d,n}|\theta_d \sim \text{Mult}(\theta_d)$ Draw word wd,n | zd,n, $\beta$1:K$\sim \text{Mult}(\beta$zd,n$)$ For each pair of documents $d,d’$: Draw binary link indicator $y|z_d,z$ d’ $\sim \psi (\cdot | z_d,z$ d’ $)$ Compare the performance of link prediction with the one of LDA. The plot below shows the predicted link probabilities from RTM against the ones of LDA for each document, and also shows the most expressed topics by the cited document. (sample 100) All rights reserved &copy; Copyright 2018, Qiuyi Wu.]]></content>
      <categories>
        <category>Research</category>
      </categories>
      <tags>
        <tag>Topic Modeling, Network Analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow for Deep Learning 3]]></title>
    <url>%2F2018%2F01%2F22%2FTensorFlow-for-Deep-Learning-3%2F</url>
    <content type="text"><![CDATA[Continue the learning process of Convolutional Neural Networks and Recurrent Neural Networks with TensorFlow in Jupyter Notebook. RNN with TensorFlow1234567891011121314151617181920212223242526272829303132import numpy as npimport tensorflow as tfimport matplotlib.pyplot as plt%matplotlib inline# class: create the data, generate the batches to send it backclass TimeSeriesData(): def __init__(self, num_points, xmin, xmax): self.xmin = xmin self.xmax = xmax self.num_points = num_points self.resolution = (xmax - xmin)/num_points self.x_data = np.linspace(xmin, xmax, num_points) self.y_true = np.sin(self.x_data) def ret_true(self, x_series): return np.sin(x_series) def next_batch(self, batch_size, steps, return_batch_ts = False): # grab a random starting point for each batch random_start = np.random.rand(batch_size,1) # convert the data to TS ts_start = random_start * (self.xmax - self.xmin - (steps * self.resolution)) # create batch time series on the x axis batch_ts = ts_start + np.arange(0.0,steps+1) * self.resolution # create the Y data for the time series x axis from previous step y_batch = np.sin(batch_ts) # formatting for RNN if return_batch_ts: return y_batch[:,:-1].reshape(-1,steps,1), y_batch[:,1:].reshape(-1,steps,1), batch_ts else: return y_batch[:,:-1].reshape(-1,steps,1), y_batch[:,1:].reshape(-1,steps,1) # original y_batch and y_batch shifted over 1 step in the future 123# Give the data and try the model!ts_data = TimeSeriesData(250,0,10)plt.plot(ts_data.x_data, ts_data.y_true) 123456num_time_steps = 60# Set 1 batch, 60 steps, return batch time series on x axisy1, y2, ts = ts_data.next_batch(1, num_time_steps, True)plt.plot(ts.flatten()[1:], y2.flatten(), '*')# ts.flatten(): Return a copy of the array collapsed into one dimension.# ts has total 251 points (including the prediction), so start from 1, to match y2 1234plt.plot(ts_data.x_data, ts_data.y_true, label ='Sin(t)')plt.plot(ts.flatten()[1:], y2.flatten(), '*', label = 'Single Training Example')plt.legend()plt.tight_layout() # Automatically adjust subplot parameters to give specified padding. 1234567# Demonstrate what's going on of the training in the model, shift over 1 stepnum_time_steps = 30train_example = np.linspace(5, 5+ts_data.resolution*(num_time_steps + 1), num_time_steps + 1)plt.title('A Traning Example')plt.plot(train_example[:-1], ts_data.ret_true(train_example[:-1]), 'bo', markersize = 15, alpha = 0.5, label = 'Example')plt.plot(train_example[1:], ts_data.ret_true(train_example[1:]), 'ko', markersize = 7, label = 'Target')plt.legend() 123456789101112131415161718192021222324252627282930313233# Create the modeltf.reset_default_graph()num_inputs = 1num_neurons = 100num_outputs = 1learning_rate = 0.0001num_train_iterations = 2000batch_size = 1# PlaceholderX = tf.placeholder(tf.float32, [None, num_time_steps, num_inputs])y = tf.placeholder(tf.float32, [None, num_time_steps, num_outputs])# Run cell layercell_input = tf.contrib.rnn.BasicRNNCell(num_units = num_neurons, activation = tf.nn.relu) # Can try other cell: GRUCell etc.cell = tf.contrib.rnn.OutputProjectionWrapper(cell_input, output_size = num_outputs)outputs, states = tf.nn.dynamic_rnn(cell, X, dtype = tf.float32)# MSEloss = tf.reduce_mean(tf.square(outputs-y))optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)train = optimizer.minimize(loss)init = tf.global_variables_initializer()# Sessionsaver = tf.train.Saver()with tf.Session() as sess: sess.run(init) for iteration in range(num_train_iterations): X_batch, y_batch = ts_data.next_batch(batch_size, num_time_steps) sess.run(train, feed_dict = &#123;X: X_batch, y: y_batch&#125;) if iteration % 100 == 0: mse = loss.eval(feed_dict = &#123;X: X_batch, y: y_batch&#125;) print(iteration, '\tMSE', mse) saver.save(sess, './rnn_time_series_model_codealong') 123456789101112131415161718# predict 1 step in the futurewith tf.Session() as sess: saver.restore(sess, './rnn_time_series_model_codealong') X_new = np.sin(np.array(train_example[:-1].reshape(-1,num_time_steps, num_inputs))) y_pred = sess.run(outputs, feed_dict = &#123;X: X_new&#125;)plt.title('Test The Model')# Training exampleplt.plot(train_example[:-1], np.sin(train_example[:-1]),'bo', markersize = 15, alpha = 0.5, label = 'Training Example')# Target to predictplt.plot(train_example[1:], np.sin(train_example[1:]),'ko', markersize = 10, label = 'Target')# Model predictionplt.plot(train_example[1:], y_pred[0,:,0], 'r.', markersize = 10, label = 'Prediction')plt.xlabel('Time')plt.legend()plt.tight_layout() To further explore the performance of RNN model, we can change the number of iteration num_train_iterations, learning rate learning_rate, and model type tf.contrib.rnn.BasicRNNCell() and then play with it.About just for time sequence that shifts 1 time step ahead, now we’ll generate a completely new sequence.123456789101112131415with tf.Session() as sess: saver.restore(sess, './rnn_time_series_model_codealong') # seed zeros # 30 0s and the generated data zero_seq_seed = [0.0 for i in range(num_time_steps)] for iteration in range(len(ts_data.x_data) - num_time_steps): X_batch = np.array(zero_seq_seed[-num_time_steps:]).reshape(1, num_time_steps,1) y_pred = sess.run(outputs, feed_dict = &#123;X: X_batch&#125;) zero_seq_seed.append(y_pred[0,-1,0]) plt.plot(ts_data.x_data, zero_seq_seed, 'b-')plt.plot(ts_data.x_data[:num_time_steps], zero_seq_seed[:num_time_steps], 'r', linewidth =3)plt.xlabel('Time')plt.ylabel('Y') Instead of zeros at the beginning, now use training example, other parts remain the same1234567891011121314with tf.Session() as sess: saver.restore(sess, './rnn_time_series_model_codealong') # 30 training points and the generated data training_example = list(ts_data.y_true[:30]) for iteration in range(len(ts_data.x_data) - num_time_steps): X_batch = np.array(training_example[-num_time_steps:]).reshape(1, num_time_steps,1) y_pred = sess.run(outputs, feed_dict = &#123;X: X_batch&#125;) training_example.append(y_pred[0,-1,0]) plt.plot(ts_data.x_data, training_example, 'b-')plt.plot(ts_data.x_data[:num_time_steps], training_example[:num_time_steps], 'r', linewidth =3)plt.xlabel('Time')plt.ylabel('Y')]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>CNN, RNN, API</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow for Deep Learning 2]]></title>
    <url>%2F2018%2F01%2F21%2FTensorFlow-for-Deep-Learning-2%2F</url>
    <content type="text"><![CDATA[Here I’ll give the theory part of Neural Networks, sepecifically three kinds of NN: Normal Neural Networks, Convolutional Neural Networks, Recurrent Neural Networks. Neural NetworksIn single neuron:$$\begin{align*} z &amp;= Wx + b \\ a &amp;= \sigma (z) \end{align*}$$ Activation Function Perceptron: binary classifier, small changes are not reflected.$$\begin{align*} f(x) = \begin{cases} 1 &amp; \text{if } Wx+b&gt;0\\ 0 &amp; \text{otherwise} \end{cases} \end{align*}$$ Sigmoid: special case of the logistic function with S shape curve, more dynamic$$\begin{align*} S(x) = \frac{1}{1 + e^{-x}} = \frac{e^x}{e^x +1} \end{align*}$$ Hyperbolic Tangent: Tanh(z)$$\begin{align*} cosh x &amp;= \frac{e^x + e^{-x}}{2}\\ sinh x &amp;= \frac{e^x - e^{-x}}{2}\\ tanh x &amp;= \frac{cosh x}{sinh x } \end{align*}$$ ReLU (Rectified Linear Unit): max(0,z)ReLU and tanh tend to have the best performance. Softmax Regression: $$\begin{align*} z_i &amp;= \sum_jW_{i,j}x_j + b_i \\ softmax(z)_i &amp;= \frac{\text{exp}(z_i)}{\sum_j \text{exp}(z_j)} \end{align*}$$ Cost/Loss FunctionCost function is the measurement of the error.$$\begin{align*} z &amp;= Wx + b \\ a &amp;= \sigma (z)\\ C &amp;= \frac{1}{n}\sum(y_{true} - a)^2 &amp;\text{Quadratic Cost}\\ C &amp;= -\frac{1}{n}\sum(y_{true}\cdot ln(a) + (1-y_{true})\cdot ln(1-a) ) &amp;\text{Cross Entropy} \end{align*}$$Quadratic Cost:The larger errors are more prominent due to the squaring. It causes a slowdown in learning speed.Cross Entropy:It allows for faster learning. The larger the difference, the faster the neuron can learn. Gradient Descent &amp; BackpropagationGradient Descent is an optimization algorithm for finding the minimum of a function. Here it minimizes the error to find the optiml value. 1-D example below shows the best parameter value (weights of the neuron inputs) we should choose to minimize the cost. For complicated cases more than 1 dimension, we use biilt-in algebra of Deep learning library to get the optimal parameters.1. Learning Rate: defines the step size during gradient descent, too small - slow pace, too small - overshooting2. Batch Size: batches allow us to use stochastic gradient descent, in case the datasets are large, if all the them are fed at once the computation would be very expensive. Too small - less representative of data, too large - longer training time3. Second-Order Behavior of Gradient Descent: adjust the learning rate based on the rate of descent(second-order behavior: derivative),large learning rate at the beginning, adjust to slower learning rate as it get closer. Methods: AdaGrad, RMSProp, AdamVanishing Gradients: when increasing the number of layers in a network, the layers towards the input will be affected less by the error calculation occuring at the output as going backwards throught the network. Initialization and Normalization will help to mitigate the issue.Backpropagation is to calculate the error contribution of each neuron after a batch of data is processed. It works by calculating the error at the output and then distributes back through the network layers. It belongs to supervised learning as it requires a known output for each input value. The mathematical detail is showed below from Andrew Ng’s Neural Networks and Deep Learning in Coursera. Initialization of WeightsZerosNo randomness (too subjective) so not a good choice Random DistributionRandom distribution near zero is not optimal and results in activaion function distorition (distorted to large values) Xavier (Glorot) InitializationThe weights drawn from uniform or normal distribution, with zero mean and specific variance $\text{Var}(W) =\frac{1}{n_{in}} $:$$\begin{align*} &amp;Y = W_1X_1 +W_2X_2 + ... + W_nX_n\\ &amp;\text{Var}(W_iX_i) = E[X_i]^2\text{Var}(W_i) + E[W_i]^2\text{Var}(X_i) + \text{Var}(W_i)\text{Var}(X_i)\\ &amp;\text{Var}(W_iX_i) = \text{Var}(W_i)\text{Var}(X_i) \qquad (\because E[X_i] = 0)\\ &amp;\text{Var}(Y) = \text{Var}(W_1X_1 + W_2X_2 +... + W_nX_n) = n \text{Var}(W_i)\text{Var}(X_i) \\ &amp;\because \text{Variance of the output is equal to the variance of the input}\\ &amp;\therefore n\text{Var}(W_i) = 1\\ &amp;\therefore \text{Var}(W_i) = \frac{1}{n} = \frac{1}{n_{in}} = \frac{2}{n_{in} + n_{out} } \end{align*}$$ Overfitting IssueWith potentially hundreds of parameters in a deep learning neural network, the possibility of overfitting is very high. We can mitigate this issue by the following ways: $L_1/L_2$ RegularizationAdd a penalty for a larger weights in the model (not unique to neural networks) DropoutRemove neurons during training randomly so that the network does not over rely on any particular neuron (unique to neural networks) Expanding DataArtificially expand data via adding noise, tilting images Convolutional Neural NetworksTensorTensor: N-dimensional arrays:Scalar: 3Vector: [3,4,5]Matrix: [[3,4],[5,6],[7,8]]Tensor: [[[1,2], [3,4]], [[5,6], [7,8]]]We use tensors to feed in sets of images into the model - (I,H,W,C)I: ImagesH: Height of Image in PixelsW: Width of Image in PixelsC: Color Channels: 1 - Grayscale, 3 - RGB DNN vs CNNConvolutionThe left figure is densely connected layer, every neuron in the layer is directly connected to the every neuron in the next layer. While each unit in the convolutional layer is connected to a smaller number of nearby units in the next layer. The reason for the idea of CNN is that most images are at least 256 by 256 pixels or greater (MNIST only 28 by 29 pixels, 784 total). So there are too many parameters unscalable to new images. Another merit of CNN is for image processing, pixels nearby to each other are much more correlated to each other for image detection. Each CNN layer looks at an increasingly larger part of the image. And having units only connected to nearby units helps invariance. CNN helps limit the search of weights to the size of the convolution. Convolutional layers are only connected to pixels in their respective fields. By adding a padding of zeros around the image we can fix the issue for edge neurons where there may not be an input for them.Take the example of 1-D Convolution, we treat the weights as a filter for edge detection, then expand one filters to multiple filters. Stride 1 means 1 unit at a time. The top circle means the zero padding added to include more edge pixels. Each filter detects a different feature.Now for simplicity, the sets of neurons are visualized as blocks.For 2-D Images and Color Images: More Info: Image Kernals SubsamplingExcept convolutional layers, there’s another kind of layers called pooling layers. Pooling layers will subsample the input image to reduce the memory use and computer load as well as reducing the number of parameters.Take example of MNIST, only select max value to the next layer, and move over by stride. So the pooling layer will finally remove a lot of information. Another technique is “Dropout”. Dropout is regarded as a form of regularization as during training, units are randomly dropped with their connection to help prevent overfitting. Recurrent Neural NetworksCommon Neural Networks can handle classification and regression problems, but for sequence information, we need Recurrent Neural Networks.Normal Neural Networks just aggregation of inputs and pass the activation function to get the output. Recurrent Neural Networks send output back to itself.Here I have to mention my previous research work – Echo State Networks, also belong to RNN: ESN Cells that are a function of inputs from previous time steps are also know as memory cells.]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow, DNN, CNN, RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow for Deep Learning 1]]></title>
    <url>%2F2018%2F01%2F16%2FTensorflow-for-Deep-Learning-1%2F</url>
    <content type="text"><![CDATA[I’m learning how to use Google’s TensorFlow framework to create artificial neural networks for deep learning with Python from Udemy. I’m using Jupiter notebook to practice and blog my learning progress here. TensorFlow is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. This architecture allows users to deploy computation to one or more CPUs or GPUs, in a desktop, server, or mobile device with a single API (Application programming interface). Install TensorFlow EnvironmentDownload Anaconda Distribution for Python 3.6 Version. Tutorial comes in that page.For MacOS: Open Terminal, use cd to the target directoryCreate the environment file: run conda env create -f tfdl_env.ymlActivate the file: run source activate tfdeeplearningNow you are in the virtual environment of tfdeeplearningIf you want to get out of this, run source deactivate Note: How is Anaconda related to Python?Anaconda is a python and R distribution. It aims to provide everything you need (python wise) for data science “out of the box”.It includes: The core python language 100+ python “packages” (libraries) Spyder (IDE/editor - like pycharm) and Jupyter conda, Anaconda’s own package manager, used for updating Anaconda and packages Also Anaconda is used majorly for the data science. which manipulates large datasets based on statistical methods. ie. Many statistical packages are already available in anaconda libraries(packages) Vanilla python installed from python.org comew with standard library is okay, in which case using pip to install manually (which comes with most python dists and you should have it if you downloaded from python.org).Learn more: Anaconda overview; Python 3 tutorial TensorFlow Basic Syntax12import tensorflow as tfprint(tf.__version__) Output: 1.3.0 Create a tensor ($\approx$ n-dimension array), the basic one (constant)123hello = tf.constant('Hello')world = tf.constant('World')type (hello) Output: tensorflow.python.framework.ops.Tensor Run this operation inside of a session:123with tf.Session() as sess: result = sess.run(hello + world)print(result) Output: b&#39;HelloWorld&#39;Note:Use with is to make sure we don’t close the session until we run a block of code then close the session.The b character prefix signifies that HelloWorld is a byte string, use result.decode(&#39;utf-8&#39;) to convert byte str to str. 12345a = tf.constant(10)b = tf.constant(20)with tf.Session() as sess: c = sess.run( a + b )print(c) Output: 30 Numpy Operations:123456789101112const = tf.constant(10) # constant operationfill_mat = tf.fill((4,4),10) # matrix operation 4x4myzeros = tf.zeros((4,4)) # 4x4 zeorsmyones = tf.ones((4,4)) # 4x4 onesmyrandn = tf.random_normal((4,4), mean = 0, stddev = 1.0) # random normal distribution myrandu = tf.random_uniform((4,4), minval = 0, maxval = 1.0) # random uniform distribution my_ops = [const, fill_mat, myzeros, myones, myrandn, myrandu]sess = tf.InteractiveSession() # Interactive Sessionfor op in my_ops: print(sess.run(op),'\n') Note:Interactive Session is particularly useful for Jupyter Notebook, it allows you to constantly call it through multiple cells.In the Interactive Session, instead of using sess.run(op), we can use op.eval(), it means “evaluate this operation”, and generates the same result. Matrix multiplication (common in neural networks)1234a = tf.constant([[1,2],[3,4]])b = tf.constant([[10],[100]])result = tf.matmul(a,b)sess.run(result) $$\begin{align*} \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix} \times \begin{bmatrix} 10 \\ 100 \end{bmatrix} = \begin{bmatrix} 210 \\ 430 \end{bmatrix} \end{align*}$$ Note: sess.run(result) can also use result.eval() instead. TensorFlow GraphsGraphs are sets of connected nodes(vertices), and the connections are called edges. In TensorFlow each node is an operation with possible inputs that can supply some outputs. When TenserFlow is started, a default graph is created. 12print(tf.get_default_graph())print(tf.Graph()) Output:fixed default graph: &lt;tensorflow.python.framework.ops.Graph object at 0x11f4c92b0&gt;Dynamic random graph: &lt;tensorflow.python.framework.ops.Graph object at 0x11f707dd8&gt; 1234graph_one = tf.Graph()graph_two = tf.get_default_graph()with graph_one.as_default(): print(graph_one is tf.get_default_graph()) Output: True1print(graph_one is tf.get_default_graph()) Output: False Variables and PlaceholdersVariables and Placeholders are two main types of tensor objects in a Graph. During the optimization process, TensorFlow tunes the parameters of the model. Variables can hold the values of weights and biases throughout the session. Variables need to be initialized. Placeholders are initially empty and are used to feed in the actual training examples. However they do need a declared expected data type (tf.float32) with an optiional shape argument.12345678### Variable ###sess = tf.InteractiveSession()my_tensor = tf.random_uniform((4,4), 0,1)my_var = tf.Variable(initial_value = my_tensor) # give value to variable # sess.run(my_var) cannot be directly run here, need initialized first!init = tf.global_variables_initializer()sess.run(init)sess.run(my_var) # Now it's the time! Output: array([[ 0.27756679, 0.82726526, 0.80544853, 0.43891859], [ 0.56279469, 0.57444489, 0.82595968, 0.63165414], [ 0.16034544, 0.86095798, 0.74416387, 0.17536163], [ 0.44427669, 0.69035304, 0.55842543, 0.00723565]], dtype=float32) 1234### Placeholder ###ph = tf.placeholder(tf.float32, shape = (4,4)) ph = tf.placeholder(tf.float32, shape = (None,5)) # can be fed in the actually number of sampesph = tf.placeholder(tf.float32) # no shape TensorFlow Neural NetworkCreate a neuron that performs a simple linear fit to some 2-D data. Graph of $wx+b = z$ Build a Graph Initiate the Session Feed Data In and get Output Add in the cost function in order to train the network to optimize the parameters1234567891011121314151617181920import numpy as npimport tensorflow as tf### Build a simple Graph ###np.random.seed(101)tf.set_random_seed(101)rand_a = np.random.uniform(0,100, (5,5))rand_b = np.random.uniform(0,100, (5,1))a = tf.placeholder(tf.float32) # default shapeb = tf.placeholder(tf.float32) # default shape# Two ways to do the operation: #tf.add(a, b) | tf.multiply(a, b) | tf.matmul(a, b) add_op = a + bmul_op = a * bwith tf.Session() as sess: add_result = sess.run(add_op, feed_dict = &#123; a:rand_a, b:rand_b&#125;) print(add_result, '\n') mul_result = sess.run(mul_op, feed_dict = &#123; a:rand_a, b:rand_b&#125;) print(mul_result) Output:`[[ 151.07165527 156.49855042 102.27921295 116.58396149 167.95948792] [ 135.45622253 82.76316071 141.42784119 124.22093201 71.06043243] [ 113.30171204 93.09214783 76.06819153 136.43911743 154.42727661] [ 96.7172699 81.83804321 133.83674622 146.38117981 101.10578918] [ 122.72680664 105.98292542 59.04463196 67.98310089 72.89292145]] [[ 5134.64404297 5674.25 283.12432861 1705.47070312 6813.83154297] [ 4341.8125 1598.26696777 4652.73388672 3756.8293457 988.9463501 ] [ 3207.8112793 2038.10290527 1052.77416992 4546.98046875 5588.11572266] [ 1707.37902832 614.02526855 4434.98876953 5356.77734375 2029.85546875] [ 3714.09838867 2806.64379883 262.76763916 747.19854736 1013.29199219]]` 1234567891011121314151617### Build a Neural Network ### n_features = 10 # input layern_dense_neurons = 3 # hidden layerx = tf.placeholder(tf.float32, (None, n_features))W = tf.Variable(tf.random_normal([n_features, n_dense_neurons]))b = tf.Variable(tf.ones([n_dense_neurons]))xW = tf.matmul(x, W) z = tf.add(xW, b)# a = tf.nn.relu | tf.tahna = tf.sigmoid(z)init = tf.global_variables_initializer()with tf.Session() as sess: sess.run(init) # DON'T FORGET! layer_out = sess.run(a, feed_dict = &#123; x: np.random.random([1,n_features])&#125;)print(layer_out) Output: [[ 0.81314439 0.98195159 0.73793817]] 12345678### Simple Regression Example ###x_data = np.linspace(0,10,10) + np.random.uniform(-1.5,1.5,10) # with noisey_label = np.linspace(0,10,10) + np.random.uniform(-1.5,1.5,10)import matplotlib.pyplot as plt%matplotlib inline # %matplotlib inline just for Jupyter notebook plt.plot(x_data, y_label, '*') Now I want the neural network to solve $ y = mx + b$ with 10 training steps:123456789101112131415161718192021222324252627# create two random variablesm = tf.Variable(0.44)b = tf.Variable(0.87)error = 0 # create an error starting from 0for x,y in zip(x_data, y_label): # zip makes a list of x,y y_hat = m * x + b error += (y - y_hat) ** 2 # cost functionoptimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.001)train = optimizer.minimize(error)init = tf.global_variables_initializer()with tf.Session() as sess: sess.run(init) training_steps = 10 for i in range(training_steps): sess.run(train) final_slope, final_intercept = sess.run([m,b])# Test x_test = np.linspace(-1,11,10)y_pred = final_slope * x_test + final_intercept plt.plot(x_test, y_pred, 'r')plt.plot(x_test, y_label,'*') TensorFlow RegressionRegression example with huge dataset:12345678910111213141516import numpy as npimport pandas as pdimport matplotlib.pyplot as plt%matplotlib inlineimport tensorflow as tfx_data = np.linspace(0.0,10.0,1000000) # huge datasetnoise = np.random.randn(len(x_data)) # add noise# y = mx + b | b = 5, m = 0.5y_true = (0.5 * x_data) + 5 + noisex_df = pd.DataFrame(data = x_data, columns = ['X Data'])y_df = pd.DataFrame(data = y_true, columns = ['Y'])my_data = pd.concat([x_df, y_df], axis = 1) # axis = 1 column concatenatemy_data.sample(n = 25) # random 25 samplesmy_data.sample(n=250).plot(kind = 'scatter', x = 'X Data', y = 'Y') For real cases, usually the datasets are humongous and we do not use all of them at once. Instead, we use batch method to feed the data batch by batch into the network to save the time. The number of batches depends on the size of the data. Here we feed 1000 batches of data, with each batch has 8 corresponding data points (x data points with corresponding y labels). To make batches useful, we grab 8 random data points via rand_ind = np.random.randint(len(x_data), size = batch_size), then train the optimizer.123456789101112131415161718192021222324252627batch_size = 8m = tf.Variable(0.81)b = tf.Variable(0.17)xph = tf.placeholder(tf.float32,[batch_size])yph = tf.placeholder(tf.float32,[batch_size])y_model = m * xph + b # the Graph I want to compute# Loss functionerror = tf.reduce_sum(tf.square(yph - y_model)) # tf.reduce_sum: Computes the sum of elements across dimensions of a tensor.# Optimizeroptimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.001) # This is gradient decent optimizer, there are many other optimizerstrain = optimizer.minimize(error)init = tf.global_variables_initializer()with tf.Session() as sess: sess.run(init) batches = 1000 for i in range(batches): rand_ind = np.random.randint(len(x_data), size = batch_size) # random index feed = &#123;xph: x_data[rand_ind], yph: y_true[rand_ind]&#125; sess.run(train, feed_dict = feed) model_m, model_b = sess.run([m, b])y_hat = x_data * model_m + model_bmy_data.sample(250).plot(kind = 'scatter', x = 'X Data', y = 'Y')plt.plot(x_data, y_hat, 'r') TensorFlow Estimator APIThis section is to solve the regression task with TensorFlow estimator API. Wait a minute, what is API? Technically, API stands for Application Programming Interface, but still, what is that? Basically, it is part of the server, that can be distinctively separated from its environment, that receives requests and sends responses. Here is an excellent article from Petr Gazarov explaining API.There are many other higher levels of APIs (Keras, Layers…). The tf.estimator API has several model types to choose from: tf.estimator.LinearClassifier: Constructs a linear classification model tf.estimator.LinearRegressor: Constructs a linear regression model tf.estimator.DNNClassifier: Constructs a neural network classification model tf.estimator.DNNRegressor: Constructs a neural network regression model tf.estimator.DNNLinearCombinedRegressor: Constructs a neural network and linear combined regression model Steps for using the Estimator API: Define a list of feature columns Create the Estimator Model Create a Data Input Function Call train, evaluate, and predict methods on the estimator object 12345678910111213141516171819feat_cols = [tf.feature_column.numeric_column('x', shape = 1)] # set all feature columnsestimator = tf.estimator.LinearRegressor(feature_columns = feat_cols) # set estimator# Split the datafrom sklearn.model_selection import train_test_splitx_train, x_test, y_train, y_test = train_test_split(x_data, y_true, test_size = 0.3, random_state = 101)# Set up Estimator Inputsinput_func = tf.estimator.inputs.numpy_input_fn(&#123;'x': x_train&#125;, y_train, batch_size = 8, num_epochs = None, shuffle = True) # we can also use "tf.estimator.inputs.pandas_input_fn"train_input_func = tf.estimator.inputs.numpy_input_fn(&#123;'x': x_train&#125;, y_train, batch_size = 8, num_epochs = 1000, shuffle = False) # change num_epochs and shuffletest_input_func = tf.estimator.inputs.numpy_input_fn(&#123;'x': x_test&#125;, y_test, batch_size = 8, num_epochs = 1000, shuffle = False) # change num_epochs and shuffle, and the dataset# Train the estimatorestimator.train(input_fn = input_func, steps = 1000)# Evaluationtrain_matrics = estimator.evaluate(input_fn = train_input_func, steps = 1000)test_matrics = estimator.evaluate(input_fn = test_input_func, steps = 1000)print('Training data matrics')print(train_matrics)print('Test data matrics')print(test_matrics) Output:Training data matrics {&#39;loss&#39;: 8.7310658, &#39;average_loss&#39;: 1.0913832, &#39;global_step&#39;: 1000} Test data matrics {&#39;loss&#39;: 8.6690454, &#39;average_loss&#39;: 1.0836307, &#39;global_step&#39;: 1000}This is a good way to check if the model is overfitting (very low loss on training data but very high loss on test data). We want the loss of training data and test data are very close to each other.12345# Get predict valuebrand_new_data = np.linspace(0,10,10)input_fn_predict = tf.estimator.inputs.numpy_input_fn(&#123;'x': brand_new_data&#125;, shuffle = False)estimator.predict(input_fn = input_fn_predict)list(estimator.predict(input_fn = input_fn_predict)) Output:[{&#39;predictions&#39;: array([ 4.43396044], dtype=float32)}, {&#39;predictions&#39;: array([ 5.06833887], dtype=float32)}, {&#39;predictions&#39;: array([ 5.7027173], dtype=float32)}, {&#39;predictions&#39;: array([ 6.33709526], dtype=float32)}, {&#39;predictions&#39;: array([ 6.97147369], dtype=float32)}, {&#39;predictions&#39;: array([ 7.60585213], dtype=float32)}, {&#39;predictions&#39;: array([ 8.24023056], dtype=float32)}, {&#39;predictions&#39;: array([ 8.87460899], dtype=float32)}, {&#39;predictions&#39;: array([ 9.50898743], dtype=float32)}, {&#39;predictions&#39;: array([ 10.14336586], dtype=float32)}]123456# plot the predictionpredictions = []for pred in estimator.predict(input_fn = input_fn_predict): predictions.append(pred['predictions'])my_data.sample(250).plot(kind = 'scatter', x = 'X Data', y = 'Y')plt.plot(brand_new_data, predictions, 'r') TensorFlow ClassificationUse real dataset “Pima Indians Diabetes Dataset”, including both categorical and continuous features, to use tf.estimator switching models – from linear classifier to dense neural network classifier. Linear Classifier12diabetes = pd.read_csv('pima-indians-diabetes.csv')diabetes.head() 1diabetes.columns Output: Index([&#39;Number_pregnant&#39;, &#39;Glucose_concentration&#39;, &#39;Blood_pressure&#39;, &#39;Triceps&#39;, &#39;Insulin&#39;, &#39;BMI&#39;, &#39;Pedigree&#39;, &#39;Age&#39;, &#39;Class&#39;, &#39;Group&#39;], dtype=&#39;object&#39;)12345# normalize the data except the catagorical part (Age, Class, Group)cols_to_norm = ['Number_pregnant', 'Glucose_concentration', 'Blood_pressure', 'Triceps', 'Insulin', 'BMI', 'Pedigree']diabetes[cols_to_norm] = diabetes[cols_to_norm].apply(lambda x: (x - x.min())/(x.max() - x.min()))diabetes.head() 1234567891011121314151617# create feature columns# continuous valuenum_preg = tf.feature_column.numeric_column('Number_pregnant')plasma_gluc = tf.feature_column.numeric_column('Glucose_concentration')dias_press = tf.feature_column.numeric_column('Blood_pressure')tricep = tf.feature_column.numeric_column('Triceps')insulin = tf.feature_column.numeric_column('Insulin')bmi = tf.feature_column.numeric_column('BMI')diabetes_pedigree = tf.feature_column.numeric_column('Pedigree')age = tf.feature_column.numeric_column('Age')# catagorical valueassigned_group = tf.feature_column.categorical_column_with_vocabulary_list('Group', ['A','B','C','D']) # assigned_group = tf.feature_column.categorical_column_with_hash_bucket('Group', hash_bucket_size = 10) # For many groups scenario we can use hash_bucket, as long as we set the hash_bucket_size &gt; # of groupsimport matplotlib.pyplot as plt%matplotlib inlinediabetes['Age'].hist(bins = 20) The distribution of Age from the histgram shows that most of the people are 20s. And instead of treating this variable as a continuous variable, we can bucket these values together, making boundary for each decade or so. Basically we use bucket system to transform continuous variable into categorial variable.12345678age_bucket = tf.feature_column.bucketized_column(age, boundaries = [20,30,40,50,60,70,80])# feature columnsfeat_cols = [num_preg ,plasma_gluc,dias_press ,tricep ,insulin,bmi,diabetes_pedigree ,assigned_group, age_bucket]# train test splitx_data = diabetes.drop('Class', axis = 1) # Class column is Ylabels = diabetes['Class']from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(x_data, labels, test_size = 0.3, random_state = 101) Create the model!12345678## Create the modelinput_func = tf.estimator.inputs.pandas_input_fn(x = X_train, y = y_train, batch_size = 10, num_epochs = 1000, shuffle = True)model = tf.estimator.LinearClassifier(feature_columns = feat_cols, n_classes = 2)model.train(input_fn = input_func, steps = 1000)## Evaluate the modeleval_input_func = tf.estimator.inputs.pandas_input_fn(x = X_test, y = y_test, batch_size = 10, num_epochs = 1, shuffle = False)results = model.evaluate(eval_input_func)results Output:{&#39;accuracy&#39;: 0.73160172, &#39;accuracy_baseline&#39;: 0.64935064, &#39;auc&#39;: 0.79658431, &#39;auc_precision_recall&#39;: 0.6441071, &#39;average_loss&#39;: 0.52852213, &#39;global_step&#39;: 1000, &#39;label/mean&#39;: 0.35064936, &#39;loss&#39;: 5.0870256, &#39;prediction/mean&#39;: 0.35784912}The accuracy is 73.16%. Prediction! 12345# No y value in prediction, put new data set in xpred_input_func = tf.estimator.inputs.pandas_input_fn(x = X_test, batch_size = 10, num_epochs = 1, shuffle = False)predictions = model.predict(pred_input_func)my_pred = list(predictions)my_pred Dense Neural Network Classifier12345678910111213# Create a three-layer neural network, with 10 neurons in each layer dnn_model = tf.estimator.DNNClassifier(hidden_units = [10,10,10], feature_columns = feat_cols, n_classes = 2)### If use the same of the previous step error will show up -- because of the categorial columns, so we need to use embeded group embedded_group_col = tf.feature_column.embedding_column(assigned_group, dimension = 4)### Reset the feature columnsfeat_cols = [num_preg ,plasma_gluc,dias_press ,tricep ,insulin,bmi,diabetes_pedigree , embedded_group_col, age_bucket]### Now do as the previous -- train the modelinput_func = tf.estimator.input.pandas_inputs_fn(X_train, y_train, batch_size = 10, num_epochs = 1000, shuffle = True)dnn_model = tf.estimator.DNNClassifier(hidden_units = [10,10,10], feature_columns = feat_cols, n_classes = 2)dnn_model.train(input_fn = input_func, steps = 1000)# Evaluate the modeleval_input_func = tf.estimator.inputs.pandas_input_fn(x = X_test, y = y_test, batch_size = 10, num_epochs = 1, shuffle = False)dnn_model.evaluate(eval_input_func) Output:{&#39;accuracy&#39;: 0.75757575, &#39;accuracy_baseline&#39;: 0.64935064, &#39;auc&#39;: 0.82814813, &#39;auc_precision_recall&#39;: 0.67539084, &#39;average_loss&#39;: 0.49014509, &#39;global_step&#39;: 1000, &#39;label/mean&#39;: 0.35064936, &#39;loss&#39;: 4.7176466, &#39;prediction/mean&#39;: 0.36287409}]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>TensorFlow, Deep Learning, Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode in Python3 - Two Sum]]></title>
    <url>%2F2018%2F01%2F09%2FLeetCode1%2F</url>
    <content type="text"><![CDATA[As a graduate student in Statistics, I profoundly believe the power of programming skills to Statistics/ Data Science. I used R or Matlab as my default programming tools and they are pretty powerful to help me solve most mathematical related tasks. However I realize in Artificial Intelligence/ Machine Learning realm, Python is more prevelent. In order to better gain the gist of this mighty programming language, in the process of learning it, I will show my LeetCode practice in Python3 with relative explanations as well as pitfalls I encounter. QuestionGiven an array of integers, return indices of the two numbers such that they add up to a specific target. You may assume that each input would have exactly one solution, and you may not use the same element twice. ExampleGiven nums = [2, 7, 11, 15], target = 9,Because nums[0] + nums[1] = 2 + 7 = 9,return [0, 1]. Solution12345class Solution: def twoSum(self, nums, target): for i in range(len(nums)): if (target - nums[i]) in nums[i+1:]: return (i, nums.index(target - nums[i], i+1) ) Analysis It is easily to ignore the condition “not use the same element twice” in the question. Initially I used return (i, nums.index(target - nums[i] ) instead of return (i, nums.index(target - nums[i], i+1 ),it works for most cases, but for example such as Given nums = [3,3], target = 6, it failed to give me [0,1], it returned [0,0]. So basically .index(A,B) is to find the position of A after Bth index. Another point worthy of mentioning is self in the definition part def twoSum( self, nums, target). self is used to refer class in python. In this case the class is Solution DifficultyEasy]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>TwoSum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Data Mining Note 4 - Discriminant Analysis and Naive Bayes]]></title>
    <url>%2F2018%2F01%2F07%2FData-Mining-Note4%2F</url>
    <content type="text"><![CDATA[The intuitive motivation of discriminant analysis is to estimate a function based on a given dataset. However, unless a funtion is derived as a result of optimizing the expected loss of interest, more has to be done to find out if that function is good with respect to the standards set by the loss funtion. Ideally, one would like to build funtions as solutions to the criterion of optimality. Unfortunately, that can be hard in both classification (and even regression). Hence it is common for researchers to come up with a decent (reasonable) function, and then compute the estimated average loss to find out if what the discovered is a pearl.]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>LDA, QDA, Navie Bayes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Data Mining Note 3 - K Nearest Neighbors]]></title>
    <url>%2F2018%2F01%2F06%2FData-Mining-Note3%2F</url>
    <content type="text"><![CDATA[This chapter talks about k Nearest Neighbors (kNN) as a metholodogy for both classification and regression problems. The kNN method serves a basic and easy to understand foundational machine learning and data mining technique. The kNN method is an excellent baseline machine learning technique, and also allows many extensions. It usually performs reasonable well or sometimes very well when compared to more sophisticated techniques. kNN classification basically means the estimated class of a vector $\mathbf{x}$ is the most frequent class label in the neighborhood of $\mathbf{x}$. kNN classifiers are inherently naturally multi-class, and are used extensively in applications such as image processing, character recognition and general pattern recognition tasks. For kNN regression, the estimated response value of a vector $\mathbf{x}$ is the average of the response values in the neighborhood of $\mathbf{x}$. Principle for kNN Classification: The reasonable class/category for a given object is the most prevalent class among its nearest neighbors.Principle k-Nearest Neighbor Regression: The reasonable prediction of the response value for a given object is the average of the response values of its nearest neighbors. General Steps for kNNComparison of Classification and Regression Choose a distance for measuring how far a given point is from another Set the size of the neighborhood k Compute the distance from each existing point to the new point Identify the class labels of the k points closest/nearest to the new point (classification)Identify the response values of the k points closest/nearest to the new point (regression) Assign the most frequent label to the new point (classification)Compute the average of the response values of those k neighbors as the best estimate of the new point (regression) DistanceFirst introduce some most commonly used distance below, which would be applied later in kNN. Euclidean distance: also known as the $l_2$ distance $$\begin{align*} d(\mathbf{x}_i,\mathbf{x}_j) = \sqrt{\sum_{l=1}^q(x_{il}-x_{jl})^2} = ||\mathbf{x}_i-\mathbf{x}_j||_2 \end{align*}$$ Manhattan distance (city block): also known as $l_2$ distance $$\begin{align*} d(\mathbf{x}_i,\mathbf{x}_j) = \sum_{l=1}^q|x_{il}-x_{jl}| = ||\mathbf{x}_i-\mathbf{x}_j||_1 \end{align*}$$ Maximum distance: also known as the infinity distance $$\begin{align*} d(\mathbf{x}_i,\mathbf{x}_j) = \underset{l=1,...,q}{\text{max}}|x_{il}-x_{jl}| = ||\mathbf{x}_i-\mathbf{x}_j||_\infty \end{align*}$$ Minkowski distance: also known as $l_p$ distance $$\begin{align*} d(\mathbf{x}_i,\mathbf{x}_j) = \Big\{\sum_{l=1}^q|x_{il}-x_{jl}| \Big\}^{1/p} \end{align*}$$ Canberra distance: $$\begin{align*} d(\mathbf{x}_i,\mathbf{x}_j) =\sum_{l=1}^q \frac{ |x_{il}-x_{jl}| }{ |x_{il}+x_{jl}| } \end{align*}$$ Jaccard/Tanimoto distance: For binary vectors ie $\mathbf{x}_i\in {0,1}^q$ $$\begin{align*} d(\mathbf{x}_i,\mathbf{x}_j) =1- \frac{ \mathbf{x}_i\cdot \mathbf{x}_j }{ |\mathbf{x}_i|^2 + |\mathbf{x}_j|^2 - \mathbf{x}_i\cdot \mathbf{x}_j} \end{align*}$$ kNN ClassificationDetailed Steps for kNN Classification$\mathcal{D} = \Big\{(x_1, Y_1),…, (x_n, Y_n) \Big\}$, with $x_i\in \mathcal{X}^q, Y_i\in \{1,…,g\}$ Choose the value of k and the distance to be used Let $\mathbf{x}^*$ be a new point. Compute $d(\mathbf{x}^*,\mathbf{x}_i) \quad i=1,2,…n $ Rank all the distances $d^∗_i$ in increasing order: $$\begin{align*} d^&lowast;_{(1)} \leq d^&lowast;_{(2)} \leq ...\leq d^&lowast;_{(k)} \leq d^&lowast;_{(k+1)} \leq ... d^&lowast;_{(n)} \end{align*}$$ Form $\mathcal{V}_k(\mathbf{x}^∗)$, the k-Neighborhood of $\mathbf{x}^∗$$$\begin{align*} \mathcal{V}_k(\mathbf{x}^&lowast;) = \Big\{ \mathbf{x}_i: d(\mathbf{x}^*,\mathbf{x}_i)\leq d^*_{(k)} \Big\} \end{align*}$$ Compute the predicted response $\hat{Y}^*$ as $$\begin{align*} \hat{Y}^*_{kNN} &amp;= \text{Most frequent label in } \mathcal{V}_k(\mathbf{x}^&lowast;) \\ &amp;= \hat{f}^*_{kNN}(\mathbf{x}^*) = \underset{j\in\{1,...,g\}}{\texttt{argmax}} \Big\{ p_j^{(k)}(\mathbf{x}^*) \Big\}\\ \text{where } p_j^{(k)}(\mathbf{x}^*)= \frac{1}{k}\sum_{\mathbf{x}^* \in \mathcal{V}_k(\mathbf{x}^&lowast;) } I(Y_i=j) &amp; \text{ estimates the probability that } \mathbf{x}^* \text{ belongs to class j based on} \mathcal{V}_k(\mathbf{x}^&lowast;) \end{align*}$$ Note: [Posterior probability estimate] $ \frac{1}{k}\sum_{\mathbf{x}^* \in \mathcal{V}_k(\mathbf{x}^∗) } I(Y_i=j) $ can be regarded as a rough estimate of $ \pi_j (\mathbf{x}^*) = Pr[Y^*=j| \mathbf{x}^* ] $ , the posterior probability of class membership of $ \mathbf{x}^* $ Comments: kNearest Neighbors (kNN) essentially performs classification by voting for the most popular response among the k nearest neighbors of $\mathbf{x}^*$. kNN provides the most basic form of nonparametric classification. Since the fundamental building block of kNN is the distance measure, one can easily perform classification beyond the traditional setting where the predictors are numeric. For instance, classification with kNN can be readily performed on indicator attributes$$ \mathbf{x}^* = (x_{i1},…, x_{ip} )^T \in \{ 0,1\}^p $$ kNN classifiers are inherently naturally multi-class, and are used in many applications. kNN RegressionDetailed Steps for kNN Regression$\mathcal{D} = \Big\{(x_1, Y_1),…, (x_n, Y_n) \Big\}$, with $x_i\in \mathcal{X}^q, Y_i\in \mathbb{R}$ Choose the value of k and the distance to be used Let $\mathbf{x}^*$ be a new point. Compute $d(\mathbf{x}^*,\mathbf{x}_i) \quad i=1,2,…n $ Rank all the distances $d^∗_i$ in increasing order: $$\begin{align*} d^&lowast;_{(1)} \leq d^&lowast;_{(2)} \leq ...\leq d^&lowast;_{(k)} \leq d^&lowast;_{(k+1)} \leq ... d^&lowast;_{(n)} \end{align*}$$ Form $\mathcal{V}_k(\mathbf{x}^∗)$, the k-Neighborhood of $\mathbf{x}^∗$$$\begin{align*} \mathcal{V}_k(\mathbf{x}^&lowast;) = \Big\{ \mathbf{x}_i: d(\mathbf{x}^*,\mathbf{x}_i)\leq d^*_{(k)} \Big\} \end{align*}$$ Compute the predicted response $\hat{Y}^*$ as $$\begin{align*} \hat{Y}^*_{kNN} &amp;= \hat{f}^*_{kNN}(\mathbf{x}^*)\\ &amp;= \frac{1}{k}\sum_{\mathbf{x}^* \in \mathcal{V}_k(\mathbf{x}^&lowast;) } Y_i \\ &amp;= \frac{1}{k}\sum_{i=1}^n Y_i I( \mathbf{x}^* \in \mathcal{V}_k(\mathbf{x}^&lowast;) ) \end{align*}$$ Comments: kNearest Neighbors (kNN) essentially performs regression by averaging the responses of the nearest neighbors of $\mathbf{x}^*$. kNN provides the most basic form of nonparametric regression Since the fundamental building block of kNN is the distance measure, one can easily perform regression beyond the traditional setting where the predictors are numeric. For instance, Regression vectors of binary with kNN can be readily performed on indicator attributes$$ \mathbf{x}^* = (x_{i1},…, x_{ip} )^T \in \{ 0,1\}^p $$ kNN somewhat performs smoothing (filtering). The estimated response kNN for $\mathbf{x}^*$ is estimator of the average response which is the conditional expectation of Y given $\mathbf{x}^*$$$ \hat{Y}^*_{kNN} =\mathbb{E}\widehat{[Y^*|\mathbf{x}^*]} $$ Basic kNN &amp; Weighted kNNLimitation of Basic kNN Equidistance: All neighbors are given the same contribution to the estimate of the response; In the estimated probability $$\begin{align*} p_j^{(k)}(\mathbf{x}^*) &amp; = \frac{1}{k}\sum_{\mathbf{x}^* \in \mathcal{V}_k(\mathbf{x}^&lowast;) } I(Y_i=j) =\sum_{\mathbf{x}^* \in \mathcal{V}_k(\mathbf{x}^&lowast;) } w_i I(Y_i=j) \\ \text{the weight } w_i = \frac{1}{k}=\texttt{constant }&amp;\text{for all points in } \mathcal{V}_k(\mathbf{x}^&lowast;) \text{ regardlessly of how far they are from } \mathbf{x}^&lowast; \end{align*}$$ No model, weak interpretability: There is no underlying model, therefore no interpretation of the response relative to the predictor variables. There is no training set, since all happens at prediction. For this reason, kNN is referred to as lazy method. Computationally intensive: Predictions are computationally very intensive, due to the fact that for each new observation, the whole dataset must be traversed to compute the response Extension: Weighted kNNkNN classification can be improved by weighting the votes as a function of the distance from $\mathbf{x}^∗$. The weights are defined so as to preserve convexity $ \sum_{i=1}^k w_i = 1$. Some of the common weighting schemes include: Exponential Decay:$$\begin{align*} w_i = \frac{e^{-d_i^*}}{\sum_{l=1}^k e^{-d_l^*} } \end{align*}$$ Inverse Distance:$$\begin{align*} w_i = \frac{ \frac{1}{1+d_i^*}}{\sum_{l=1}^k \frac{1}{1+d_l^*} } \end{align*}$$ Effect of kNNEffect of k k controls the complexity of the underlying classifier, with small k yielding very complex classifiers and large k yielding rather simple ones. If k is small, the estimated class is determined based on very few neighbors, the resulting kNN classifier will have very low bias, but very high variance. Take the example of k=1, the decision boundary will perfectly separate the classes on the training set, but will perform poorly on the test set. If k is large, the estimated class is determined based on very many neighbors from far and wide, the resulting kNN classifier willhave very large bias, but very low variance. In fact, for truly large k, the decision boundary will be a constant hyperplane. The determination of an optimal k desires the trade-off between bias and variance:Determine k by cross validationDetermine k by direct minimization of the estimated prediction error via a suitably chosen test set Effect of nAs stated before, kNN is a lazy method, everything happens at prediction step. Thus the sample size n plays a crucial role, and large n would lead to intense prediction. Effect of pThe dimensionality p of the input space is only felt by the function that computes the distances. If the function is optimized, kNN should be unaffected by this dimensionality Effect of distanceSome distances are more robust to extreme observations. Pros &amp; Cons of kNNStrength The kNN method is intuitively appealing and very easy to understand, explain, program/code and interpret The kNN method provides a decent estimate of $Pr[Y = j|x]$, the posterior probability of class membership The kNN method easily handles missing values (by restricting distance calculations to subspace) As the number of training samples grows larger, the asymptotic misclassification error rate is bounded by twice the Bayes risk.$$ \underset{n\rightarrow \infty}{lim} R(\hat{f}_n^{(kNN)}) \leq 2R^* $$ The kNN method is naturally suitable for sequential/incremental machine learning. The kNN method is also suitable where the hypothesis space is variable in size. The kNN method can handle non-numeric data as long as the distance can be defined. The kNN methods can handle mixed types of data as long as the distance are computed as hybrid or combinations. The kNN method is inherently multi-class. This is very important because for some other methods, going beyond binary classification requires some sophisticated mathematics. It also handles very flexible decision boundaries. Weakness The computational complexity of kNN is very high in prediction. Specifically, it is $\mathcal{O}(nmp)$ where n is the training set size, m is the test set size and p is the number of predictor variables. This means that kNN requires large amount of memory, and therefore does NOT scale well. This failure in scalability is addressed using various heuristics and strategies. kNN methods suffer from the Curse of Dimensionality (COD). When p is large and n is small (short fat data/ dimension of the space very high), the concept of nearness becomes meaningless to the point of being ill-defined, because the ”neighborhood” becomes very large. Thus, the nearest neighbor could be very far when the space is high dimensional and there are very few observations. The kNN method does not yield a model, and therefore no parameter to help explain why the method performs as it does. The kNN method is heavily affected by the local structure, and it is very sensitive to both irrelevant and correlated features. Unless the distance is well chosen and properly calibrated, kNN methods will be sensitive to outliers and all sorts of noise in the data. Unless the distance is used in some way to weight the neighbors, more frequent classes will dominate in the determination of the estimated label. This means one has to be careful with kNN when one class has a far larger proportion of observations than the others. The measurement scale of each variable affect the kNN method more than most methods. (measurement scale: standardizing, unitizing or cubizing/squeezing the data). Application of kNN Handwritten Digit Recognition is usually the first task in some Data Analytics competitions. Text Mining and specific topic of text categorization/classification has made successful use of kNearest Neighbors approach Credit Scoring is another application that has been connected with k Nearest Neighbors Classification Disease Diagnostics also has been tackled using k Nearest Neighbors Classifiers]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>kNN, Classification, Regression, Distance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Data Mining Note 2 - Binary Classification]]></title>
    <url>%2F2018%2F01%2F05%2FData-Mining-Note2%2F</url>
    <content type="text"><![CDATA[This chapter gives basic introduction of Vapnik-Chervonenkis Theory on Binary Classification. When it comes to binary classification task, we always want to know the functional relationship between $\mathbf{x}$ and $y$, or how to determine the “best” approach to determining from the available observaions such that given a new observation $\mathbf{x}^{new}$, we can predict its class $y^{new}$ as accurately as possible. Universal BestConstructing a classification rule that puts all the points in their corresponding classes could be dangerous for classifying new observations not present in the current collection of observations. So to find an classification rule that achieves the absolute very best on the present data is not enough since infinitely many more observations can be generated. Even the universally best classifier will make mistakes. Of all the functions in $\mathcal{Y}^{\mathcal{X}}$, assume that there is a function $f^*$ that maps any $\mathbf{x}\in\mathcal{X}$ to the corresponding $y\in\mathcal{Y}$ with the minimum number of mistakes. $f^*$ is denoted as universal best$$\begin{align*} f^* :\quad &amp; \mathcal{X} \rightarrow \mathcal{Y}\\ &amp; \mathbf{x} \rightarrow f^*(\mathbf{x}) \end{align*}$$ $f$ denote any generic function mapping an element $\mathbf{x}$ of $\mathcal{X}$ to its corresponding image $f(\mathbf{x})$ in $\mathcal{Y}$. Each time $\mathbf{x}$ is drawn from $\mathbb{P}(\mathbf{x})$, the disagreement between the image $f(\mathbf{x})$ and the true image y is called the loss, denoted by $l(y, f(\mathbf{x}))$. The expected value of this loss function with respect to the distribution $\mathbb{P}(\mathbf{x})$ is called the risk functional of $f$, denoted as $R(f)$.$$ R(f) = \mathbb{E}[l(Y, f(\mathbf{X}))] = \int l(y, f(\mathbf{x})) d \mathbb{P}(\mathbf{x}) $$The best function $f^*$ over the space $\mathcal{Y}^{\mathcal{X}}$ of all measurable functions from $\mathcal{X}$ to $\mathcal{Y}$ is therefore$$\begin{align*} f^* &amp;=\texttt{arg} \underset{f}{inf} R(f) \\ R(f^*) &amp;= R^* = \underset{f}{inf} R(f) \end{align*}$$Note: Finding $f^*$ without the knowledge of $\mathbb{P}(\mathbf{x})$ means having to search the infinite dimensional function space $\mathcal{Y}^{\mathcal{X}}$ of all mappings from $\mathcal{X}$ to $\mathcal{Y}$, which is an ill-posed and computationally nasty problem (No way to get the universal best). Thus, we seek a more reasonable way to solve the problem via choosing from a function space $F\subset \mathcal{Y}^{\mathcal{X}} $ , a function $f^+ \in F$ that best estimates the dependencies between $\mathbf{x}$ and $y$. The goal of statistical function estimation is to devise a technique (strategy) that chooses from the function class $\mathcal{F}$, the one function whose true risk is as close as possible to the lowest risk in class $\mathcal{F}$. So it is crucial to define what is meant to be best estimates. Hence we need to know what is loss function and risk functional. Loss Functioin and Risk FunctionalFor classification task, the so-called 0-1 loss function defined below is used.$$ l(y, f(\mathbf{x})) = \mathbb{1}_{Y\neq f(X)} = \begin{cases} 0, &amp;\text{if } y = f(\mathbf{x}) \\ 1, &amp;\text{if } y \neq f(\mathbf{x}) \end{cases} $$ And the corresponding risk functional, also named cost function showed below confirms the intuition because it is estimated in practice by simply computng the proportion of misclassfied entities.$$ R(f)= \int l(y, f(\mathbf{x})) d \mathbb{P}(\mathbf{x}) = \mathbb{E}[\mathbb{1}_{Y\neq f(X)}] = \underset{(X,Y)\sim \mathbb{P}}{Pr}[Y\neq f(X)]$$ The minimizer of the 0-1 risk functional over all possible classifiers is the so-called Bayes classifier which we shall denote here by $f^*$. It minimizes the rate of misclassifications.$$ f^* =\texttt{arg} \underset{f}{inf} R(f) = \texttt{arg} \underset{f}{inf} [\underset{(X,Y)\sim \mathbb{P}}{Pr}[Y\neq f(X)] ]$$ Specifically, the Bayes’ classifier $f^*$ is given by the posterior probability of class membership, namely$$ f^* =\texttt{arg} \underset{y\in \mathcal{Y}}{max} [Pr[Y=y | \mathbf{x}] ]$$ Because it is impossible to find the universal best $f^*$, We need to select a reasonable function space $F\subset \mathcal{Y}^{\mathcal{X}} $, and then choose the best estimator $f^+$ from $\mathcal{F}$. For the binary pattern recognition problem, one may consider finding the best linear separating hyperplane. For empirical risk minimization,$$\begin{align*} \hat{R}(f) &amp;= \frac{1}{n}\sum_{i=1}^n \mathbb{1}_{Y\neq f(X)}\\ \hat{f} &amp;= \underset{f\in \mathcal{F}}{\texttt{argmin}} [ \frac{1}{n}\sum_{i=1}^n \mathbb{1}_{Y\neq f(X)} ] \end{align*}$$ Bias-Variance Trade-OffIn statistical estimation, the blow important issues needs to be taken into account: Bias of the estimator Variance of the estimator Consistency of the estimatorIn point estimation, given $\theta$ as the true value of the parameter, $\hat{\theta}$ is a point estimator of $\theta$, then the total error can be decomposed as: $$\begin{align*} \hat{\theta}-\theta = \underbrace{ \hat{\theta}- \mathbb{E}[ \hat{\theta}]}_\text{Estimation error} + \underbrace{\mathbb{E}[ \hat{\theta}]-\theta}_\text{Bias} \end{align*}$$ Under the squared error loss, one seeks $\hat{\theta}$ that minimizes the mean squared error, rather than trying to find the minimum variance unbiased estimator (MVUE).$$\begin{align*} \hat{\theta}=\underset{\theta\in \Theta}{\texttt{argmin}}\mathbb{E}[ (\hat{\theta}-\theta)^2] = \underset{\theta\in \Theta}{\texttt{argmin}}MSE(\hat{\theta}) \end{align*}$$ Actually, the traditional bias-variance decomposition of the MSE reveals the bias-variance trade-off: $$\begin{align*} MSE(\hat{\theta}) &amp;= \mathbb{E}[ (\hat{\theta}-\theta)^2] \\ &amp;= \mathbb{E}[ (\hat{\theta}- \mathbb{E}[ \hat{\theta}])^2] + \mathbb{E}[(\mathbb{E}[ \hat{\theta}]-\theta)^2 ] \\ &amp;= \text{Variance} + \text{Bias}^2 \end{align*}$$ Again becasue of the previous statement that we cannot get the true value of $\theta$, MVUE will not help, which means the estimation we get contains bias. If the bias is too small, variance would be very large. Vice versa. The best compromise is then to trade-off bias and variance. In functional terms we call it trade-off between approximation error and estimation error. Statistical Consistency$\hat{\theta}_n$ is a consistent estimator of $\theta$ if $\forall \epsilon &gt;0$$$ \underset{n\rightarrow \infty}{lim}Pr[|\hat{\theta}_n-\theta|&gt;\epsilon] = 0 $$ It turns out that for unbiased estimators $\hat{\theta}_n$, consistency is straightforward as direct consequence of a basic probabilistic inequality like Chebyshev’s inequality. However, for biased estimators, one has to be more careful.The ERM (Consistency of the Empirical Risk Minimization) principle is consistent if it provides a sequence of functions for which both the expected risk and the empirical risk converge to the minimal possible value of the risk in the function class under consideration.$$\begin{align*} R(\hat{f}_n) &amp; \xrightarrow[n \rightarrow \infty]{P} \underset{f\in \mathcal{F}}{inf}R(f) = R(f^+) \\ \underset{n\rightarrow \infty}{lim} Pr&amp;[\underset{f\in\mathcal{F}}{sup}|R(f)-\hat{R}_n(f)|&gt;\epsilon] = 0 \end{align*}$$As $\hat{R}_n(f) $ reveals the disagreement between classifier $f$ and the truth about the label $y$ of $\mathbf{x}$ based on information contained in the sample $\mathcal{D}$. So for a given (fixed) function (classifier) $f$,$$ \mathcal{E}[\hat{R}_n(f)] = R(f) $$ Key Question:Since one cannot calculate the true error, how can one devise a learning strategy for choosing classifiers based on it?Tentative Answer:At least devise strategies that yield functions for which the upper bound on the theoretical risk is as tight as possible: With probability $1 − \delta$ over an i.i.d. draw of some sample according to the distribution $\mathcal{P}$, the expected future error rate of some classifier is bounded by a function $g$ ($\delta$, error rate on sample) of $\delta$ and the error rate on sample. $\Downarrow\Downarrow\Downarrow$$$\begin{align*} \text{Pr}\Big\{ \texttt{TestError} \leq \texttt{TrainError} + \phi(n,\delta, \kappa (\mathcal{F})) \Big\} \leq 1-\delta \end{align*}$$ Thorem: (Vapnik and Chervonenkis, 1971)Let $\mathcal{F}$ be a class of functions implementing so learning machines, and let $\xi = VCdim(\mathcal{F})$ be the VC dimension of F. Let the theoretical and the empirical risks be defined as earlier and consider any data distribution in the population of interest. Then $\forall f\in \mathcal{F}$, the prediction error (theoretical risk) is bounded by$$ R(f) \leq \hat{R}_n(f) + \sqrt{ \frac{\zeta(\text{log}\frac{2n}{\zeta}+1)-\text{log}\frac{\eta}{4} }{n}} $$with probability of at least $1-\eta$. or$$\text{Pr} \Big( \texttt{TestError} \leq \texttt{TrainError} + \sqrt{ \frac{\zeta(\text{log}\frac{2n}{\zeta}+1)-\text{log}\frac{\eta}{4} }{n}} \Big) \leq 1-\eta $$ VC Bound From the expression of the VC Bound, to improve the predictive performance (reduce prediction error) of a class of machines is to achieve atrade-off (compromise) between small VC dimension and minimization of the empirical risk. One of the greatest appeals of the VC bound is that, though applicable to function classes of infinite dimension, it preserves the same intuitive form as the bound derived for finite dimensional $\mathcal{F}$ VC bound is acting in a way similar to the number of parameters, since it serves as a measure of the complexity of $\mathcal{F}$. One should seek to construct a classifier that achieves the best trade-off between complexity of function class (measured by VC dimension) and fit to the training data (measured by empirical risk).]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>classification, Vapnik-Chervonenkis, binary</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Data Mining Note 1 - Introduction]]></title>
    <url>%2F2018%2F01%2F05%2FData-Mining-Note1%2F</url>
    <content type="text"><![CDATA[I took Dr. Ernest Fokoue’s course Data Mining (STAT 747) in my Master study in RIT and gained tremendous fascinating modern Statistical Machine Learning technique skills. I want to share the marvelous essence of this course and my self-learning and self-reflection towards this course. This course covers topics such as clustering, classification and regression trees, multiple linear regression under various conditions, logistic regression, PCA and kernel PCA, model-based clustering via mixture of gaussians, spectral clustering, text mining, neural networks, support vector machines, multidimensional scaling, variable selection, model selection, k-means clustering, k-nearest neighbors classifiers, statistical tools for modern machine learning and data mining, naïve Bayes classifiers, variance reduction methods (bagging) and ensemble methods for predictive optimality. I will show the roadmap of this note in this post and follow the order. Basically, each post contains one essential data mining technique and later I will show some relative examples and exercises based on these methods. Supervised Learning Classification Regression Unsupervised Learning Clustering Analysis Factor Analysis Topic Modeling Recommender System Application in Statistical Machine Learning Handwritten Digit Recognition (MNIST) Text Mining Credit Scoring Disease Diagonostics Audio Processing Speaker Recognition &amp; Speaker Identification Computing Tools in R1234567891011121314151617library(ctv)library(MachineLearning)library(HighPerformanceComputing)library(TimeSeries)library(Bayesian)library(Robust)library(biglm)library(foreach)library(glmnet)library(kernlab)library(randomForest)library(ada)library(audio)library(rpart)library(e1071)library(MASS)library(kernlab) Important Aspects of Machine LearningMachines Inherently designed to handle p larger than n problems Classification and Regression Trees Support Vector Machines Relevance Vector Machines (n &lt; 500) Gaussian Process Learning Machines (n &lt; 500) k-Nearest Neighbors Learning Machines (Watch for the curse of dimensionality) Kernel Machines in generalMachines can handle p larger than n problems if regularized with suitable constraints Multiple Linear Regression Models Generalized Linear Models Discriminant AnalysisEnsemble Learning Machines Random Subspace Learning Ensembles (Random Forest) Boosting and its extensions Note: Red parts in this Note Series remain questionable and I will update and add explanations for those parts as soon as I figure them out.]]></content>
      <categories>
        <category>Data Mining</category>
      </categories>
      <tags>
        <tag>SVM, KNN, KMeans, Ensemble, NN</tag>
      </tags>
  </entry>
</search>
