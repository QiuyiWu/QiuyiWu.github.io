<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Bayesian Decision Theory Notes</title>
      <link href="/2019/03/29/DecisionTheory/"/>
      <url>/2019/03/29/DecisionTheory/</url>
      
        <content type="html"><![CDATA[<p>Deep gratitude to Prof. Ed George, who opened a door to the world of emperical bayes for me, I began to read Prof. Jim Bergerâ€™s book <a href="https://www.springer.com/us/book/9780387960982" target="_blank" rel="noopener"><em>Statistical Decision Theory and Bayesian Analysis</em></a> (1985). Here I mainly focus on Chapter 1, 3, 4, 5 of Jimâ€™s book and also share some thoughts on Edâ€™s paper <a href="https://projecteuclid.org/euclid.aos/1176349849" target="_blank" rel="noopener"><em>Minimax Multiple Shrinkage Estimation</em></a> (1986). </p><h1 id="Why-Bayesian"><a href="#Why-Bayesian" class="headerlink" title="Why Bayesian?"></a>Why Bayesian?</h1><p>Different people would have different conclusions based on their prior beliefs of the plausibility of the event, Baysian analysis is to seek to utilize prior information.<br><a id="more"></a><br><strong>Some Definitions:</strong></p><ul><li>Bayeisan expected loss: <span>$\rho (\pi^*, a) = E^{\pi^*}L(\theta, a) = \int_\Theta L(\theta, a)dF^{\pi^*}(\theta)$</span><!-- Has MathJax --></li><li>Risk function of decision rule $\delta(x)$:  <span>$R(\theta, \delta) = E^X_\theta [L(\theta,\delta(X))] = \int_x L(\theta, \delta(x))dF^X(x|\theta)$</span><!-- Has MathJax --></li><li>A decision rule $\delta$ is admissible if there exists no R-better decision rule. Inadmissible should not be used because we can found better decision rule with smaller risk. </li><li>Bayesian risk of decision rule $\delta$: <span>$r(\pi, \delta) = E^\pi [R(\theta, \delta) ]$</span><!-- Has MathJax --></li><li>Randomized decision rule: $\delta^*(x,A)$ is a probability distribution on $\mathscr{A}$ that for each $x$, if observed, an action $A$ will be chosen.  </li></ul><h2 id="Decision-Principle"><a href="#Decision-Principle" class="headerlink" title="Decision Principle"></a>Decision Principle</h2><h3 id="Bayes-Risk-Principle"><a href="#Bayes-Risk-Principle" class="headerlink" title="Bayes Risk Principle"></a>Bayes Risk Principle</h3><p>A decision rule $\delta_1$ is preferred to a rule $\delta_2$ if $r(\pi, \delta_1) &lt; r(\pi, \delta_2)$. <strong>Bayes rule</strong> $\delta^\pi$ is optimal that minimizes <strong>Bayes risk</strong> $r(\pi, \delta^\pi)$. It can also be written as $r(\pi)$.</p><p><strong>Example:</strong><br>Assume $X$ is $\mathcal{N}(\theta, 1)$, and it is desired to estimate $\theta$ under the squared-error loss $L(\theta, a) = (\theta-a)^2$. Consider the decision rule $\delta_c(x) = cx$, then<br><span>$$\begin{align}R(\theta, \delta_c) &amp;= E_\theta^XL(\theta, \delta_c(X)) =  E_\theta^XL(\theta - cX)^2 \\&amp;=  E_\theta^X(c[\theta-X] + [1-c]\theta)^2\\&amp;= c^2  E_\theta^X [\theta-X]^2 + 2c(1-c)\theta E_\theta^X[\theta-X] + (1-c)^2\theta^2\\ &amp;= c^2 + (1-c)^2\theta^2\end{align}$$</span><!-- Has MathJax --><br>So the Bayes risk $r(\pi, \delta_c) = c^2 + (1-c)^2\tau^2$ for $\pi\sim \mathcal{N}(0,\tau^2)$ is minimized when $c = c_0 = \tau^2/(1+\tau^2)$, which is<br><span>$$\begin{align}r(\pi) &amp;= r(\pi, \delta_{c_0}) = c_0^2 + (1-c_0)^2\tau^2 = (\frac{\tau^2}{1+\tau^2})^2 + (\frac{1}{1+\tau^2})^2\tau^2 = \frac{\tau^2}{1+\tau^2}\end{align}$$</span><!-- Has MathJax --></p><h3 id="Minimax-Principle"><a href="#Minimax-Principle" class="headerlink" title="Minimax Principle"></a>Minimax Principle</h3><p>Let <span>$\delta^*\in \mathscr{D}^*$</span><!-- Has MathJax --> be a randomized rule, the quantity <span>$\underset{\theta\in \Theta}{\text{sup }} R(\theta, \delta^* )$</span><!-- Has MathJax --> is the worst scenario for the rule <span>$\delta^*$</span><!-- Has MathJax -->. To protect the worst possible state of nature, we come to use <strong>minimax principle</strong>.<br>A rule $\delta^{*M}$ is a minimax decision rule if it minimizes <span>$\underset{\theta}{\text{sup }} R(\theta, \delta^* )$</span><!-- Has MathJax --> among all randomized rules in <span>$\mathscr{D}^*$</span><!-- Has MathJax -->:<br><span>$$\begin{align}\underset{\theta}{\text{sup }} R(\theta, \delta^{*M} ) = \underset{\delta^*\in\mathscr{D}^*}{\text{inf }} \underset{\theta\in \Theta}{\text{sup }} R(\theta, \delta^* ) \end{align}$$</span><!-- Has MathJax --> </p><h3 id="Invariance-Principle"><a href="#Invariance-Principle" class="headerlink" title="*Invariance Principle"></a>*Invariance Principle</h3><h2 id="Foundation"><a href="#Foundation" class="headerlink" title="Foundation"></a>Foundation</h2><p>What we are really interested in determining is whether or not the null hypothesis is <em>approximately</em> true.</p><h2 id="Convexity"><a href="#Convexity" class="headerlink" title="Convexity"></a>Convexity</h2><ul><li>Convex: A set is convex if if for any two pooints $\mathbf{x}$ and $\mathbf{y}$ in $\Omega$, the point $[\alpha\mathbf{x}+(1-\alpha)\mathbf{y} ]$ is in $\Omega$ for $0\leq \alpha \leq 1$.</li><li>Convex combination: If ${\mathbf{x}^1, \mathbf{x}^2, â€¦}$ is a sequence of points in R^m, and $0\leq\alpha_i\leq 1$ are numbers such that $\sum_{i=1}^\infty\alpha_i =1$, then $\sum_{i=1}^\infty\alpha_i\mathbf{x}^i$ is convex combination of ${\mathbf{x}^i}$</li></ul><h1 id="Prior-Information"><a href="#Prior-Information" class="headerlink" title="Prior Information"></a>Prior Information</h1><p><strong>Havenâ€™t finished yet. To be continuedâ€¦</strong></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1] Berger James, O. â€œStatistical decision theory and Bayesian analysis.â€ Berlin: Spring-Verlag (1985).<br>[2] George, Edward I. â€œMinimax multiple shrinkage estimation.â€ The Annals of Statistics 14.1 (1986): 188-205.</p>]]></content>
      
      
      <categories>
          
          <category> Statistics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Statistics </tag>
            
            <tag> Minimax </tag>
            
            <tag> Decision Theory </tag>
            
            <tag> Bayesian Analysis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Agent-based Modeling Wrap-up</title>
      <link href="/2019/03/13/ABM/"/>
      <url>/2019/03/13/ABM/</url>
      
        <content type="html"><![CDATA[<p>I attended the <a href="https://www.samsi.info/programs-and-activities/year-long-research-programs/model-uncertainty-mathematical-statistical-mums/agent-based-modeling-workshop-march-11-12-2019/" target="_blank" rel="noopener">SAMSI Agent-based Modeling Workshop</a> in Duke University on March 11-12, 2019. As one of the youngest attendants I would like to share some of the limelights discussed in this workshop. </p><p><strong>Description</strong>: Agent-based modeling is widely used across many disciplines to study complex emergent behavior generated from simulated entities that interact with each other and their environment according to relatively simple rules. Applications include automobile traffic modeling, weather forecasting, and the study of epidemics. The inferential challenge of agent-based models is that (in general) there is no tractable likelihood function, and thus it is difficult to fit the model or make quantified statements about the accuracy of predictions. This workshop addressed that challenge from the perspective of uncertainty quantification, so that emulator methodology could be used to make approximate principled inferences about agent-based simulations.<br><a id="more"></a></p><h1 id="Challenges-for-Statistics-History-of-ABM"><a href="#Challenges-for-Statistics-History-of-ABM" class="headerlink" title="Challenges for Statistics (History of ABM)"></a>Challenges for Statistics (History of ABM)</h1><ul><li>Statistical theory for agent-based model is almost primitive. More work needs to be done in methodology scenario.</li><li>Understanding of the parameterization is essential. We can possibly try to map from $\mathbb{R}^p$ to the input space.</li><li>Calibration method for agent-based model (face validity currently) can miss important structure.</li><li>Uncertainty expression in agent-based model hasnâ€™t been adressed yet. </li></ul><p><strong>Havenâ€™t finished yet. To be continuedâ€¦</strong></p><!---# Active Subspaces vs Deep Neural Netowrk (Transprtation System)# Economics (Exchange Equilibrium)# Stochastic Simulation-->]]></content>
      
      
      <categories>
          
          <category> Conference </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Agent-based Modeling </tag>
            
            <tag> SAMSI </tag>
            
            <tag> Statistics </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bayesian Nonparametrics Notes</title>
      <link href="/2019/03/07/BayesianNonparametrics/"/>
      <url>/2019/03/07/BayesianNonparametrics/</url>
      
        <content type="html"><![CDATA[<p>The note is partially based on the Bayesian Nonparametrics Machine Learning lectures by Yee Whye Teh at Max Planck Institute for Intelligent Systems in TÃ¼bingen, Germany. </p><p>Machine learning is all about data, and the uncertainty and complex process in the data. Probability theory is a rich language to express uncertainties. Graphical tool and complex models are develped to help visualize and derive algorithmic solutions.<br><a id="more"></a></p><h1 id="Probabilistic-Modelling"><a href="#Probabilistic-Modelling" class="headerlink" title="Probabilistic Modelling"></a>Probabilistic Modelling</h1><ul><li>Data: $x_1, x_2, â€¦, x_n$</li><li>Latent variables: $y_1, y_2, â€¦, y_n$</li><li>Parameter: $\theta$</li><li>Probabilistic model (generative model): parametrized joint distribution over variables<br>$P(x_1, x_2, â€¦, x_n, y_1, y_2, â€¦, y_n|\theta)$</li><li>Inference, of latent variables given observed data:<br>$P(y_1, y_2, â€¦, y_n|x_1, x_2, â€¦, x_n, \theta) = \frac{P(x_1, x_2, â€¦, x_n, y_1, y_2, â€¦, y_n|\theta)}{P(x_1, x_2, â€¦, x_n|\theta)}$</li><li>Learning, of parameters by maximum likelihood:<br>$\theta^{ML} = \underset{\theta}{\text{argmax }}P(x_1, x_2, â€¦, x_n|\theta) $</li><li>Prediction: $P( x_{n+1}, y_{n+1} |x_1, x_2, â€¦, x_n,\theta)$</li><li>Classification: $\underset{c}{\text{argmax }}P(x_{n+1}|\theta^c) $</li></ul><h1 id="Bayesian-Modelling"><a href="#Bayesian-Modelling" class="headerlink" title="Bayesian Modelling"></a>Bayesian Modelling</h1><ul><li>Prior distribution: $P(\theta)$</li><li>Posterior distribution (both inference and learning):<br>$P(y_1, y_2, â€¦, y_n,\theta |x_1, x_2, â€¦, x_n) = \frac{P(x_1, x_2, â€¦, x_n, y_1, y_2, â€¦, y_n|\theta) P(\theta)  }{  P(x_1, x_2, â€¦, x_n) }$</li><li>Prediction: $P( x_{n+1}| x_1, x_2, â€¦, x_n) = \int P( x_{n+1}|\theta )P(\theta| x_1, x_2, â€¦, x_n  )d\theta $</li><li>Classification: $P( x_{n+1}^c| x_1^c, x_2^c, â€¦, x_n^c) = \int P( x_{n+1}|\theta^c )P(\theta^c| x_1^c, x_2^c, â€¦, x_n^c  )d\theta^c $</li></ul><h2 id="Model-based-Clustering"><a href="#Model-based-Clustering" class="headerlink" title="Model-based Clustering"></a>Model-based Clustering</h2><p>Given data from heterogeneous unknown sources, and each cluster modelled using a parametric model:</p><ul><li>Data item:<br>$z_i|\pi\sim \text{Discrete}(\pi)$<br>$x_i|z_i,\theta_k \sim F(\theta))$</li><li>Mising proportions: $\pi = (\pi_1, \pi_2,â€¦, \pi_K)|\alpha \sim \text{Dirichlet}(\alpha/K,â€¦, \alpha/K)$</li><li>Cluster k: $\theta_k|\beta \sim \beta$<center><br><img class="left" src="/images/2019/03/model.png" width="50%" height="50%"><br></center></li></ul><h2 id="Hidden-Markov-Models"><a href="#Hidden-Markov-Models" class="headerlink" title="Hidden Markov Models"></a>Hidden Markov Models</h2><ul><li>Popular model for time series data (clustering over time)</li><li>Unobserved dynamic modelled using a Markov model</li><li>Observations modelled as independent conditioned on current state<center><br><img class="left" src="/images/2019/03/HMM.png" width="70%" height="70%"><br></center></li></ul><h2 id="Collaborative-Filtering"><a href="#Collaborative-Filtering" class="headerlink" title="Collaborative Filtering"></a>Collaborative Filtering</h2><p>A kind of recommendation system. e.g.: predict how much users would like products that they havenâ€™t seen based on the previous data. </p><p>Data: Rating $R_{ij}$ of a user $\xi_i$ for a certain product $\eta_j$<br>$R_{ij}|\xi_i,\eta_j  \sim \mathcal{N}(\xi_i^\top \eta_j, \sigma^2) $</p><center><br><img class="left" src="/images/2019/03/recommend.png" width="40%" height="40%"><br></center><h1 id="Bayesian-Nonparametrics"><a href="#Bayesian-Nonparametrics" class="headerlink" title="Bayesian Nonparametrics"></a>Bayesian Nonparametrics</h1><p>As I discussed in the previous blog post (<a href="https://qiuyiwu.github.io/2019/02/19/GaussianProcess/">Why are Gaussian Process Models called Nonparametric?</a>), nonparametric model is a <strong>parametric</strong> model where the number of parameters increases with data. It just cannot be described by fixed number of parameters. And nonparametric models still make model assumptions, but they are less constrained. </p><p>Why do we want Bayesian nonparametrics?</p><ol><li>Model Selection:<br>Typically we use parametric model, say clustering, we need to decide the number of clusters. If we have too many clusters we would encounter overfitting issue, and underfitting if we have too little clusters. In Bayesian model we donâ€™t do model optimization to find the maximum likelihood parameters for the optimal number of clusters, we just compute posterior distribution for the unknown part. And the Bayesian nonparametric model grows with amount of data, thus we can prevent overfitting and underfitting issues. </li><li>Large Function Spaces:<br>We would like to learn the large function space and infer the infinite dimensional objects themselves, and we want to learn the density of the input space.</li><li>Structural Learning:<br>We want to learn the structures of the data (e.g. hierarchical clustering), and we can use bayesian prior over combinatorial structures. Commonly, nonparametric priors alwayes lead to simpler learning algorithm than parametric priors because you donâ€™t need to worry about complicated model selection (e.g. number of latent variables, the way of latent variables connecting to other variables etc.). </li><li>Novel &amp; Useful Properties<ul><li>Exchangeability: permute your dataset without effecting the analysis</li><li>Power laws behaviors: Pitman-Yor process, Indian Buffet process</li><li>Flexible to build complex models: hierarchical nonparametric models, dependent Dirichlet process</li></ul></li></ol><h1 id="Dirichlet-Process"><a href="#Dirichlet-Process" class="headerlink" title="Dirichlet Process"></a>Dirichlet Process</h1><p>Dirichlet process is the cornerstone of modern Bayesian nonparametrics. It is the infinite limit of finite mixture models. It was defined by Ferguson in 1973 as a distribution over measures.  </p><h2 id="Finite-Mixture-Models"><a href="#Finite-Mixture-Models" class="headerlink" title="Finite Mixture Models"></a>Finite Mixture Models</h2><p>Dirichlet distrbution on K-dimensional probability simplex $\{\pi | \sum_k \pi_k=1 \}$:<br>$P(d\pi| \alpha) = \frac{\Gamma (\alpha)}{\Pi_k\Gamma(\alpha/K)}\Pi_{k=1}^K \pi_k^{\alpha/K-1}d\pi \quad$     with $\Gamma (\alpha) = \int_0^\infty x^{\alpha -1} e^x dx$</p><center><br><img class="left" src="/images/2019/03/dirichlet.png" width="80%" height="80%"><br></center><p>See <strong>Model-based Clustering</strong> part in the previous section. Given the observations $\mathbf{x}$ we want to learn the latent variables $\mathbf{z}, \pi, \mathbf{\theta}$. We can use MCMC sampling to infer the unknown parameters. We are constructing a Markov chain such that we can do the inference in an iterative manner as the chain would converge at some point. </p><h3 id="Gibbs-Sampling"><a href="#Gibbs-Sampling" class="headerlink" title="Gibbs Sampling"></a>Gibbs Sampling</h3><center><br><img class="left" src="/images/2019/03/model.png" width="50%" height="50%"><br></center><ul><li>Calculate the conditional distribution of the latent variables given the parameters and the observations: $ p(z_i=k|\text{ others }) \propto \pi_k f(x_i |\theta_k^*) $<br>$ \pi|\text{ others } \sim \text{Dirichlet}(\frac{\alpha}{K}+n_1,â€¦, \frac{\alpha}{K}+n_K) $</li><li>Calculate the conditional distribution of the parameters given the latent variables and the observations: $ p(\theta_k^*=\theta|\text{ others }) \propto B(\theta)\Pi_{j:z_j=k} f(x_j |\theta) $ </li><li>Iterate this process until they (latent variables and parameters) converge to the posterior distribution. </li></ul><h3 id="Collapsed-Gibbs-Sampling"><a href="#Collapsed-Gibbs-Sampling" class="headerlink" title="Collapsed Gibbs Sampling"></a>Collapsed Gibbs Sampling</h3><p>A more efficient approach of MCMC argorithm: integrate out $\pi, \theta^*$</p><ul><li>Marginalize out the parameters of the model </li><li>All we donâ€™t know is the latent variable (the clusters), and now we have the reduced space to update.<br>$ p(z_i=k|\text{ others }) \propto  \frac{\frac{\alpha}{K} + n_k^{-i}}{\alpha+n-1}f(x_i|z_j=k)$<br>where $ f(x_i|z_j=k) \propto \int B(\theta)f(x_i|\theta)\Pi_{j\neq i: z_j = k} f(x_j|\theta)d\theta  $<br>The prior term $\frac{\frac{\alpha}{K} + n_k^{-i}}{\alpha+n-1}$ is interpretable, $n_k^{-i}$ denotes the number of other data iterms currently assigned to cluster $k$ besides the current item $i$ we are interested in. $\frac{\alpha}{K}$ means even no item belongs to cluster $k$, we still have some probability for cluster $k$. The likelihood $f(x_i|z_j=k)$ means the conditional probability of the observation given the observations that are currently assigned to cluster $k$</li><li>Conditional distributions can be easily computed if $F$ is conjugate to $B$.</li></ul><h3 id="Infinite-Limit-of-Collapsed-Gibbs-Sampling"><a href="#Infinite-Limit-of-Collapsed-Gibbs-Sampling" class="headerlink" title="Infinite Limit of Collapsed Gibbs Sampling"></a>Infinite Limit of Collapsed Gibbs Sampling</h3><p>Assume a very large varlue of $K \rightarrow \infty$, there are at most $n&lt;K$ occupied clusters (most components are empty). We can combine these empty components together:<br>$ p(z_i=k|\text{ others }) = \frac{n_k^{-i}+ \frac{\alpha}{K}}{n-1+\alpha}f(x_i|z_j=k)$<br>$ p(z_i=k_{empty}|\text{ others }) = \frac{\alpha\frac{K-K^*}{K}}{n-1+\alpha}f(x_i|\{\})$<br>When $K \rightarrow \infty$, we get:<br>$ p(z_i=k|\text{ others }) = \frac{n_k^{-i}}{n-1+\alpha}f(x_i|z_j=k)$<br>$ p(z_i=k_{empty}|\text{ others }) = \frac{\alpha}{n-1+\alpha}f(x_i|\{\})$</p><ul><li>The actual infinite limit of the finite mixture model has problems: any particular cluster will get a mixing proportion of 0.</li><li>Better way for the infinite limit thinking:<ol><li>Chinese restaurant process</li><li>Stick-breaking construction</li></ol></li><li>Both are infinite dimensional Dirichlet distributions of Dirichlet Process(DP) </li></ul><h3 id="Chinese-Restaurant-Process"><a href="#Chinese-Restaurant-Process" class="headerlink" title="Chinese Restaurant Process"></a>Chinese Restaurant Process</h3><h4 id="Partitions"><a href="#Partitions" class="headerlink" title="Partitions"></a>Partitions</h4><p>A partition $\varrho$ of a set $S$ is</p><ul><li>A disjoint family of non-empty of subsets $S$ whose union in $S$</li><li>e.g. $S = \{ A,B,C,D,E,F,G  \}$</li><li>e.g. $\varrho = \{ \{A\},\{B,C\},\{D,E,F,G\}  \}$</li><li>The set of all partitions of $S$ is $\mathcal{P}_S$</li><li><em>Random partitions</em> are random variables taking values in $\mathcal{P}_S$</li></ul><h4 id="Chinese-Restaurant-Process-1"><a href="#Chinese-Restaurant-Process-1" class="headerlink" title="Chinese Restaurant Process"></a>Chinese Restaurant Process</h4><ul><li>Each customer comes into restaurant and sits at a table<br>$P(\text{sit at table }c) = \frac{n_c}{\alpha+\sum_{c\in \varrho} n_c} $<br>$P(\text{sit at new table}) = \frac{\alpha}{\alpha+\sum_{c\in \varrho} n_c} $</li><li>Customers correspond to element $S$, tables correspond to clusters in $\varrho$</li><li>Rich-gets-richer: larger clusters are more likely to attract more customers</li><li>Multiplying conditional probabilities together, the overall probability of $\varrho$ (exchangeable partition probability function (EPPF)) is<br>$P(\varrho|\alpha)=\frac{\alpha^{|\varrho|}\Gamma(\alpha)}{\Gamma(n+\alpha)}\Pi_{c\in \varrho} \Gamma(|c|) $<br>The distribution over the partition does not relate to the order of the customers enter the restaurant, and it only depends on the number of the clusters and the size of the clusters. </li></ul><h5 id="Number-of-Clusters"><a href="#Number-of-Clusters" class="headerlink" title="Number of Clusters"></a>Number of Clusters</h5><p>The prior mean and variance of $K$ are:<br>$ E[ \rho| \alpha, n ] = \alpha(\psi(\alpha+n)-\psi(\alpha))\approx \alpha \text{ log}(1+\frac{n}{\alpha}) $<br>$ V[ \rho| \alpha, n ] = \alpha(\psi(\alpha+n)-\psi(\alpha)) + \alpha^2(\psiâ€™(\alpha+n) - \psiâ€™(\alpha)) \approx \alpha \text{ log}(1+\frac{n}{\alpha}) $<br>where $\psi(\alpha) = \frac{\partial }{\partial \alpha}\text{ log }\Gamma(\alpha)$<br><img src="/images/2019/03/crp.png" alt="Sample Image Added via Markdown"></p><h4 id="CRP-Mixture-Model"><a href="#CRP-Mixture-Model" class="headerlink" title="CRP Mixture Model"></a>CRP Mixture Model</h4><p>Given a dataset $S$, we want to partition it into clusters of similar items. And we can use CRP prior over partitioins $\varrho$.</p><ul><li>$\varrho\sim \text{CRP }(\alpha)$</li><li>$\theta^*|\varrho \sim H \text{ for } c\in \rho$</li><li><p>$x_i|\theta,\varrho \sim F(\theta_c^*) \text{ for }c\in \varrho \text{ with } i\in c$</p></li><li><p>CRP prior over $\varrho$, iid prior $H$ over cluster parameters</p><center><br><img class="left" src="/images/2019/03/crpmm.png" width="40%" height="40%"><br></center></li><li><p>CRP can thus induce distribution over partitions: </p><ul><li>$P(\mathbf{z}|\alpha) = \frac{\Gamma(\alpha)}{\Pi_k\Gamma(\alpha/K)}\frac{\Pi_k\Gamma(n_k+\alpha/K)}{\Gamma(\alpha+n)}$ describes a partition of the data set into clusters, and a labelling of<br>each cluster with a mixture component index.</li><li>Induces a distribution over partitions $\varrho$ (without labelling) of the data set:<br>$P(\varrho|\alpha) = [K]^k_{-1}\frac{\Gamma(\alpha)}{\Gamma(\alpha+n)} \Pi_{c\in\varrho} \frac{\Gamma(|c|+\alpha/K)}{\Gamma(\alpha/K)}$</li><li>Assume $K\rightarrow \infty$ (maximum number of clusters in our partitions),  we get a proper distribution over partitions without a limit on the number of clusters:<br>$P (\varrho|\alpha) \rightarrow \frac{\alpha^{|\varrho|}\Gamma(\alpha)}{\Gamma(\alpha+n)} \Pi_{c\in\varrho} \Gamma(|c|)$ </li><li>So the induced distribution over partitions approach Chinese Restaurant Process. </li></ul></li></ul><p>In Chinese Restaurant Process is the distribution over partitions of a collection of objects, which can be used to build nonparametric model-based clustering models. It is related to the infinite limit of finite mixture models (solve the limitation of the originial infinite models).</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1] <a href="https://kourouklides.fandom.com/wiki/Bayesian_Nonparametrics" target="_blank" rel="noopener">Bayesian Nonparametrics Fandom Page</a><br>[2] <a href="http://blog.bogatron.net/blog/2014/02/02/visualizing-dirichlet-distributions/" target="_blank" rel="noopener">Visualizing Dirichlet Distributions with Matplotlib</a><br>[3] <a href="https://www.youtube.com/watch?v=dNeW5zoNJ7g" target="_blank" rel="noopener">Bayesian Nonparametrics 1 - Yee Whye Teh - MLSS 2013 TÃ¼bingen</a></p>]]></content>
      
      
      <categories>
          
          <category> Statistics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Statistics </tag>
            
            <tag> Nonparametric </tag>
            
            <tag> Chinese Restaurant </tag>
            
            <tag> Indian Buffet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Why are Gaussian Process Models called Nonparametric?</title>
      <link href="/2019/02/19/GaussianProcess/"/>
      <url>/2019/02/19/GaussianProcess/</url>
      
        <content type="html"><![CDATA[<p>As we know, <a href="https://en.wikipedia.org/wiki/Gaussian_process" target="_blank" rel="noopener">Gaussian processes</a> modeling is often refer to as nonparametric modeling. But why? It has parameters in its covariance kernel:<br><span>$$\begin{align*}K(x_i,x_j) = h^2\text{exp}\left(\frac{-(x_i-x_j)^2}{\lambda^2}\right)\end{align*}$$</span><!-- Has MathJax --><br>From the example of Gaussian kernel above, $h$ and $\lambda$ are the hyperparameters.<br><a id="more"></a></p><h3 id="Some-Points"><a href="#Some-Points" class="headerlink" title="Some Points:"></a>Some Points:</h3><ul><li>â€œParametricâ€ versus â€œNonparametricâ€ are terms that do not apply to particular processes, they apply to the entire family of processes that could be fit to data. </li><li>So if thereâ€™s no limit tothe number of parameters, we are estimating the parameter of infinite dimension, it turns out to be a nonparametric problem. </li><li>Models can be nonparametric in one sense and parametric in another (e.g. semiparametric models). </li><li>Nonparametric just means we cannot describe the model by a fixed set of parameters but they still can have parameters. </li><li>Return back to the example above, $h$ and $\lambda$ are not parameters because they do not represent anything physical about the data itself. They just parametrize the covariance. </li></ul><p><br></p><h3 id="Relevant-Materials"><a href="#Relevant-Materials" class="headerlink" title="Relevant Materials:"></a>Relevant Materials:</h3><p>[1] Anna Scaife: Machine Learning: Gaussian Process Modelling in Python <a href="https://www.youtube.com/watch?v=UpsV1y6wMQ8&amp;feature=youtu.be&amp;t=1386" target="_blank" rel="noopener">[lecture]</a><br>[2] Yee Whye Teh: Bayesian Nonparametric <a href="http://videolectures.net/mlss2011_teh_nonparametrics/" target="_blank" rel="noopener">[lecture]</a></p>]]></content>
      
      
      <categories>
          
          <category> Statistics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Nonparametric </tag>
            
            <tag> Gaussian Process </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Make a Birthday Cake in R</title>
      <link href="/2019/01/29/Birthday/"/>
      <url>/2019/01/29/Birthday/</url>
      
        <content type="html"><![CDATA[<p>Today is my friendâ€™s BIG DAY! Happy Birthday!!! He is one of the four people who changed my life (yes four people have seminal impact in my life so far). I want to thank him for all the things he has done for me and all the opportunites he gave me. So I decide to make a birthday cake for him. </p><p>Here it is!!!<br><a id="more"></a><br><img src="/images/2019/01/cake.png" alt="Sample Image Added via Markdown"></p><p>Initially I was trying to make a 3D fancy cake in MATLAB, after one-hour struggling I realized how bad I am in MATLABâ€¦ so I turned to R for a simple 2D tasty cake lol.<br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">candle = <span class="keyword">function</span>(pos)</span><br><span class="line">&#123;</span><br><span class="line">  x=pos[<span class="number">1</span>]</span><br><span class="line">  y=pos[<span class="number">2</span>]</span><br><span class="line">  rect(x,y,x+<span class="number">.2</span>,y+<span class="number">2</span>,col=<span class="string">"red"</span>)</span><br><span class="line">  polygon(c(x+<span class="number">.05</span>,x-<span class="number">.1</span>,x+<span class="number">.1</span>,x+<span class="number">.3</span>,x+<span class="number">.15</span>,x+<span class="number">0.05</span>), c(y+<span class="number">2</span>,y+<span class="number">2.3</span>,y+<span class="number">2.6</span>,y+<span class="number">2.3</span>,y+<span class="number">2</span>,y+<span class="number">2</span>),col=<span class="string">"#EECF08"</span>)</span><br><span class="line">  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">plot(c(<span class="number">0</span>,<span class="number">10</span>), c(<span class="number">0</span>,<span class="number">10</span>),type=<span class="string">"n"</span>, bty=<span class="string">"n"</span>,xaxt=<span class="string">"n"</span>,yaxt=<span class="string">"n"</span>, main=<span class="string">"Cake"</span>, xlab=<span class="string">""</span>,ylab=<span class="string">""</span>)</span><br><span class="line">draw.ellipse(<span class="number">5</span>,<span class="number">2</span>,col=<span class="string">"#73EA71"</span>,a=<span class="number">4.4</span>,b=<span class="number">1.7</span>,border=<span class="number">1</span>)</span><br><span class="line">draw.ellipse(<span class="number">5</span>,<span class="number">2</span>,col=<span class="string">"#BAE5FF"</span>,a=<span class="number">4</span>,b=<span class="number">1.4</span>,border=<span class="number">1</span>)</span><br><span class="line">rect(<span class="number">1</span>,<span class="number">2</span>,<span class="number">9</span>,<span class="number">5</span>,col=<span class="string">"#BAE5FF"</span>,border=<span class="string">"#BAE5FF"</span>)</span><br><span class="line">lines(c(<span class="number">1</span>,<span class="number">1</span>),c(<span class="number">2</span>,<span class="number">5</span>))</span><br><span class="line">lines(c(<span class="number">9</span>,<span class="number">9</span>),c(<span class="number">2</span>,<span class="number">5</span>))</span><br><span class="line">draw.ellipse(<span class="number">5</span>,<span class="number">5</span>,col=<span class="string">"#FCD2DE"</span>,a=<span class="number">4</span>,b=<span class="number">1.4</span>)</span><br><span class="line"></span><br><span class="line">candle(c(<span class="number">2.5</span>,<span class="number">4.5</span>))</span><br><span class="line">candle(c(<span class="number">3</span>,<span class="number">5</span>))</span><br><span class="line">candle(c(<span class="number">4</span>,<span class="number">4.5</span>))</span><br><span class="line">candle(c(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">candle(c(<span class="number">6</span>,<span class="number">4.5</span>))</span><br><span class="line">candle(c(<span class="number">7</span>,<span class="number">5.2</span>))</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> JUJUs </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Birthday </tag>
            
            <tag> R </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Blog Theme æ—¥ç¥ x é…’ç¥</title>
      <link href="/2019/01/27/BlogTheme/"/>
      <url>/2019/01/27/BlogTheme/</url>
      
        <content type="html"><![CDATA[<p>My current blog theme is æ—¥ç¥ x é…’ç¥ (Apollinian and Dionysian). </p><p><strong>Apollo:</strong> the God of the sun, truth, light, and logic, is the namesake for the first, ordered, half. This is the half that covers everything which is structured.<br><strong>Dionysus:</strong> the God of wine, festivals, and madness lends his name to the later, frenzied, half. The Dionysian doesnâ€™t categorize and tends to blur the boundaries between the self and nature with emotion.</p><p>I consent Nietzscheâ€™s idea that we need both in our life to be complete people. The fusion of these two drives allows the tremendous frenzied energy of the Dionysian to be applied constructively inside an Apollonian framework.</p><a id="more"></a><p>Want more?<br>Feed yourself  ğŸ¥âœ¨<em>The Birth of Tragedy</em> <a href="http://www.gutenberg.org/files/51356/51356-h/51356-h.htm" target="_blank" rel="noopener">[online reading]</a>âœ¨ </p><hr><p>æˆ‘å½“å‰çš„åšå®¢ä¸»é¢˜æ˜¯ æ—¥ç¥ x é…’ç¥<br>æ—¥ç¥ï¼šå¤ªé˜³ç¥é˜¿æ³¢ç½—ï¼Œæ—¥ç¥ç²¾ç¥ç»™äººä»¥å…‰æ˜ç¾å¥½ï¼Œåº„é‡çº¯ç²¹çš„æ¢¦å¹»æ„Ÿ<br>é…’ç¥ï¼šç‹„å¥¥å°¼ç´¢æ–¯ï¼Œé…’ç¥ç²¾ç¥æ˜¯ä¸€ç§ä¸å¯åçŠ¶åˆä¸å¯éåˆ¶çš„ç”Ÿå‘½çš„æ¿€æƒ…ä¸å†²åŠ¨</p><p>æˆ‘è®¤åŒå°¼é‡‡çš„ä¸–ç•ŒäºŒå…ƒæ€§æ‚²å‰§æœ¬æºï¼Œä¸–ç•Œå°±åƒç¿»è…¾çš„æµ·æ´‹ï¼Œæ¯ä¸ªç¬é—´éƒ½åœ¨å˜åŒ–ï¼Œæ²¡æœ‰ä»€ä¹ˆä¸œè¥¿æ˜¯æ°¸æ’ä¸æœ½çš„ã€‚æ—¥ç¥ä¸é…’ç¥ç²¾ç¥çš„å¯è´µåœ¨äºä¸¤ç§å¯¹ç«‹ç²¾ç¥çš„çŸ›ç›¾ï¼Œä¹Ÿæ˜¯ç”Ÿå‘½æœ¬çœŸçš„ä½“ç°ã€‚é¢å¯¹äººç”Ÿæœ¬è´¨çš„è‹¦éš¾ï¼Œé€šè¿‡éŸ³ä¹ã€æ‚²å‰§ä¸é…’ç¥ç²¾ç¥ï¼ŒæŠŠè¿™ä¸€åˆ‡å¹»åŒ–ä¸ºç¾å­¦ã€‚</p><p>æƒ³äº†è§£æ›´å¤šï¼Ÿ<br>è¯·æ— æœŸé™èµå‘³  ğŸ¥âœ¨<em>æ‚²å‰§çš„è¯ç”Ÿï¼ˆè‹±æ–‡ç‰ˆï¼‰</em><a href="http://www.gutenberg.org/files/51356/51356-h/51356-h.htm" target="_blank" rel="noopener">[åœ¨çº¿é˜…è¯»]</a>âœ¨ </p><hr><p>Historical theme: è§å¤©åœ° è§ä¼—ç”Ÿ è§è‡ªå·±</p>]]></content>
      
      
      <categories>
          
          <category> Philosophy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Philosophy </tag>
            
            <tag> Nietzsche </tag>
            
            <tag> å°¼é‡‡ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Add Article Views to Your Hexo Blog</title>
      <link href="/2019/01/26/Hexo-View/"/>
      <url>/2019/01/26/Hexo-View/</url>
      
        <content type="html"><![CDATA[<p>When we write articles, we always want to know the popularity of our work. As a <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> blogger, I tried several different visitor counter system and here, like the previous post, I will summarize two options for visitor counts to each articles. But this time, I have preference to a specific one. Guess which one?</p><ul><li>Option 1: LeanCloud Counter </li><li>Option 2: Firestore Counter</li></ul><h2 id="LeanCloud-Counter"><a href="#LeanCloud-Counter" class="headerlink" title="LeanCloud Counter"></a>LeanCloud Counter</h2><p>LeanCloud is a Chinese friendly system, and unfortunately it doesnâ€™t have English version. Here Iâ€™ll try to make it as clear as possible. For those whoâ€™ve read the previous post, it is not hard for you to find out <strong>Valine Comment</strong> actually use <strong>LeanCloud</strong> system so if you already have <strong>Valine</strong>, you donâ€™t need to create the account again.<br><a id="more"></a></p><h3 id="Vanilla-Steps"><a href="#Vanilla-Steps" class="headerlink" title="Vanilla Steps"></a>Vanilla Steps</h3><ol><li><p>Creat an account in <a href="https://leancloud.cn/dashboard/login.html#/signup" target="_blank" rel="noopener">LeanCloud</a>.</p></li><li><p>Create new application in the console, and name it.<br><img src="/images/2019/01/l1.png" alt="Sample Image Added via Markdown"></p></li><li><p>In the new Application you built, create a Class named <strong>Counter</strong>.<br><em>Note: the class name must be Counter based on the source code of NexT Theme.</em><br><img src="/images/2019/01/l2.png" alt="Sample Image Added via Markdown"></p></li><li><p>Go to <strong>Apply Key</strong> section in the <strong>Setting</strong>, and you will get your APP ID/APP Key.<br><img src="/images/2019/01/s4.png" alt="Sample Image Added via Markdown"></p></li><li><p>Paste <code>App ID</code> and <code>App Key</code> to theme config file <strong>_config.yml</strong> like this:</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">leancloud_visitors:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  app_id:</span> <span class="string">YOUR</span> <span class="string">APP</span> <span class="string">ID</span></span><br><span class="line"><span class="attr">  app_key:</span> <span class="string">YOUR</span> <span class="string">APP</span> <span class="string">KEY</span></span><br><span class="line">  <span class="comment"># Dependencies: https://github.com/theme-next/hexo-leancloud-counter-security</span></span><br><span class="line">  <span class="comment"># If you don't care about security in lc counter and just want to use it directly</span></span><br><span class="line">  <span class="comment"># (without hexo-leancloud-counter-security plugin), set the `security` to `false`.</span></span><br><span class="line"><span class="attr">  security:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  betterPerformance:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure></li><li><p>Go to <strong>Security Center</strong> to set your security domain that allows you to call the server resource through the JavaScript SDK under the domain name.<br><img src="/images/2019/01/s5.png" alt="Sample Image Added via Markdown"><br><strong>Congratulations! You are all settled!!!</strong><br><strong>Well, if you want more, continue reading!</strong></p></li></ol><h3 id="Advanced-Steps"><a href="#Advanced-Steps" class="headerlink" title="Advanced Steps"></a>Advanced Steps</h3><ol start="6"><li>Thanks to <em>@Lawrence Ye</em> for the contribution of this section. The Leancloud visitor counter plugin used in NexT has a big security bug, by which someone could change your visitor number easily and even add/delete records in your database. This bug is found by <a href="https://github.com/LEAFERx/" target="_blank" rel="noopener">LEAFERx</a> and confirmed by <a href="https://github.com/ivan-nginx" target="_blank" rel="noopener">Ivan.Nginx</a>. This bug could only be fixed manually.<br><em>Note: All NexT sites using Leancloud visitor counter that are not fixed and other sites integrated this function by similiar ways are considered unsecurity.</em></li></ol><ul><li><p>Use Online Editor in the deploy section.<br><img src="/images/2019/01/b1.png" alt="Sample Image Added via Markdown"></p></li><li><p>After click <strong>Online Edit</strong> button, create function as showed below.<br><img src="/images/2019/01/b2.png" alt="Sample Image Added via Markdown"></p></li><li><p>In the pop up window, choose <strong>Hook</strong>, and then select <strong>beforeUpdate</strong> and <strong>Counter</strong>. In the code block paste the following code:</p><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> query = <span class="keyword">new</span> AV.Query(<span class="string">"Counter"</span>);</span><br><span class="line"><span class="keyword">if</span> (request.<span class="keyword">object</span>.updatedKeys.indexOf(<span class="string">'time'</span>) !== <span class="number">-1</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> query.<span class="keyword">get</span>(request.<span class="keyword">object</span>.id).then(function (obj) &#123;</span><br><span class="line">        <span class="keyword">if</span> (obj.<span class="keyword">get</span>(<span class="string">"time"</span>) + <span class="number">1</span> !== request.<span class="keyword">object</span>.<span class="keyword">get</span>(<span class="string">"time"</span>)) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> AV.Cloud.Error(<span class="string">'Invalid update!'</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>  <img src="/images/2019/01/b3.png" alt="Sample Image Added via Markdown"></p></li><li><p>Deploy the <code>Hook Counter</code> you just created.<br><img src="/images/2019/01/b4.png" alt="Sample Image Added via Markdown"></p></li><li><p>Click the <strong>Deploy</strong> button on the pop up window.<br><img src="/images/2019/01/b5.png" alt="Sample Image Added via Markdown"></p></li><li><p>After deploy completed, close the window.<br><img src="/images/2019/01/b6.png" alt="Sample Image Added via Markdown"></p></li></ul><ol start="7"><li>Go to theme config file <strong>_config.yml</strong>, set <code>leancloud_visitors: security</code> to <strong>true</strong>:<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">leancloud_visitors:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  app_id:</span> <span class="string">YOUR</span> <span class="string">APP</span> <span class="string">ID</span></span><br><span class="line"><span class="attr">  app_key:</span> <span class="string">YOUR</span> <span class="string">APP</span> <span class="string">KEY</span></span><br><span class="line"><span class="attr">  security:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  betterPerformance:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure></li></ol><ul><li><p>Due to the limitation of LeanCloud developerâ€™s plan in requsted thread amount and running time, counter number could sometimes be loaded very slowly. The situation can be enhanced via setting <code>betterPerformance: true</code>. This that scenario, we need to install an extra plugin:</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">npm</span> <span class="selector-tag">install</span> <span class="selector-tag">hexo-leancloud-counter-security</span> <span class="selector-tag">--save</span></span><br></pre></td></tr></table></figure></li><li><p>Go to <strong>site</strong> config file <strong>_config.yml</strong>, add security config.</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">leancloud_counter_security:</span></span><br><span class="line"><span class="attr">  enable_sync:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  app_id:</span> <span class="string">YOUR</span> <span class="string">APP</span> <span class="string">ID</span></span><br><span class="line"><span class="attr">  app_key:</span> <span class="string">YOUR</span> <span class="string">APP</span> <span class="string">KEY</span></span><br><span class="line"><span class="attr">  username:</span> </span><br><span class="line"><span class="attr">  password:</span></span><br></pre></td></tr></table></figure></li><li><p>Now the security config has been installed. Give a <code>username</code> and <code>password</code>(no need to be the same as leancloud account). They will be used in the hexo deploying.</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">hexo</span> <span class="selector-tag">lc-counter</span> <span class="selector-tag">register</span> &lt;&lt;<span class="selector-tag">username</span>&gt;&gt; &lt;&lt;<span class="selector-tag">password</span>&gt;&gt;</span><br></pre></td></tr></table></figure></li><li><p>Then go to <strong>site</strong> config file <strong>_config.yml</strong> again, and put the <code>username</code> and <code>password</code> you just set.</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">leancloud_counter_security:</span></span><br><span class="line"><span class="attr">  enable_sync:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  app_id:</span> <span class="string">YOUR</span> <span class="string">APP</span> <span class="string">ID</span></span><br><span class="line"><span class="attr">  app_key:</span> <span class="string">YOUR</span> <span class="string">APP</span> <span class="string">KEY</span></span><br><span class="line"><span class="attr">  username:</span> <span class="string">YOUR</span> <span class="string">USERNAME</span> </span><br><span class="line"><span class="attr">  password:</span> <span class="string">YOUR</span> <span class="string">PASSWORD</span></span><br></pre></td></tr></table></figure></li><li><p>Add deployer in <code>deploy</code> section of <strong>site</strong> config file.</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="comment"># other deployer</span></span><br><span class="line"><span class="attr">  - type:</span> <span class="string">leancloud_counter_security_sync</span></span><br></pre></td></tr></table></figure></li></ul><ol start="8"><li>Return back to LeanCloud console. Go to <strong>_User</strong> section and check if your record is added.<br><img src="/images/2019/01/b7.png" alt="Sample Image Added via Markdown"></li></ol><ul><li><p>Set permission in the <strong>Counter</strong> section.<br><img src="/images/2019/01/b8.png" alt="Sample Image Added via Markdown"></p></li><li><p>In <strong>create</strong> setting, select <strong>specific user</strong> and enter your username.<br><img src="/images/2019/01/b9.png" alt="Sample Image Added via Markdown"></p></li><li><p>In <strong>delete</strong> setting, select <strong>specific user</strong> and close the window.<br><img src="/images/2019/01/b10.png" alt="Sample Image Added via Markdown"><br><strong>Congratulations! Now the database can only be changed by you.</strong></p></li></ul><h2 id="Firestore-Counter"><a href="#Firestore-Counter" class="headerlink" title="Firestore Counter"></a>Firestore Counter</h2><p><a href="https://firebase.google.com/docs/firestore/" target="_blank" rel="noopener">Firestore</a> for <a href="https://github.com/theme-next" target="_blank" rel="noopener">NexT</a> blog article counter is a fairly new tool developed by <em>@maple3142</em>. It is a flexible, scalable database for mobile, web, and server development from Firebase and Google Cloud Platform. Like Firebase Realtime Database, it keeps your data in sync across client apps through realtime listeners and offers offline support for mobile and web so you can build responsive apps that work regardless of network latency or Internet connectivity. </p><h3 id="Steps"><a href="#Steps" class="headerlink" title="Steps"></a>Steps</h3><ol><li><p>Create a project in <a href="https://console.firebase.google.com/u/0/" target="_blank" rel="noopener">Firebase</a>.<br><img src="/images/2019/01/f1.png" alt="Sample Image Added via Markdown"></p></li><li><p>Go to <strong>Database</strong> section create a database.<br><img src="/images/2019/01/f2.png" alt="Sample Image Added via Markdown"></p></li><li><p>We first start in test mode (we will modify the security level later).<br><img src="/images/2019/01/f3.png" alt="Sample Image Added via Markdown"></p></li><li><p>Go to the <strong>Project setting</strong>.<br><img src="/images/2019/01/f4.png" alt="Sample Image Added via Markdown"></p></li><li><p>Get your <code>projectid</code> and <code>apikey</code>.<br><img src="/images/2019/01/f5.png" alt="Sample Image Added via Markdown"></p></li><li><p>Go to your theme config file <strong>_config.yml</strong>, and set your firestore section value as below.</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">firestore:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  collection:</span> <span class="string">articles</span> <span class="comment">#required, a string collection name to access firestore database</span></span><br><span class="line"><span class="attr">  apiKey:</span> <span class="string">YOUR</span> <span class="string">API</span> <span class="string">KEY</span></span><br><span class="line"><span class="attr">  projectId:</span> <span class="string">YOUR</span> <span class="string">PROJECT</span> <span class="string">ID</span></span><br><span class="line"><span class="attr">  bluebird:</span> <span class="literal">true</span> <span class="comment">#enable this if you want to include bluebird 3.5.1(core version) Promise polyfill</span></span><br></pre></td></tr></table></figure></li><li><p>Go back to Firebase console, enter <strong>Rules</strong> in <strong>Database</strong> section and slightly modify the code, and then publish it.<br><em>Modify: Change document to article to constrain the collection.</em><br><img src="/images/2019/01/f6.png" alt="Sample Image Added via Markdown"></p></li><li><p>Note: <a href="https://github.com/theme-next" target="_blank" rel="noopener">Hexo NexT</a> theme version should be above <code>v5.1.3</code>, otherwise you may encounter the issue that count numbers appear on index page. <a href="https://github.com/iissnan/hexo-theme-next/pull/1984" target="_blank" rel="noopener">Bug#1984</a> is fixed <a href="https://github.com/iissnan/hexo-theme-next/commit/3d4cccd048fd95bd7b5b10ac51c9180ea5341721" target="_blank" rel="noopener">here</a>.</p><p><strong>Congratulations! Everything is settled now!</strong></p></li></ol><h2 id="Comparison-amp-My-Choice"><a href="#Comparison-amp-My-Choice" class="headerlink" title="Comparison &amp; My Choice"></a>Comparison &amp; My Choice</h2><p>If you read the whole rambling steps above, I bet you would go for <strong>Firestore Counter</strong> as I did. Actually I failed to make <strong>Firestore Counter</strong> work at first. So I went for <strong>LeanCloud Counter</strong>. It works well, but very soon I noticed a problem â€“ the weboage loading is very slow!<br>Use <code>alt</code>+<code>cmd</code>+<code>i</code> to open the console</p><p><img src="/images/2019/01/console.png" alt="Sample Image Added via Markdown"><br>It is obvious that <strong>LeanCloud Counter</strong> gave me the trouble. One possible reason could be the system is not smooth enough for international users because most users located in China donâ€™t have this problem. So I went for <strong>Firestore Counter</strong> again. This time I made it, and fix the bug as I mentioned in <em>Step 8</em> of the previous section. </p><p>Apart from this, original <strong>LeanCloud Counter</strong> has a big <a href="https://github.com/theme-next/hexo-leancloud-counter-security" target="_blank" rel="noopener">security bug</a>, by which someone could change your visitor number easily and even add/delete records in your database. So you need to do <em>Advance Steps</em> I added in Section 1.2.</p><p>I also notice for <strong>Firestore Counter</strong>, refreshing webpage wonâ€™t change the count number, while for <strong>LeanCloud Counter</strong> you can easily modify the record number via visit the same page for multiple times. Somehow <strong>Firestore Counter</strong> recording is closer to the truth. </p><p>In all, I vote for <strong>Firestore Counter</strong> based on the following merits:</p><ul><li>Non-repetitive counter number</li><li>English support console</li><li>Easier setting process</li><li>Faster and smoother</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] <a href="https://leaferx.online/2018/03/16/lc-security-en/" target="_blank" rel="noopener">A Guide to fix a security bug in Leancloud visitor counter</a><br>[2] <a href="https://blog.maple3142.net/2017/11/04/hexo-next-readcount/" target="_blank" rel="noopener">Hexo NexT ä¸»é¡Œçš„é–±è®€æ¬¡æ•¸çµ±è¨ˆ</a><br>[3] <a href="http://yohnz.win/2016/02/17/hexo%E6%89%93%E5%BC%80%E6%85%A2%E7%9A%84%E9%97%AE%E9%A2%98/" target="_blank" rel="noopener">hexo NEXTä¸»é¢˜æ‰“å¼€æ…¢çš„é—®é¢˜</a></p><h2 id="Relevant-Articles"><a href="#Relevant-Articles" class="headerlink" title="Relevant Articles"></a>Relevant Articles</h2><p>[1] <a href="https://qiuyiwu.github.io/2019/01/25/Hexo-Comment/">Add Comments Section to Your Hexo Blog</a><br>[2] <a href="https://qiuyiwu.github.io/2019/01/25/Hexo-LocalSearch/">Create Local Search for Your Hexo Blog</a></p>]]></content>
      
      
      <categories>
          
          <category> Hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> blog </tag>
            
            <tag> Firestore </tag>
            
            <tag> Firebase </tag>
            
            <tag> LeanCloud </tag>
            
            <tag> visitor counts </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Add Comments Section to Your Hexo Blog</title>
      <link href="/2019/01/25/Hexo-Comment/"/>
      <url>/2019/01/25/Hexo-Comment/</url>
      
        <content type="html"><![CDATA[<p>Hexo as a static site generator has become a popular choice for bloggers. When using something like WordPress or Medium you get a comment section out of the box sponstaneously, it is not as straightforward in a static environment. In this article, I will summarize two options to provide your audience with a way to leave a comment.</p><ul><li>Option 1: Disqus Comment</li><li>Option 2: Valine Comment</li></ul><h2 id="Disque-Comment"><a href="#Disque-Comment" class="headerlink" title="Disque Comment"></a>Disque Comment</h2><p>Disqus Comment is a great way to let people comment on your articles. Disqus has automatic setup for nearly every Content Management System (CMS) except Hexo.  Here is the easy way to set this up:<br><a id="more"></a></p><ol><li>Create an account in <a href="https://disqus.com/" target="_blank" rel="noopener">Disqus</a>. As part of that process, you will choose a shortname value. Copy that value. Itâ€™s a little tough to find because there are TWO setting pages in Disqus - one for your account and one for your site.</li></ol><ul><li><p>At top right choose <strong>Admin</strong>:<br><img class="left" src="/images/2019/01/s1.png" width="50%" height="50%"></p></li><li><p>You can find your shortname on the <strong>Settings</strong> page in the <strong>admin</strong> for your site here:<br><img class="left" src="/images/2019/01/s2.png" width="80%" height="80%"></p></li><li><p>Your shortname is uniquely identified for your website on Disqus. Once set up, it cannot be changed.<br><img class="left" src="/images/2019/01/s3.png" width="80%" height="80%"></p></li></ul><ol start="2"><li><p>Open your <strong>_config.yml</strong>, and then enter the <code>disqus_shortname</code> parameter and assign it to your shortname, like so:</p><figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">disqus:</span><br><span class="line">  enable: <span class="built_in">true</span></span><br><span class="line">  shortname: YOUR SHORTNAME</span><br><span class="line">  count: <span class="built_in">true</span></span><br></pre></td></tr></table></figure></li><li><p>If you use <a href="https://github.com/theme-next" target="_blank" rel="noopener">NexT Theme</a>, <strong>congratulation! everything is settled now.</strong><br>If you build your blog from scratch, then now you need to open <strong>footer.ejs</strong> file, and write the universal disqus code as below:</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">%</span> <span class="attr">if</span> (<span class="attr">config.disqus_shortname</span>)&#123; %&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="undefined"></span></span><br><span class="line"><span class="xml">  var disqus_shortname = '<span class="tag">&lt;<span class="name">%=</span> <span class="attr">config.disqus_shortname</span> %&gt;</span>';</span></span><br><span class="line"><span class="xml">  <span class="tag">&lt;<span class="name">%</span> <span class="attr">if</span> (<span class="attr">page.permalink</span>)&#123; %&gt;</span></span></span><br><span class="line"><span class="xml">  var disqus_url = '<span class="tag">&lt;<span class="name">%=</span> <span class="attr">page.permalink</span> %&gt;</span>';</span></span><br><span class="line"><span class="xml">  <span class="tag">&lt;<span class="name">%</span> &#125; %&gt;</span></span></span><br><span class="line"><span class="undefined">  (function()&#123;</span></span><br><span class="line"><span class="undefined">    var dsq = document.createElement('script');</span></span><br><span class="line"><span class="undefined">    dsq.type = 'text/javascript';</span></span><br><span class="line"><span class="undefined">    dsq.async = true;</span></span><br><span class="line"><span class="xml">    dsq.src = '//go.disqus.com/<span class="tag">&lt;<span class="name">%</span> <span class="attr">if</span> (<span class="attr">page.comments</span>)&#123; %&gt;</span>embed.js<span class="tag">&lt;<span class="name">%</span> &#125; <span class="attr">else</span> &#123; %&gt;</span>count.js<span class="tag">&lt;<span class="name">%</span> &#125; %&gt;</span>';</span></span><br><span class="line"><span class="undefined">    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);</span></span><br><span class="line"><span class="undefined">  &#125;)();</span></span><br><span class="line"><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">%</span> &#125; %&gt;</span></span><br></pre></td></tr></table></figure></li></ol><p>Then paste <code>&lt;div id=&quot;disqus_thread&quot;&gt;&lt;/div&gt;</code> where you want the comments to go.<br><strong>Now you are settled!</strong></p><h2 id="Valine-Comment"><a href="#Valine-Comment" class="headerlink" title="Valine Comment"></a>Valine Comment</h2><p><a href="https://valine.js.org/en/hexo.html" target="_blank" rel="noopener">Valine</a> was designed in August 7, 2017. Itâ€™s a fast, simple &amp; efficient Leancloud-based no-back-end comment system. It is, theoretically, but not limited to static blog. Hexo, Jekyll, Typecho, Hugo and other blog programs are currently compatiable with Valine. </p><p><em>Note: Valine is a Chinese friendly system.</em></p><h3 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h3><ul><li>Emoji ğŸ˜‰</li><li>High speed.</li><li>Safe by default.</li><li>No server-side implementation.</li><li>Support for full markdown syntax.</li><li>Simple and lightweight (15kB gzipped).</li><li>Article reading statistics <code>v1.2.0+</code></li></ul><h3 id="Steps"><a href="#Steps" class="headerlink" title="Steps"></a>Steps</h3><ol><li><p>Creat an account in <a href="https://leancloud.cn/dashboard/login.html#/signup" target="_blank" rel="noopener">LeanCloud</a>.</p></li><li><p>Create new application in LeanCloud <a href="https://leancloud.cn/dashboard/applist.html#/newapp" target="_blank" rel="noopener">here</a>, and you will get APP ID/APP Key.<br>Details: Name your application, then go to <strong>Setting</strong>, then <strong>Apply Key</strong>.</p></li></ol><p><img src="/images/2019/01/s4.png" alt="Sample Image Added via Markdown"></p><ol start="3"><li>If you are not using <a href="https://github.com/theme-next" target="_blank" rel="noopener">NexT Theme</a>, jump to <strong>Step 5</strong> directly.<br>For <a href="https://github.com/theme-next" target="_blank" rel="noopener">NexT Theme</a> user, configure this plugin in <strong>_config.yml</strong> file of your theme.</li></ol><p>For theme <strong>version 5</strong>, it looks like:<br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">valine:</span></span><br><span class="line"><span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">appid:</span> <span class="string">YOUR</span> <span class="string">APPID</span></span><br><span class="line"><span class="attr">appkey:</span> <span class="string">YOUR</span> <span class="string">APPKEY</span></span><br><span class="line"><span class="attr">language:</span> <span class="string">'en'</span> </span><br><span class="line"><span class="attr">notify:</span> <span class="literal">false</span> <span class="comment"># mail notifier , https://github.com/xCss/Valine/wiki</span></span><br><span class="line"><span class="attr">verify:</span> <span class="literal">false</span> <span class="comment"># Verification code</span></span><br><span class="line"><span class="attr">placeholder:</span> <span class="string">Comment</span> <span class="string">here...</span> <span class="comment"># comment box placeholder</span></span><br><span class="line"><span class="attr">avatar:</span> <span class="string">retro</span> <span class="comment"># gravatar style: ''/mm/identicon/monsterid/wavatar/retro/hide</span></span><br><span class="line"><span class="attr">guest_info:</span> <span class="string">nick,mail,link</span> <span class="comment"># custom comment header</span></span><br><span class="line"><span class="attr">pageSize:</span> <span class="number">10</span> <span class="comment"># pagination size</span></span><br></pre></td></tr></table></figure></p><p>For theme <strong>version 6</strong>, it looks slightly different:<br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">valine:</span>  </span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  app_id:</span> <span class="string">YOUR</span> <span class="string">APPID</span></span><br><span class="line"><span class="attr">  app_key:</span> <span class="string">YOUR</span> <span class="string">APPKEY</span></span><br><span class="line"><span class="attr">  language:</span> <span class="string">'en'</span> </span><br><span class="line"><span class="attr">  notify:</span> <span class="literal">false</span> <span class="comment"># mail notifier , https://github.com/xCss/Valine/wiki</span></span><br><span class="line"><span class="attr">  verify:</span> <span class="literal">false</span> <span class="comment"># Verification code</span></span><br><span class="line"><span class="attr">  placeholder:</span> <span class="string">Comment</span> <span class="string">here...</span> <span class="comment"># comment box placeholder</span></span><br><span class="line"><span class="attr">  avatar:</span> <span class="string">retro</span> <span class="comment"># gravatar style: ''/mm/identicon/monsterid/wavatar/retro/hide</span></span><br><span class="line"><span class="attr">  guest_info:</span> <span class="string">nick,mail,link</span> <span class="comment"># custom comment header</span></span><br><span class="line"><span class="attr">  pageSize:</span> <span class="number">10</span> <span class="comment"># pagination size</span></span><br></pre></td></tr></table></figure></p><p><em>Note: <code>appid</code> and <code>appkey</code> changed a little bit in two versions. If you dismatch the version, it wonâ€™t work.</em></p><ol start="4"><li>At last, go to <strong>Security Center</strong> to set your security domain that allows you to call the server resource through the JavaScript SDK under the domain name.</li></ol><p><img src="/images/2019/01/s5.png" alt="Sample Image Added via Markdown"></p><p><strong>Congratulations! You are all settled!!!</strong></p><ol start="5"><li><p>For non-<a href="https://github.com/theme-next" target="_blank" rel="noopener">NexT Theme</a> user, include the following HTML code in the appropriate location on the article page that requires the comment box:</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"//cdn1.lncld.net/static/js/3.0.4/av-min.js"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">'//unpkg.com/valine/dist/Valine.min.js'</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">    ...</span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"vcomment"</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="undefined"></span></span><br><span class="line"><span class="undefined">        new Valine(&#123;</span></span><br><span class="line"><span class="undefined">            el: '#vcomment' ,</span></span><br><span class="line"><span class="xml">            appId: '<span class="tag">&lt;<span class="name">APP_ID</span>&gt;</span>',</span></span><br><span class="line"><span class="xml">            appKey: '<span class="tag">&lt;<span class="name">APP_Key</span>&gt;</span>'</span></span><br><span class="line"><span class="undefined">        &#125;);</span></span><br><span class="line"><span class="undefined">    </span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br></pre></td></tr></table></figure><p>Valine has been released to npm and can be installed directly by command:</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Install leancloud's js-sdk</span></span><br><span class="line">npm install leancloud-storage --save</span><br><span class="line"><span class="comment"># Install valine</span></span><br><span class="line">npm install valine --save</span><br></pre></td></tr></table></figure></li></ol><p>More Information can be found <a href="https://valine.js.org/en/quickstart.html" target="_blank" rel="noopener">here</a>.</p><h2 id="Comparison-Between-Disqus-amp-Valine"><a href="#Comparison-Between-Disqus-amp-Valine" class="headerlink" title="Comparison Between Disqus &amp; Valine"></a>Comparison Between Disqus &amp; Valine</h2><p>There are some different merits between the two Comment systems.</p><p>For <strong>Disqus</strong>:</p><ul><li>Reactions support</li><li>Multi-language support</li><li>.gif/.png/.jpg/â€¦ support</li><li>Social media share (FB/Twitter)</li><li>Server-side implementation (Disqus/FB/Twitter/Google+)</li><li>Anoymous support (You can leave email for further connection)</li></ul><p><img src="/images/2019/01/v1.gif" alt="Sample Image Added via Markdown"></p><p>For <strong>Valine</strong>:</p><ul><li>Emoji supportğŸ˜‰</li><li>Preview allowed</li><li>Location tracked</li><li>Chinese friendly interface</li><li>Support for full markdown syntax</li><li>Anoymous support (You can leave email/website for further connection)</li></ul><p><img src="/images/2019/01/v2.gif" alt="Sample Image Added via Markdown"></p><h2 id="Some-Tips"><a href="#Some-Tips" class="headerlink" title="Some Tips"></a>Some Tips</h2><h3 id="Modify-Comment-background-and-size"><a href="#Modify-Comment-background-and-size" class="headerlink" title="Modify Comment background and size"></a>Modify Comment background and size</h3><p>Under route <code>theme/source/css/_custom/custom.styl</code>, we can modify the size and background of the comment section.<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.comments</span> &#123;</span><br><span class="line">    background-color: rgb(255, 255, 0);;</span><br><span class="line">    <span class="selector-tag">box-shadow</span>: 0<span class="selector-tag">px</span> 0<span class="selector-tag">px</span> 10<span class="selector-tag">px</span> 0<span class="selector-tag">px</span> <span class="selector-tag">rgb</span>(255, 255, 255);;</span><br><span class="line">    <span class="selector-tag">margin</span>: 50<span class="selector-tag">px</span> <span class="selector-tag">-50px</span> 20<span class="selector-tag">px</span> <span class="selector-tag">-50px</span>;</span><br><span class="line">    <span class="selector-tag">padding</span>: 2<span class="selector-tag">rem</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="Turn-off-Comment-in-other-page"><a href="#Turn-off-Comment-in-other-page" class="headerlink" title="Turn off Comment in other page"></a>Turn off Comment in other page</h3><p>You may notice <strong>Comment</strong> appears in your category and tag and other pages, which you do not want it happen.<br><img src="/images/2019/01/tag.png" alt="Sample Image Added via Markdown"></p><p>In this senario, you only need to false the comment with the command <code>comments: false</code> in front-matter block at the top of the file. Front-matter is a block of YAML or JSON at the beginning of the file that is used to configure settings for your writings. Front-matter is terminated by three dashes when written in YAML or three semicolons when written in JSON.</p><figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">title: tags</span><br><span class="line">date: <span class="number">2018</span>-<span class="number">01</span>-<span class="number">01</span> <span class="number">16</span>:<span class="number">55</span>:<span class="number">34</span></span><br><span class="line"><span class="keyword">type</span>: <span class="string">"tags"</span></span><br><span class="line">comments: <span class="built_in">false</span></span><br></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] <a href="https://www.bluelzy.com/articles/use_valine_for_your_blog.html" target="_blank" rel="noopener">ä¸ºä½ çš„HexoåŠ ä¸Šè¯„è®ºç³»ç»Ÿ-Valine</a><br>[2] <a href="https://equalsequals.io/2017/12/13/how-to-add-disqus-comments-to-hexo-blog/" target="_blank" rel="noopener">Add Disqus Comments to Your Hexo Site</a></p><h2 id="Relevant-Articles"><a href="#Relevant-Articles" class="headerlink" title="Relevant Articles"></a>Relevant Articles</h2><p>[1] <a href="https://qiuyiwu.github.io/2019/01/26/Hexo-View/">Add Article Views to Your Hexo Blog</a><br>[2] <a href="https://qiuyiwu.github.io/2019/01/25/Hexo-LocalSearch/">Create Local Search for Your Hexo Blog</a></p>]]></content>
      
      
      <categories>
          
          <category> Hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> comment </tag>
            
            <tag> Disqus </tag>
            
            <tag> Valine </tag>
            
            <tag> blog </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Create Local Search for Your Hexo Blog</title>
      <link href="/2019/01/25/Hexo-LocalSearch/"/>
      <url>/2019/01/25/Hexo-LocalSearch/</url>
      
        <content type="html"><![CDATA[<p>I created my blog with <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> and in this article Iâ€™ll talk about how to create Local Search with <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener">hexo-generator-search plugin</a>. I use <a href="https://github.com/theme-next" target="_blank" rel="noopener">hexo-theme-next</a> version 5.1.4. (for the previous version <strong>localsearch.swig</strong> file needs to be modified.) </p><h2 id="Install-Plugin"><a href="#Install-Plugin" class="headerlink" title="Install Plugin"></a>Install Plugin</h2><p>First, install <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener">hexo-generator-search plugin</a> plugin by the command below:<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-generator-search --save</span><br></pre></td></tr></table></figure></p><p>Then configure this plugin in <strong>_config.yml</strong> file of your theme.<a id="more"></a><br><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">search:</span></span><br><span class="line"><span class="attr">  path:</span> <span class="string">search.xml</span></span><br><span class="line"><span class="attr">  field:</span> <span class="string">post</span></span><br><span class="line"><span class="attr">  content:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></p><p>Parameters can be changed for your preference. </p><ol><li>path: file path, default is search.xml</li><li>field: the search scope you want to search, you can choose:<ul><li>page - will only covers all the pages of your blog.</li><li>all - will covers all the posts and pages of your blog.</li><li>post (by default) - will only covers all the posts of your blog.</li></ul></li><li>content: whether contains the whole content of each article. If false, the generated results only cover title and other meta info without mainbody. By default is true.<br>More information can be found <a href="https://www.npmjs.com/package/hexo-generator-search" target="_blank" rel="noopener">here</a>.</li></ol><h2 id="Extra-Info"><a href="#Extra-Info" class="headerlink" title="Extra Info"></a>Extra Info</h2><p>Version 5.1.3 hexo NexT theme <strong>localsearch.swig</strong> file looks like<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"popup search-popup local-search-popup"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"local-search-header clearfix"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"search-icon"</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">"fa fa-search"</span>&gt;</span><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"popup-btn-close"</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">"fa fa-times-circle"</span>&gt;</span><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"local-search-input-wrapper"</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">input</span> <span class="attr">autocomplete</span>=<span class="string">"off"</span></span></span><br><span class="line"><span class="tag">             <span class="attr">placeholder</span>=<span class="string">"&#123;&#123; __('search.placeholder') &#125;&#125;"</span> <span class="attr">spellcheck</span>=<span class="string">"false"</span></span></span><br><span class="line"><span class="tag">             <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">id</span>=<span class="string">"local-search-input"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"local-search-result"</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure></p><h2 id="Bug-Fixed"><a href="#Bug-Fixed" class="headerlink" title="Bug Fixed"></a>Bug Fixed</h2><p>The above setting works pretty well until one day I encountered a loading failure. The local search function failed to load the popup search window, keeping loading. The issue can be found <a href="https://github.com/theme-next/hexo-theme-next/issues/190" target="_blank" rel="noopener">here</a>. So in this case, we just need to uninstall the hexo-generator-searchdb module with the following command:<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm uninstall hexo-generator-searchdb --save</span><br></pre></td></tr></table></figure></p><p>Because of some confliction between <code>hexo-generator-searchdb</code> and <code>hexo-generator-search</code>, we cannot have the two modules in the system at the same time.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] <a href="https://zty.js.org/post/2016/07/08/hexo-localsearch.html" target="_blank" rel="noopener">å®ç° Hexo next ä¸»é¢˜åšå®¢æœ¬åœ°ç«™å†…æœç´¢</a><br>[2] <a href="http://junonguyen.herokuapp.com/2016/01/22/How-to-create-Local-Search-Engine-for-your-blog-with-hexo-generate-search-plugin/" target="_blank" rel="noopener">How to create Local Search for your Blog with hexo-generate-search plugin</a></p><h2 id="Relevant-Articles"><a href="#Relevant-Articles" class="headerlink" title="Relevant Articles"></a>Relevant Articles</h2><p>[1] <a href="https://qiuyiwu.github.io/2019/01/26/Hexo-View/">Add Article Views to Your Hexo Blog</a><br>[2] <a href="https://qiuyiwu.github.io/2019/01/25/Hexo-Comment/">Add Comments Section to Your Hexo Blog</a></p>]]></content>
      
      
      <categories>
          
          <category> Hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> blog </tag>
            
            <tag> local search </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reading Notes for Astrostatistics</title>
      <link href="/2018/12/24/NoteAstro/"/>
      <url>/2018/12/24/NoteAstro/</url>
      
        <content type="html"><![CDATA[<p>Iâ€™m starting to read Feigelson &amp; Babuâ€™s <em>Modern Statistical Methods for Astronomy</em> this Christmas and hope to finish it before Spring break in March, 2019. This book covers the fundamental statistics theories and methodologies in application on Astronomy. It also aims to help astronomers perceive megadatas from celestial objects via modern statistical analysis and interpret cosmic phenomena in advanced statistical language. It is the bible for Astrostatistics! I take notes and record here for myself better understanding this fantastic field. </p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Collaborations betweeen astronomers and statisticians:<br><a href="http://hea-www.harvard.edu/AstroStat" target="_blank" rel="noopener">Californiaâ€“Harvard Astro-Statistical Collaboration</a><br><a href="http://www.incagroup.org" target="_blank" rel="noopener">International Computational Astrostatistics Group centered in Pittsburgh</a><br><a href="http://astrostatistics.psu.edu" target="_blank" rel="noopener">Center for Astrostatistics at Penn State</a></p><a id="more"></a><h2 id="Astronomy"><a href="#Astronomy" class="headerlink" title="Astronomy"></a>Astronomy</h2><h3 id="Term"><a href="#Term" class="headerlink" title="Term:"></a>Term:</h3><p><strong>Astronomy</strong>: the observational study of matter beyond Earth.<br><strong>Astrophysics</strong>: the study of intrinsic nature of astronomical bodies and the processes by which they binteract and evolve. </p><h3 id="Fields"><a href="#Fields" class="headerlink" title="Fields:"></a>Fields:</h3><p><strong>Planetary Astronomer</strong>: study the Solar System and sxtra-solar planetary system<br><strong>Solar Physicist</strong>: study the Sun<br><strong>Stellar Astronomer</strong>: study other stars<br><strong>Galactic Astronomer</strong>: study Milky Way Galaxy<br><strong>Extragalactic Astronomer</strong>: study other galaxies<br><strong>Cosmologist</strong>: study the Universe as a whole</p><h2 id="Statistics"><a href="#Statistics" class="headerlink" title="Statistics"></a>Statistics</h2><p>Limitations of the spectrograph and observing conditions lead to uncertainty in the measured radial velocities. The uncertainty in our knowledge could be due to the current level of understanding of the phenomenon, which could be reduced in the future. The uncertainty of our knowledge could be due to future choices or events. </p><h3 id="Space-and-Event"><a href="#Space-and-Event" class="headerlink" title="Space and Event"></a>Space and Event</h3><p><strong>Outcome space/sample space</strong>: The set of all outcomes $\Omega$ of an experiment<br><strong>Event</strong>: subset of a sample space<br><strong>Discrete sample space</strong>: A finite (or countably infinite) sample space</p><p>Astronomers deal with both countable space (such as the number of stars in the Galaxy, or the set of photons from a quasar arriving at a detector) and uncountable spaces (such as the variability characteristics of a quasar, or the background noise in an image constructed from interferometry observations).</p><h3 id="Axioms-of-Probability"><a href="#Axioms-of-Probability" class="headerlink" title="Axioms of Probability"></a>Axioms of Probability</h3><p><strong>Probability space</strong>: $(\Omega, \mathcal{F}, P)$ where $\Omega$ is sample space, $\mathcal{F}$ is a class of events, and $P$ is function that assigns probability to events in $\mathcal{F}$.</p><p><strong>Axiom 1</strong>: $0\leq P(A) \leq 1$, for all events $A$<br><strong>Axiom 2</strong>: $P(\Omega) = 1$<br><strong>Axiom 3</strong>: For mutually exclusive (pairwise disjoint) events $A_1, A_2,â€¦,$ we have $P(A_1\cup A_2\cup A_3 â€¦) = P(A_1)+P(A_2)+P(A_3)+â€¦$<br>$\qquad \qquad$  So if for all $i\neq j, A_i \cap A_j = \emptyset$, then $ P( \bigcup_{i=1}^\infty A_i ) = \sum_{i=1}^\infty P(A_i)  $<br><br><br>The generalization to $n$ events, $E_1,â€¦,E_n$ below is called the <strong>inclusion-exclusion formula</strong>:<br><span>$$\begin{align*}P(E_1\cup E_2\cup ...\cup E_n ) &amp;= \sum_{i=1}^\infty P(E_i) - \sum_{ i_1 &lt; i_2 } P(E_{i_1}\cap E_{i_2}) +... \\&amp;+  (-1)^{r+1} \sum_{ i_1 &lt; i_2 &lt; ... &lt; i_r} P(E_{i_1}\cap E_{i_2} \cap ... \cap E_{i_r})  + ...\\&amp;+  (-1)^{n+1} P(E_1\cap E_2 \cap ... \cap E_n) \end{align*}$$</span><!-- Has MathJax --></p><p><strong>Multiplication rule</strong>: $P(A_1\cap A_2\cap â€¦ \cap A_n ) = P(A_1)\times P(A_2|A_1)â€¦P(A_{n-1}|A_1,â€¦A_{n-2})\times P(A_{n}|A_1,â€¦A_{n-1})$</p><p>Except for the rare circumstance when an entirely new phenomenon is discovered, astronomers are measuring properties of celestial bodies or populations for which some distinctive properties are already available. Consider, for example, a subpopulation of galaxies found to exhibit Seyfert-like spectra in the optical band (property A) that have already been examined for nonthermal lobes in the radio band (property B). Then the conditional probability that a galaxy has a Seyfert nucleus given that it also has radio lobes is given by Equation $P(A|B) = \frac{P(A\cap B)}{P(B)}$, and this probability can be estimated from careful study of galaxy samples. The composition of a Solar System minor body can be predominately ices or rock. Icy bodies are more common at large orbital distances and show spectral signatures of water (or other) ice rather than the spectral signatures of silicates. The probability that a given asteroid, comet or Kuiper Belt Object is mostly icy is then conditioned on its semi-major axis and spectral characteristics.</p><p><span style="color:red"><em>To be continueâ€¦</em></span></p>]]></content>
      
      
      <categories>
          
          <category> Statistics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Statistics </tag>
            
            <tag> Astrostatistics </tag>
            
            <tag> Astronomy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Data Fusion</title>
      <link href="/2018/12/20/DataFusion/"/>
      <url>/2018/12/20/DataFusion/</url>
      
        <content type="html"><![CDATA[<p><center><br>Ongoing resaerch projects in SAMSI Program on Model Uncertainty.<br>To be announced in May, 2019.</center></p><p>Summary:<br>In fall 2018, I presented my previous work (collaborating with Ernest Fokoue) about music mining in the group meeting, and also submitted a paper [1] to introduce the idea of representing any given piece of music as a collection of â€œmusical wordsâ€ that we codenamed â€œmuseletsâ€, which are essentially musical words of various lengths. We specifically herein construct a naive dictionary featuring a corpus made up of African American, Chinese, Japanese and Arabic music, on which we perform both topic modelling and pattern recognition. Although some of the results based on the Naive Dictionary are reasonably good, we anticipate phenomenal predictive performances once we get around to actually building a full scale complete version of our intended dictionary of muselets. The idea of Data Fusion in this work is that we create uniform representation of music based on different sources and forms of musical data.<br><a id="more"></a><br>In spring 2019, I will collaborate with Dr. Jong-Min Kim who extensively studies Mixture of D-vine copulas [2]. We plan to combine text mining and copula methods in the application of movie markets. Specifically, we study Chinese and American movie markets, to see the effects of different types of movies on the returns. For the most profitable movies, we study multiple effects that make those movies win the sales (potential factors: movie types, movie director, actors, actress etc.). </p><p>[1] Wu, Qiuyi; Fokoue, Ernest. (2018) Naive Dictionary On Musical Corpora: From Knowledge Representation To Pattern Recognition. arXiv:1811.12802<br>[2] Kim, D., Kim, J. M., Liao, S. M., &amp; Jung, Y. S. (2013). Mixture of D-vine copulas for modeling dependence. Computational Statistics &amp; Data Analysis, 64, 1-19.</p><p>All rights reserved &copy; Copyright 2018, Qiuyi Wu.</p>]]></content>
      
      
      <categories>
          
          <category> Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Fusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Music Mining</title>
      <link href="/2018/12/20/MusicMining/"/>
      <url>/2018/12/20/MusicMining/</url>
      
        <content type="html"><![CDATA[<p>Music and text are similar in the way that both of them can be regraded as information carrier and emotion deliverer. People get daily information from reading newspaper, magazines, blogs etc., and they can also write diary or personal journal to reflect on daily life, let out pent up emotions, record ideas and experience. Same power could come from music! Composers express their feelings through music with different combinations of notes, diverse tempo, and dynamics levels, as another version of language. All these similarities drive people to ask questions like:</p><ul><li>Could music deliver information tantamount to text?</li><li>Can we efficiently use text mining approach in music field?</li><li>Why music from diverse culture can bring people so many different feelings?</li><li>Whatâ€™s the similarity between music from different culture, or composers, or genres?</li><li>To what extend do people grasp the meaning behind each piece of music expressed by the composer?</li></ul><a id="more"></a><p>Take the tragedy Titanic as an example, we learn the tragedy from the newspaper and feel anguished, but we can also get the mourning from the song <em>My Heart Will Go On</em>. The melody contains a lot of minor keys (e.g. $D\flat$, $F\sharp$, $A\flat$), which are more likely to trigger the dissonance via two closely spaced notes hitting the ear simultaneously and thus to make people feel sad.</p><center><br><img class="left" src="/images/2018/12/titanic.png" width="80%" height="80%"><br></center><p>In this project we employ latent Dirichlet allocation model into the music concept. Assume an album, as a collection of songs, are the mixture of different topics (melodies). These topics are the distributions over a series of notes (left part of the figure). In each song, notes in every measure are chosen based on the topic assignments (colorful tokens), while the topic assignments are drawn from the document-topic distribution.</p><center><br><img class="left" src="/images/2018/12/music.png" width="80%" height="80%"><br></center><p>More details can be found in my <a href="https://arxiv.org/abs/1811.12802" target="_blank" rel="noopener">paper</a> and <a href="https://scholarworks.rit.edu/theses/9932/" target="_blank" rel="noopener">thesis</a>.</p><p>All rights reserved &copy; Copyright 2018, Qiuyi Wu.</p><p>Recommended Citation:<br>[1] Wu, Qiuyi, and Ernest Fokoue. â€œNaive Dictionary On Musical Corpora: From Knowledge Representation To Pattern Recognition.â€ arXiv preprint arXiv:1811.12802 (2018).</p><p>[2] Wu, Qiuyi, â€œStatistical Aspects of Music Mining: Naive Dictionary Representationâ€ (2018). Thesis. Rochester Institute of Technology. Accessed from <a href="https://scholarworks.rit.edu/theses/9932" target="_blank" rel="noopener">https://scholarworks.rit.edu/theses/9932</a></p>]]></content>
      
      
      <categories>
          
          <category> Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Topic Modeling </tag>
            
            <tag> music </tag>
            
            <tag> Data Mining </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Storm Surge</title>
      <link href="/2018/12/19/StormSurge/"/>
      <url>/2018/12/19/StormSurge/</url>
      
        <content type="html"><![CDATA[<p><center><br>Ongoing resaerch projects in SAMSI Program on Model Uncertainty.<br>To be announced in May, 2019.</center></p><p>Mid-term summary:<br>In fall 2018, I mainly focused on input distribution subgroup demonstrated my initial exploratory analysis of KE synthetic storm tracks, and compared the simulated tracks with the real IBTrack storm data. One of the main discussions in this semester is how to improve the current practice with the technique of spatial statistics, such as using hierarchical model to improve the estimation of input distribution, or spatial-temporal point process modeling for the storm occurrence rate. Diverse existing projects giving by different researchers are shared and discussed in weekly group meetings.<br>For spring 2019, we will (1.) Measure the model of storm evolution (e.g. the sudden change of the characteristics when the storms landed based on the coastline and how simulation data handle this); (2.) Think about how to impose some structure to improve the estimation concerning input distribution.</p><a id="more"></a><p>All rights reserved &copy; Copyright 2018, Qiuyi Wu.</p>]]></content>
      
      
      <categories>
          
          <category> Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Statistics </tag>
            
            <tag> storm surge </tag>
            
            <tag> Spatial Statistics </tag>
            
            <tag> hurrican tracks </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Research on Public Policy Blogs</title>
      <link href="/2018/06/05/Public-Policy-Blogs/"/>
      <url>/2018/06/05/Public-Policy-Blogs/</url>
      
        <content type="html"><![CDATA[<p>Use different Topic Modeling approaches on Political Blogs to see the performance of diverse methods.</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="Types-of-Models-in-Comparison"><a href="#Types-of-Models-in-Comparison" class="headerlink" title="Types of Models in Comparison"></a>Types of Models in Comparison</h2><ul><li>General LDA (<a href="https://cran.r-project.org/web/packages/lda/lda.pdf" target="_blank" rel="noopener">R package</a>)</li><li>Supervised LDA (<a href="https://arxiv.org/pdf/1003.0783.pdf" target="_blank" rel="noopener">David M. Blei, Jon D. McAuliffe</a>)</li><li>Relational Topic Model (<a href="http://proceedings.mlr.press/v5/chang09a/chang09a.pdf" target="_blank" rel="noopener">Jonathan Chang, David M. Blei</a>)</li><li>Topic Link Block Model (Derek Owens-Oas)</li><li>Poisson Factor Modeling (<a href="http://people.ee.duke.edu/~lcarin/Mingyuan_PAMI_6.pdf" target="_blank" rel="noopener">Beta Negative Binomial Process Topic Model</a>)</li><li>Dynamic Text Network Model (<a href="https://arxiv.org/pdf/1610.05756.pdf" target="_blank" rel="noopener">Teague Henry, David Banks et al.</a>)</li></ul><a id="more"></a><h2 id="Key-Values-in-Cleaned-Blog-Posts"><a href="#Key-Values-in-Cleaned-Blog-Posts" class="headerlink" title="Key Values in Cleaned Blog Posts"></a>Key Values in Cleaned Blog Posts</h2><p>After preprocessing the text extracted from blog posts:</p><ul><li>dates: string of the given date in mm/dd/yy format</li><li>domains: string of the blog website where post was found  (remove â€œwww.â€)</li><li>links: string of other websites occured in the post as hyperlinks  (sorted alphabetically)</li><li>words: filtered words from raw text in the blog posts  (TFIDF variance threading used)</li><li>rawText: direct content from blog posts  (remove short posts and duplicate posts )</li><li>words_stem: stemmed words using Hunspell stemmer  (e.g., apples -&gt; apple)</li></ul><h1 id="Analysis-via-several-Topic-Modeling-Methods"><a href="#Analysis-via-several-Topic-Modeling-Methods" class="headerlink" title="Analysis via several Topic Modeling Methods"></a>Analysis via several Topic Modeling Methods</h1><h2 id="General-LDA"><a href="#General-LDA" class="headerlink" title="General LDA"></a>General LDA</h2><p>General LDA Model via Collapsed Gibbs Sampling Methods for Topic Models: </p><center><br><img class="left" src="/images/2018/06/lda.png" width="80%" height="80%"><br></center><h2 id="Supervised-LDA"><a href="#Supervised-LDA" class="headerlink" title="Supervised LDA"></a>Supervised LDA</h2><p>Here use Blog Site as labels.</p><center><br><img class="left" src="/images/2018/06/slda.png" width="80%" height="80%"><br></center><h2 id="Relational-Topic-Model"><a href="#Relational-Topic-Model" class="headerlink" title="Relational Topic Model"></a>Relational Topic Model</h2><p>RTM models the link as binary random variable that is conditioned on their text. The model can predict links between documents and predict words within them. The algorithm is based on variational EM algorithm.</p><ol><li>For each document $d$:<ol><li>Draw topic proportions $\theta_d|\alpha \sim \text{Dir}(\alpha)$</li><li>For each word $w_{d,n}$:</li></ol><ul><li>Draw assignment $z_{d,n}|\theta_d \sim \text{Mult}(\theta_d)$</li><li>Draw word w<sub>d,n</sub> | z<sub>d,n</sub>, $\beta$<sub>1:K</sub>$\sim \text{Mult}(\beta$<sub>z<sub>d,n</sub></sub>$)$</li></ul></li><li>For each pair of documents $d,dâ€™$:<ul><li>Draw binary link indicator $y|z_d,z$ <sub>dâ€™</sub> $\sim \psi (\cdot | z_d,z$ <sub>dâ€™</sub> $)$</li></ul></li></ol><center><br><img class="left" src="/images/2018/06/rtm.png" width="80%" height="80%"><br></center><p>Compare the performance of link prediction with the one of LDA. The plot below shows the predicted link probabilities from RTM against the ones of LDA for each document, and also shows the most expressed topics by the cited document. (sample 100) </p><center><br><img class="left" src="/images/2018/06/rtm2.png" width="80%" height="80%"><br></center><p>All rights reserved &copy; Copyright 2018, Qiuyi Wu.</p>]]></content>
      
      
      <categories>
          
          <category> Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Topic Modeling </tag>
            
            <tag> Network Analysis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow for Deep Learning 3</title>
      <link href="/2018/01/22/TensorFlow-for-Deep-Learning-3/"/>
      <url>/2018/01/22/TensorFlow-for-Deep-Learning-3/</url>
      
        <content type="html"><![CDATA[<p>Continue the learning process of Convolutional Neural Networks and Recurrent Neural Networks with TensorFlow in Jupyter Notebook.</p><h1 id="RNN-with-TensorFlow"><a href="#RNN-with-TensorFlow" class="headerlink" title="RNN with TensorFlow"></a>RNN with TensorFlow</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="comment"># class: create the data, generate the batches to send it back</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TimeSeriesData</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_points, xmin, xmax)</span>:</span></span><br><span class="line">        self.xmin = xmin</span><br><span class="line">        self.xmax = xmax</span><br><span class="line">        self.num_points = num_points</span><br><span class="line">        self.resolution = (xmax - xmin)/num_points</span><br><span class="line">        self.x_data = np.linspace(xmin, xmax, num_points)</span><br><span class="line">        self.y_true = np.sin(self.x_data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ret_true</span><span class="params">(self, x_series)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> np.sin(x_series)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next_batch</span><span class="params">(self, batch_size, steps, return_batch_ts = False)</span>:</span></span><br><span class="line">        <span class="comment"># grab a random starting point for each batch</span></span><br><span class="line">        random_start = np.random.rand(batch_size,<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># convert the data to TS</span></span><br><span class="line">        ts_start = random_start * (self.xmax - self.xmin - (steps * self.resolution))</span><br><span class="line">        <span class="comment"># create batch time series on the x axis</span></span><br><span class="line">        batch_ts = ts_start + np.arange(<span class="number">0.0</span>,steps+<span class="number">1</span>) * self.resolution</span><br><span class="line">        <span class="comment"># create the Y data for the time series x axis from previous step</span></span><br><span class="line">        y_batch = np.sin(batch_ts)</span><br><span class="line">        <span class="comment"># formatting for RNN</span></span><br><span class="line">        <span class="keyword">if</span> return_batch_ts:</span><br><span class="line">            <span class="keyword">return</span> y_batch[:,:<span class="number">-1</span>].reshape(<span class="number">-1</span>,steps,<span class="number">1</span>), y_batch[:,<span class="number">1</span>:].reshape(<span class="number">-1</span>,steps,<span class="number">1</span>), batch_ts</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> y_batch[:,:<span class="number">-1</span>].reshape(<span class="number">-1</span>,steps,<span class="number">1</span>), y_batch[:,<span class="number">1</span>:].reshape(<span class="number">-1</span>,steps,<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># original y_batch and y_batch shifted over 1 step in the future</span></span><br></pre></td></tr></table></figure><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Give the data and try the model!</span></span><br><span class="line">ts_data = TimeSeriesData(<span class="number">250</span>,<span class="number">0</span>,<span class="number">10</span>)</span><br><span class="line">plt.plot(ts_data.x_data, ts_data.y_true)</span><br></pre></td></tr></table></figure><center><br><img class="left" src="/images/2018/01/sin1.png" width="50%" height="50%"><br></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">num_time_steps = <span class="number">60</span></span><br><span class="line"><span class="comment"># Set 1 batch, 60 steps, return batch time series on x axis</span></span><br><span class="line">y1, y2, ts = ts_data.next_batch(<span class="number">1</span>, num_time_steps, <span class="keyword">True</span>)</span><br><span class="line">plt.plot(ts.flatten()[<span class="number">1</span>:], y2.flatten(), <span class="string">'*'</span>)</span><br><span class="line"><span class="comment"># ts.flatten(): Return a copy of the array collapsed into one dimension.</span></span><br><span class="line"><span class="comment"># ts has total 251 points (including the prediction), so start from 1, to match y2</span></span><br></pre></td></tr></table></figure><center><br><img class="left" src="/images/2018/01/sin2.png" width="50%" height="50%"><br></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(ts_data.x_data, ts_data.y_true, label =<span class="string">'Sin(t)'</span>)</span><br><span class="line">plt.plot(ts.flatten()[<span class="number">1</span>:], y2.flatten(), <span class="string">'*'</span>, label = <span class="string">'Single Training Example'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.tight_layout()   <span class="comment"># Automatically adjust subplot parameters to give specified padding.</span></span><br></pre></td></tr></table></figure><center><br><img class="left" src="/images/2018/01/sin3.png" width="50%" height="50%"><br></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Demonstrate what's going on of the training in the model, shift over 1 step</span></span><br><span class="line">num_time_steps = <span class="number">30</span></span><br><span class="line">train_example = np.linspace(<span class="number">5</span>, <span class="number">5</span>+ts_data.resolution*(num_time_steps + <span class="number">1</span>), num_time_steps + <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">'A Traning Example'</span>)</span><br><span class="line">plt.plot(train_example[:<span class="number">-1</span>], ts_data.ret_true(train_example[:<span class="number">-1</span>]), <span class="string">'bo'</span>, markersize = <span class="number">15</span>, alpha = <span class="number">0.5</span>, label = <span class="string">'Example'</span>)</span><br><span class="line">plt.plot(train_example[<span class="number">1</span>:], ts_data.ret_true(train_example[<span class="number">1</span>:]), <span class="string">'ko'</span>, markersize = <span class="number">7</span>, label = <span class="string">'Target'</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure><center><br><img class="left" src="/images/2018/01/sin4.png" width="50%" height="50%"><br></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create the model</span></span><br><span class="line">tf.reset_default_graph()</span><br><span class="line">num_inputs = <span class="number">1</span></span><br><span class="line">num_neurons = <span class="number">100</span></span><br><span class="line">num_outputs = <span class="number">1</span></span><br><span class="line">learning_rate = <span class="number">0.0001</span></span><br><span class="line">num_train_iterations = <span class="number">2000</span></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Placeholder</span></span><br><span class="line">X = tf.placeholder(tf.float32, [<span class="keyword">None</span>, num_time_steps, num_inputs])</span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="keyword">None</span>, num_time_steps, num_outputs])</span><br><span class="line"><span class="comment"># Run cell layer</span></span><br><span class="line">cell_input = tf.contrib.rnn.BasicRNNCell(num_units = num_neurons, activation = tf.nn.relu) <span class="comment"># Can try other cell: GRUCell etc.</span></span><br><span class="line">cell = tf.contrib.rnn.OutputProjectionWrapper(cell_input, output_size = num_outputs)</span><br><span class="line"></span><br><span class="line">outputs, states = tf.nn.dynamic_rnn(cell, X, dtype = tf.float32)</span><br><span class="line"><span class="comment"># MSE</span></span><br><span class="line">loss = tf.reduce_mean(tf.square(outputs-y))</span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)</span><br><span class="line">train = optimizer.minimize(loss)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="comment"># Session</span></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> iteration <span class="keyword">in</span> range(num_train_iterations):</span><br><span class="line">        X_batch, y_batch = ts_data.next_batch(batch_size, num_time_steps)</span><br><span class="line">        sess.run(train, feed_dict = &#123;X: X_batch, y: y_batch&#125;)</span><br><span class="line">        <span class="keyword">if</span> iteration % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            mse = loss.eval(feed_dict = &#123;X: X_batch, y: y_batch&#125;)</span><br><span class="line">            print(iteration, <span class="string">'\tMSE'</span>, mse)</span><br><span class="line">    saver.save(sess, <span class="string">'./rnn_time_series_model_codealong'</span>)</span><br></pre></td></tr></table></figure><center><br><img class="left" src="/images/2018/01/sin5.png" width="30%" height="30%"><br></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># predict 1 step in the future</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, <span class="string">'./rnn_time_series_model_codealong'</span>)</span><br><span class="line"></span><br><span class="line">    X_new = np.sin(np.array(train_example[:<span class="number">-1</span>].reshape(<span class="number">-1</span>,num_time_steps, num_inputs)))</span><br><span class="line">    y_pred = sess.run(outputs, feed_dict = &#123;X: X_new&#125;)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'Test The Model'</span>)</span><br><span class="line"><span class="comment"># Training example</span></span><br><span class="line">plt.plot(train_example[:<span class="number">-1</span>], np.sin(train_example[:<span class="number">-1</span>]),<span class="string">'bo'</span>, markersize = <span class="number">15</span>, alpha = <span class="number">0.5</span>, label = <span class="string">'Training Example'</span>)</span><br><span class="line"><span class="comment"># Target to predict</span></span><br><span class="line">plt.plot(train_example[<span class="number">1</span>:], np.sin(train_example[<span class="number">1</span>:]),<span class="string">'ko'</span>, markersize = <span class="number">10</span>, label = <span class="string">'Target'</span>)</span><br><span class="line"><span class="comment"># Model prediction</span></span><br><span class="line">plt.plot(train_example[<span class="number">1</span>:], y_pred[<span class="number">0</span>,:,<span class="number">0</span>], <span class="string">'r.'</span>, markersize = <span class="number">10</span>, label = <span class="string">'Prediction'</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">'Time'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.tight_layout()</span><br></pre></td></tr></table></figure><center><br><img class="left" src="/images/2018/01/sin6.png" width="50%" height="50%"><br></center><br>To further explore the performance of RNN model, we can change the number of iteration <code>num_train_iterations</code>, learning rate <code>learning_rate</code>, and model type <code>tf.contrib.rnn.BasicRNNCell()</code> and then play with it.<br><br>About just for time sequence that shifts 1 time step ahead, now weâ€™ll generate a completely new sequence.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, <span class="string">'./rnn_time_series_model_codealong'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># seed zeros</span></span><br><span class="line">    <span class="comment"># 30 0s and the generated data</span></span><br><span class="line">    zero_seq_seed = [<span class="number">0.0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(num_time_steps)]</span><br><span class="line">    <span class="keyword">for</span> iteration <span class="keyword">in</span> range(len(ts_data.x_data) - num_time_steps):</span><br><span class="line">        X_batch = np.array(zero_seq_seed[-num_time_steps:]).reshape(<span class="number">1</span>, num_time_steps,<span class="number">1</span>)</span><br><span class="line">        y_pred = sess.run(outputs, feed_dict = &#123;X: X_batch&#125;)</span><br><span class="line">        zero_seq_seed.append(y_pred[<span class="number">0</span>,<span class="number">-1</span>,<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">plt.plot(ts_data.x_data, zero_seq_seed, <span class="string">'b-'</span>)</span><br><span class="line">plt.plot(ts_data.x_data[:num_time_steps], zero_seq_seed[:num_time_steps], <span class="string">'r'</span>, linewidth =<span class="number">3</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Time'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Y'</span>)</span><br></pre></td></tr></table></figure><br><br><center><br><img class="left" src="/images/2018/01/sin7.png" width="50%" height="50%"><br></center><p>Instead of zeros at the beginning, now use training example, other parts remain the same<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, <span class="string">'./rnn_time_series_model_codealong'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 30 training points and the generated data</span></span><br><span class="line">    training_example = list(ts_data.y_true[:<span class="number">30</span>])</span><br><span class="line">    <span class="keyword">for</span> iteration <span class="keyword">in</span> range(len(ts_data.x_data) - num_time_steps):</span><br><span class="line">        X_batch = np.array(training_example[-num_time_steps:]).reshape(<span class="number">1</span>, num_time_steps,<span class="number">1</span>)</span><br><span class="line">        y_pred = sess.run(outputs, feed_dict = &#123;X: X_batch&#125;)</span><br><span class="line">        training_example.append(y_pred[<span class="number">0</span>,<span class="number">-1</span>,<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">plt.plot(ts_data.x_data, training_example, <span class="string">'b-'</span>)</span><br><span class="line">plt.plot(ts_data.x_data[:num_time_steps], training_example[:num_time_steps], <span class="string">'r'</span>, linewidth =<span class="number">3</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Time'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Y'</span>)</span><br></pre></td></tr></table></figure></p><center><br><img class="left" src="/images/2018/01/sin8.png" width="50%" height="50%"><br></center>]]></content>
      
      
      <categories>
          
          <category> TensorFlow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
            <tag> RNN </tag>
            
            <tag> API </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow for Deep Learning 2</title>
      <link href="/2018/01/21/TensorFlow-for-Deep-Learning-2/"/>
      <url>/2018/01/21/TensorFlow-for-Deep-Learning-2/</url>
      
        <content type="html"><![CDATA[<p>Here Iâ€™ll give the theory part of Neural Networks, sepecifically three kinds of NN: Normal Neural Networks, Convolutional Neural Networks, Recurrent Neural Networks. </p><h1 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks"></a>Neural Networks</h1><h2 id="In-single-neuron"><a href="#In-single-neuron" class="headerlink" title="In single neuron:"></a>In single neuron:</h2><span>$$\begin{align*}z &amp;= Wx + b \\a &amp;= \sigma (z)\end{align*}$$</span><!-- Has MathJax --><h2 id="Activation-Function"><a href="#Activation-Function" class="headerlink" title="Activation Function"></a>Activation Function</h2><ul><li><strong>Perceptron:</strong> binary classifier, small changes are not reflected.<span>$$\begin{align*}f(x) = \begin{cases}1 &amp; \text{if } Wx+b&gt;0\\ 0 &amp; \text{otherwise}\end{cases}\end{align*}$$</span><!-- Has MathJax --></li></ul><a id="more"></a><center><br><img src="/images/2018/01/activation1.png" width="50%" height="50%"><br></center><br><em> <strong>Sigmoid:</strong> special case of the logistic function with S shape curve, more dynamic<br><span>$$\begin{align*}S(x) = \frac{1}{1 + e^{-x}} = \frac{e^x}{e^x +1} \end{align*}$$</span><!-- Has MathJax --><br><center><br><img src="/images/2018/01/activation2.png" width="50%" height="50%"><br></center></em> <strong>Hyperbolic Tangent</strong>: Tanh(z)<br><span>$$\begin{align*}cosh x &amp;= \frac{e^x + e^{-x}}{2}\\ sinh x &amp;= \frac{e^x - e^{-x}}{2}\\ tanh x &amp;= \frac{cosh x}{sinh x }\end{align*}$$</span><!-- Has MathJax --><br><center><br><img src="/images/2018/01/activation3.png" width="50%" height="50%"><br></center><ul><li><p><strong>ReLU (Rectified Linear Unit):</strong> max(0,z)<br><center><br><img src="/images/2018/01/activation4.png" width="50%" height="50%"><br></center><br>ReLU and tanh tend to have the best performance.</p></li><li><p><strong>Softmax Regression:</strong> </p><span>$$\begin{align*}z_i &amp;= \sum_jW_{i,j}x_j + b_i  \\softmax(z)_i &amp;= \frac{\text{exp}(z_i)}{\sum_j \text{exp}(z_j)}\end{align*}$$</span><!-- Has MathJax --></li></ul><h2 id="Cost-Loss-Function"><a href="#Cost-Loss-Function" class="headerlink" title="Cost/Loss Function"></a>Cost/Loss Function</h2><p>Cost function is the measurement of the error.<br><span>$$\begin{align*}z &amp;= Wx + b \\a &amp;= \sigma (z)\\C &amp;= \frac{1}{n}\sum(y_{true} - a)^2 &amp;\text{Quadratic Cost}\\C &amp;= -\frac{1}{n}\sum(y_{true}\cdot ln(a) + (1-y_{true})\cdot ln(1-a) ) &amp;\text{Cross Entropy}\end{align*}$$</span><!-- Has MathJax --><br><strong>Quadratic Cost:</strong><br>The larger errors are more prominent due to the squaring. It causes a slowdown in learning speed.<br><strong>Cross Entropy:</strong><br>It allows for faster learning. The larger the difference, the faster the neuron can learn.</p><h2 id="Gradient-Descent-amp-Backpropagation"><a href="#Gradient-Descent-amp-Backpropagation" class="headerlink" title="Gradient Descent &amp; Backpropagation"></a>Gradient Descent &amp; Backpropagation</h2><p><strong>Gradient Descent</strong> is an optimization algorithm for finding the minimum of a function. Here it minimizes the error to find the optiml value. 1-D example below shows the best parameter value (weights of the neuron inputs) we should choose to minimize the cost.</p><center><br><img src="/images/2018/01/GD.png" width="30%" height="30%"><br></center><br>For complicated cases more than 1 dimension, we use biilt-in algebra of Deep learning library to get the optimal parameters.<br><br>1. <em>Learning Rate:</em> defines the step size during gradient descent, too small - slow pace, too small - overshooting<br>2. <em>Batch Size:</em> batches allow us to use stochastic gradient descent, in case the datasets are large, if all the them are fed at once the computation would be very expensive. Too small - less representative of data, too large - longer training time<br>3. <em>Second-Order Behavior of Gradient Descent:</em> adjust the learning rate based on the rate of descent(second-order behavior: derivative),large learning rate at the beginning, adjust to slower learning rate as it get closer. Methods: AdaGrad, RMSProp, Adam<br><br>Vanishing Gradients: when increasing the number of layers in a network, the layers towards the input will be affected less by the error calculation occuring at the output as going backwards throught the network. Initialization and Normalization will help to mitigate the issue.<br><br><strong>Backpropagation</strong> is to calculate the error contribution of each neuron after a batch of data is processed. It works by calculating the error at the output and then distributes back through the network layers. It belongs to supervised learning as it requires a known output for each input value. The mathematical detail is showed below from Andrew Ngâ€™s Neural Networks and Deep Learning in Coursera.<br><center><br><img src="/images/2018/01/BP.png" width="70%" height="70%"><br></center><h2 id="Initialization-of-Weights"><a href="#Initialization-of-Weights" class="headerlink" title="Initialization of Weights"></a>Initialization of Weights</h2><h3 id="Zeros"><a href="#Zeros" class="headerlink" title="Zeros"></a>Zeros</h3><p>No randomness (too subjective) so not a good choice</p><h3 id="Random-Distribution"><a href="#Random-Distribution" class="headerlink" title="Random Distribution"></a>Random Distribution</h3><p>Random distribution near zero is not optimal and results in activaion function distorition (distorted to large values)</p><h3 id="Xavier-Glorot-Initialization"><a href="#Xavier-Glorot-Initialization" class="headerlink" title="Xavier (Glorot) Initialization"></a>Xavier (Glorot) Initialization</h3><p>The weights drawn from uniform or normal distribution, with <strong>zero mean</strong> and <strong>specific variance</strong> $\text{Var}(W) =\frac{1}{n_{in}} $:<br><span>$$\begin{align*}&amp;Y = W_1X_1 +W_2X_2 + ... + W_nX_n\\&amp;\text{Var}(W_iX_i) = E[X_i]^2\text{Var}(W_i) + E[W_i]^2\text{Var}(X_i) + \text{Var}(W_i)\text{Var}(X_i)\\ &amp;\text{Var}(W_iX_i) = \text{Var}(W_i)\text{Var}(X_i) \qquad (\because E[X_i] = 0)\\ &amp;\text{Var}(Y) = \text{Var}(W_1X_1 + W_2X_2 +... + W_nX_n)  = n  \text{Var}(W_i)\text{Var}(X_i) \\ &amp;\because \text{Variance of the output is equal to the variance of the input}\\ &amp;\therefore n\text{Var}(W_i) = 1\\&amp;\therefore \text{Var}(W_i) = \frac{1}{n} =  \frac{1}{n_{in}}  = \frac{2}{n_{in} + n_{out} }\end{align*}$$</span><!-- Has MathJax --></p><h2 id="Overfitting-Issue"><a href="#Overfitting-Issue" class="headerlink" title="Overfitting Issue"></a>Overfitting Issue</h2><p>With potentially hundreds of parameters in a deep learning neural network, the possibility of overfitting is very high. We can mitigate this issue by the following ways:</p><ul><li><strong>$L_1/L_2$ Regularization</strong><br>Add a penalty for a larger weights in the model (not unique to neural networks)</li><li><strong>Dropout</strong><br>Remove neurons during training randomly so that the network does not over rely on any particular neuron (unique to neural networks)</li><li><strong>Expanding Data</strong><br>Artificially expand data via adding noise, tilting images </li></ul><h1 id="Convolutional-Neural-Networks"><a href="#Convolutional-Neural-Networks" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h1><h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h2><p><strong>Tensor:</strong> N-dimensional arrays:<br>Scalar: 3<br>Vector: [3,4,5]<br>Matrix: [[3,4],[5,6],[7,8]]<br>Tensor: [[[1,2], [3,4]],  [[5,6], [7,8]]]<br>We use tensors to feed in sets of images into the model - (I,H,W,C)<br>I: Images<br>H: Height of Image in Pixels<br>W: Width of Image in Pixels<br>C: Color Channels: 1 - Grayscale, 3 - RGB</p><h2 id="DNN-vs-CNN"><a href="#DNN-vs-CNN" class="headerlink" title="DNN vs CNN"></a>DNN vs CNN</h2><h3 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h3><p>The left figure is <strong>densely connected layer</strong>, every neuron in the layer is directly connected to the every neuron in the next layer. While each unit in the <strong>convolutional layer</strong> is connected to a smaller number of nearby units in the next layer. </p><center><br><img class="left" src="/images/2018/01/DNNCNN.png" width="60%" height="50%"><br></center><br>The reason for the idea of CNN is that most images are at least 256 by 256 pixels or greater (MNIST only 28 by 29 pixels, 784 total). So there are too many parameters unscalable to new images. Another merit of CNN is for image processing, pixels nearby to each other are much more correlated to each other for image detection. Each CNN layer looks at an increasingly larger part of the image. And having units only connected to nearby units helps invariance. CNN helps limit the search of weights to the size of the convolution. <em>Convolutional layers are only connected to pixels in their respective fields.</em> By <strong>adding a padding of zeros</strong> around the image we can fix the issue for edge neurons where there may not be an input for them.<br>Take the example of 1-D Convolution, we treat the weights as a filter for edge detection, then expand one filters to multiple filters. Stride 1 means 1 unit at a time. The top circle means the zero padding added to include more edge pixels. Each filter detects a different feature.<br><center><br><img class="left" src="/images/2018/01/CNN12.png" width="70%" height="60%"><br></center><br>Now for simplicity, the sets of neurons are visualized as blocks.<br><center><br><img class="left" src="/images/2018/01/CNN34.png" width="80%" height="70%"><br></center><br>For 2-D Images and Color Images:<br><center><br><img class="left" src="/images/2018/01/2D.png" width="80%" height="70%"><br></center><p>More Info: <a href="http://setosa.io/ev/image-kernels/" target="_blank" rel="noopener">Image Kernals</a></p><h3 id="Subsampling"><a href="#Subsampling" class="headerlink" title="Subsampling"></a>Subsampling</h3><p>Except <strong>convolutional layers</strong>, thereâ€™s another kind of layers called <strong>pooling layers</strong>. Pooling layers will subsample the input image to reduce the memory use and computer load as well as reducing the number of parameters.<br>Take example of MNIST, only select max value to the next layer, and move over by stride. So the pooling layer will finally remove a lot of information.</p><center><br><img class="left" src="/images/2018/01/pooling.png" width="60%" height="50%"><br></center><p>Another technique is â€œDropoutâ€. Dropout is regarded as a form of regularization as during training, units are randomly dropped with their connection to help prevent overfitting. </p><center><br><img class="left" src="/images/2018/01/diagram.png" width="80%" height="70%"><br></center><h1 id="Recurrent-Neural-Networks"><a href="#Recurrent-Neural-Networks" class="headerlink" title="Recurrent Neural Networks"></a>Recurrent Neural Networks</h1><p>Common Neural Networks can handle classification and regression problems, but for sequence information, we need Recurrent Neural Networks.<br>Normal Neural Networks just aggregation of inputs and pass the activation function to get the output. Recurrent Neural Networks send output back to itself.<br>Here I have to mention my previous research work â€“ Echo State Networks, also belong to RNN: <a href="https://qiuyiwu.github.io/ESN/"><strong>ESN</strong></a> </p><p><center><br><img class="left" src="/images/2018/01/RNN.png" width="90%" height="80%"><br></center><br>Cells that are a function of inputs from previous time steps are also know as memory cells. </p>]]></content>
      
      
      <categories>
          
          <category> TensorFlow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TensorFlow </tag>
            
            <tag> DNN </tag>
            
            <tag> CNN </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow for Deep Learning 1</title>
      <link href="/2018/01/16/Tensorflow-for-Deep-Learning-1/"/>
      <url>/2018/01/16/Tensorflow-for-Deep-Learning-1/</url>
      
        <content type="html"><![CDATA[<p>Iâ€™m learning how to use Googleâ€™s TensorFlow framework to create artificial neural networks for deep learning with Python from Udemy. Iâ€™m using Jupiter notebook to practice and blog my learning progress here.</p><p>TensorFlow is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. This architecture allows users to deploy computation to one or more CPUs or GPUs, in a desktop, server, or mobile device with a single API (Application programming interface).</p><a id="more"></a><h1 id="Install-TensorFlow-Environment"><a href="#Install-TensorFlow-Environment" class="headerlink" title="Install TensorFlow Environment"></a>Install TensorFlow Environment</h1><p><a href="https://www.anaconda.com/download/#macos" target="_blank" rel="noopener">Download Anaconda Distribution</a> for Python 3.6 Version. Tutorial comes in that page.<br>For MacOS: Open Terminal, use <code>cd</code> to the target directory<br>Create the environment file: run <code>conda env create -f tfdl_env.yml</code><br>Activate the file: run <code>source activate tfdeeplearning</code><br>Now you are in the virtual environment of tfdeeplearning<br>If you want to get out of this, run <code>source deactivate</code></p><p><strong>Note:</strong> <a href="https://stackoverflow.com/questions/42096280/how-is-anaconda-related-to-python#comment71363474_42096429" target="_blank" rel="noopener">How is Anaconda related to Python?</a><br>Anaconda is a python and R distribution. It aims to provide everything you need (python wise) for data science â€œout of the boxâ€.<br>It includes:</p><ul><li>The core python language</li><li>100+ python â€œpackagesâ€ (libraries)</li><li>Spyder (IDE/editor - like pycharm) and Jupyter</li><li><code>conda</code>, Anacondaâ€™s own package manager, used for updating Anaconda and packages</li></ul><p>Also Anaconda is used majorly for the data science. which manipulates large datasets based on statistical methods. ie. Many statistical packages are already available in anaconda libraries(packages) </p><p>Vanilla python installed from python.org comew with <a href="https://docs.python.org/3/library/" target="_blank" rel="noopener">standard library</a> is okay, in which case using <a href="https://pypi.python.org/pypi/pip" target="_blank" rel="noopener">pip</a> to install manually (which comes with most python dists and you should have it if you downloaded from python.org).<br>Learn more: <a href="https://www.continuum.io/anaconda-overview" target="_blank" rel="noopener">Anaconda overview</a>; <a href="https://docs.python.org/3/tutorial/" target="_blank" rel="noopener">Python 3 tutorial</a></p><h1 id="TensorFlow-Basic-Syntax"><a href="#TensorFlow-Basic-Syntax" class="headerlink" title="TensorFlow Basic Syntax"></a>TensorFlow Basic Syntax</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">print(tf.__version__)</span><br></pre></td></tr></table></figure><p>Output: <code>1.3.0</code></p><p><strong>Create a tensor ($\approx$ n-dimension array), the basic one (constant)</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hello = tf.constant(<span class="string">'Hello'</span>)</span><br><span class="line">world = tf.constant(<span class="string">'World'</span>)</span><br><span class="line">type (hello)</span><br></pre></td></tr></table></figure></p><p>Output: <code>tensorflow.python.framework.ops.Tensor</code></p><p><strong>Run this operation inside of a session:</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    result = sess.run(hello + world)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure></p><p>Output: <code>b&#39;HelloWorld&#39;</code><br>Note:<br>Use <code>with</code> is to make sure we donâ€™t close the session until we run a block of code then close the session.<br>The <code>b</code> character prefix signifies that <code>HelloWorld</code> is a <a href="https://stackoverflow.com/questions/6224052/what-is-the-difference-between-a-string-and-a-byte-string" target="_blank" rel="noopener">byte string</a>, use <code>result.decode(&#39;utf-8&#39;)</code> to convert byte str to str.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant(<span class="number">10</span>)</span><br><span class="line">b = tf.constant(<span class="number">20</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    c = sess.run( a + b )</span><br><span class="line">print(c)</span><br></pre></td></tr></table></figure><p>Output: <code>30</code></p><p><strong>Numpy Operations:</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">const = tf.constant(<span class="number">10</span>)        <span class="comment"># constant operation</span></span><br><span class="line">fill_mat = tf.fill((<span class="number">4</span>,<span class="number">4</span>),<span class="number">10</span>)   <span class="comment"># matrix operation 4x4</span></span><br><span class="line">myzeros = tf.zeros((<span class="number">4</span>,<span class="number">4</span>))      <span class="comment"># 4x4 zeors</span></span><br><span class="line">myones = tf.ones((<span class="number">4</span>,<span class="number">4</span>))        <span class="comment"># 4x4 ones</span></span><br><span class="line">myrandn = tf.random_normal((<span class="number">4</span>,<span class="number">4</span>), mean = <span class="number">0</span>, stddev = <span class="number">1.0</span>)         <span class="comment"># random normal distribution </span></span><br><span class="line">myrandu = tf.random_uniform((<span class="number">4</span>,<span class="number">4</span>), minval = <span class="number">0</span>, maxval = <span class="number">1.0</span>)      <span class="comment"># random uniform distribution </span></span><br><span class="line"></span><br><span class="line">my_ops = [const, fill_mat, myzeros, myones, myrandn, myrandu]</span><br><span class="line"></span><br><span class="line">sess = tf.InteractiveSession()      <span class="comment"># Interactive Session</span></span><br><span class="line"><span class="keyword">for</span> op <span class="keyword">in</span> my_ops:</span><br><span class="line">    print(sess.run(op),<span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure></p><p>Note:<br><code>Interactive Session</code> is particularly useful for Jupyter Notebook, it allows you to constantly call it through multiple cells.<br>In the Interactive Session, instead of using <code>sess.run(op)</code>, we can use <code>op.eval()</code>, it means â€œevaluate this operationâ€, and generates the same result.</p><p><strong>Matrix multiplication (common in neural networks)</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">b = tf.constant([[<span class="number">10</span>],[<span class="number">100</span>]])</span><br><span class="line">result = tf.matmul(a,b)</span><br><span class="line">sess.run(result)</span><br></pre></td></tr></table></figure></p><span>$$\begin{align*}    \begin{bmatrix}        1 &amp; 2 \\        3 &amp; 4    \end{bmatrix} \times \begin{bmatrix}      10 \\ 100    \end{bmatrix} = \begin{bmatrix}      210 \\ 430    \end{bmatrix}\end{align*}$$</span><!-- Has MathJax --><p>Note: <code>sess.run(result)</code> can also use <code>result.eval()</code> instead.</p><h1 id="TensorFlow-Graphs"><a href="#TensorFlow-Graphs" class="headerlink" title="TensorFlow Graphs"></a>TensorFlow Graphs</h1><p>Graphs are sets of connected <em>nodes</em>(vertices), and the connections are called <em>edges</em>. In TensorFlow each node is an operation with possible inputs that can supply some outputs. When TenserFlow is started, a default graph is created.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(tf.get_default_graph())</span><br><span class="line">print(tf.Graph())</span><br></pre></td></tr></table></figure><p>Output:<br>fixed default graph: <code>&lt;tensorflow.python.framework.ops.Graph object at 0x11f4c92b0&gt;</code><br>Dynamic random graph: <code>&lt;tensorflow.python.framework.ops.Graph object at 0x11f707dd8&gt;</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">graph_one = tf.Graph()</span><br><span class="line">graph_two = tf.get_default_graph()</span><br><span class="line"><span class="keyword">with</span> graph_one.as_default():</span><br><span class="line">    print(graph_one <span class="keyword">is</span> tf.get_default_graph())</span><br></pre></td></tr></table></figure><p>Output: <code>True</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(graph_one <span class="keyword">is</span> tf.get_default_graph())</span><br></pre></td></tr></table></figure></p><p>Output: <code>False</code></p><h1 id="Variables-and-Placeholders"><a href="#Variables-and-Placeholders" class="headerlink" title="Variables and Placeholders"></a>Variables and Placeholders</h1><p><em>Variables</em> and <em>Placeholders</em> are two main types of tensor objects in a Graph. During the optimization process, TensorFlow tunes the parameters of the model. <em>Variables</em> can hold the values of weights and biases throughout the session. <strong><em>Variables</em> need to be initialized</strong>. <em>Placeholders</em> are initially empty and are used to feed in the actual training examples. However they do need a declared expected data type (tf.float32) with an optiional shape argument.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### Variable ###</span></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">my_tensor = tf.random_uniform((<span class="number">4</span>,<span class="number">4</span>), <span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">my_var = tf.Variable(initial_value = my_tensor)  <span class="comment"># give value to variable </span></span><br><span class="line"><span class="comment"># sess.run(my_var) cannot be directly run here, need initialized first!</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line">sess.run(my_var) <span class="comment"># Now it's the time!</span></span><br></pre></td></tr></table></figure></p><p>Output: <code>array([[ 0.27756679,  0.82726526,  0.80544853,  0.43891859],        [ 0.56279469,  0.57444489,  0.82595968,  0.63165414],        [ 0.16034544,  0.86095798,  0.74416387,  0.17536163],        [ 0.44427669,  0.69035304,  0.55842543,  0.00723565]], dtype=float32)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### Placeholder ###</span></span><br><span class="line">ph = tf.placeholder(tf.float32, shape = (<span class="number">4</span>,<span class="number">4</span>)) </span><br><span class="line">ph = tf.placeholder(tf.float32, shape = (<span class="keyword">None</span>,<span class="number">5</span>)) <span class="comment"># can be fed in the actually number of sampes</span></span><br><span class="line">ph = tf.placeholder(tf.float32) <span class="comment"># no shape</span></span><br></pre></td></tr></table></figure><h1 id="TensorFlow-Neural-Network"><a href="#TensorFlow-Neural-Network" class="headerlink" title="TensorFlow Neural Network"></a>TensorFlow Neural Network</h1><p>Create a neuron that performs a simple linear fit to some 2-D data. Graph of $wx+b = z$</p><ol><li>Build a Graph</li><li>Initiate the Session</li><li>Feed Data In and get Output</li><li>Add in the cost function in order to train the network to optimize the parameters<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">### Build a simple Graph ###</span></span><br><span class="line">np.random.seed(<span class="number">101</span>)</span><br><span class="line">tf.set_random_seed(<span class="number">101</span>)</span><br><span class="line"></span><br><span class="line">rand_a = np.random.uniform(<span class="number">0</span>,<span class="number">100</span>, (<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">rand_b = np.random.uniform(<span class="number">0</span>,<span class="number">100</span>, (<span class="number">5</span>,<span class="number">1</span>))</span><br><span class="line">a = tf.placeholder(tf.float32) <span class="comment"># default shape</span></span><br><span class="line">b = tf.placeholder(tf.float32) <span class="comment"># default shape</span></span><br><span class="line"><span class="comment"># Two ways to do the operation: </span></span><br><span class="line"><span class="comment">#tf.add(a, b)  |  tf.multiply(a, b)  |  tf.matmul(a, b) </span></span><br><span class="line">add_op = a + b</span><br><span class="line">mul_op = a * b</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    add_result = sess.run(add_op, feed_dict = &#123; a:rand_a, b:rand_b&#125;) </span><br><span class="line">    print(add_result, <span class="string">'\n'</span>)</span><br><span class="line">    mul_result = sess.run(mul_op, feed_dict = &#123; a:rand_a, b:rand_b&#125;)</span><br><span class="line">    print(mul_result)</span><br></pre></td></tr></table></figure></li></ol><p>Output:<br>`[[ 151.07165527  156.49855042  102.27921295  116.58396149  167.95948792]<br> [ 135.45622253   82.76316071  141.42784119  124.22093201   71.06043243]<br> [ 113.30171204   93.09214783   76.06819153  136.43911743  154.42727661]<br> [  96.7172699    81.83804321  133.83674622  146.38117981  101.10578918]<br> [ 122.72680664  105.98292542   59.04463196   67.98310089   72.89292145]] </p><p>[[ 5134.64404297  5674.25         283.12432861  1705.47070312  6813.83154297]<br> [ 4341.8125      1598.26696777  4652.73388672  3756.8293457    988.9463501 ]<br> [ 3207.8112793   2038.10290527  1052.77416992  4546.98046875  5588.11572266]<br> [ 1707.37902832   614.02526855  4434.98876953  5356.77734375  2029.85546875]<br> [ 3714.09838867  2806.64379883   262.76763916   747.19854736 1013.29199219]]`</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### Build a Neural Network ### </span></span><br><span class="line">n_features = <span class="number">10</span>      <span class="comment"># input layer</span></span><br><span class="line">n_dense_neurons = <span class="number">3</span>  <span class="comment"># hidden layer</span></span><br><span class="line">x = tf.placeholder(tf.float32, (<span class="keyword">None</span>, n_features))</span><br><span class="line">W = tf.Variable(tf.random_normal([n_features, n_dense_neurons]))</span><br><span class="line">b = tf.Variable(tf.ones([n_dense_neurons]))</span><br><span class="line"></span><br><span class="line">xW = tf.matmul(x, W) </span><br><span class="line">z = tf.add(xW, b)</span><br><span class="line"><span class="comment"># a = tf.nn.relu | tf.tahn</span></span><br><span class="line">a = tf.sigmoid(z)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)        <span class="comment"># DON'T FORGET!</span></span><br><span class="line">    layer_out = sess.run(a, feed_dict = &#123; x: np.random.random([<span class="number">1</span>,n_features])&#125;)</span><br><span class="line">print(layer_out)</span><br></pre></td></tr></table></figure><p>Output: <code>[[ 0.81314439  0.98195159  0.73793817]]</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### Simple Regression Example ###</span></span><br><span class="line">x_data = np.linspace(<span class="number">0</span>,<span class="number">10</span>,<span class="number">10</span>) + np.random.uniform(<span class="number">-1.5</span>,<span class="number">1.5</span>,<span class="number">10</span>)  <span class="comment"># with noise</span></span><br><span class="line">y_label = np.linspace(<span class="number">0</span>,<span class="number">10</span>,<span class="number">10</span>) + np.random.uniform(<span class="number">-1.5</span>,<span class="number">1.5</span>,<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline     </span><br><span class="line"><span class="comment"># %matplotlib inline    just for Jupyter notebook   </span></span><br><span class="line">plt.plot(x_data, y_label, <span class="string">'*'</span>)</span><br></pre></td></tr></table></figure><p><img src="/images/2018/01/TF1.png" alt="Sample Image Added via Markdown"><br>Now I want the neural network to solve $ y = mx + b$ with 10 training steps:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create two random variables</span></span><br><span class="line">m = tf.Variable(<span class="number">0.44</span>)</span><br><span class="line">b = tf.Variable(<span class="number">0.87</span>)</span><br><span class="line"></span><br><span class="line">error = <span class="number">0</span>  <span class="comment"># create an error starting from 0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> zip(x_data, y_label):   <span class="comment"># zip makes a list of x,y</span></span><br><span class="line">    y_hat = m * x + b</span><br><span class="line">    error += (y - y_hat) ** <span class="number">2</span>       <span class="comment"># cost function</span></span><br><span class="line"></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate = <span class="number">0.001</span>)</span><br><span class="line">train = optimizer.minimize(error)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    training_steps = <span class="number">10</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(training_steps):</span><br><span class="line">        sess.run(train)</span><br><span class="line">    final_slope, final_intercept = sess.run([m,b])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test </span></span><br><span class="line">x_test = np.linspace(<span class="number">-1</span>,<span class="number">11</span>,<span class="number">10</span>)</span><br><span class="line">y_pred = final_slope * x_test + final_intercept </span><br><span class="line"></span><br><span class="line">plt.plot(x_test, y_pred, <span class="string">'r'</span>)</span><br><span class="line">plt.plot(x_test, y_label,<span class="string">'*'</span>)</span><br></pre></td></tr></table></figure></p><p><img src="/images/2018/01/TF2.png" alt="Sample Image Added via Markdown"></p><h1 id="TensorFlow-Regression"><a href="#TensorFlow-Regression" class="headerlink" title="TensorFlow Regression"></a>TensorFlow Regression</h1><p>Regression example with huge dataset:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">x_data = np.linspace(<span class="number">0.0</span>,<span class="number">10.0</span>,<span class="number">1000000</span>)  <span class="comment"># huge dataset</span></span><br><span class="line">noise = np.random.randn(len(x_data))    <span class="comment"># add noise</span></span><br><span class="line"><span class="comment"># y = mx + b | b = 5, m = 0.5</span></span><br><span class="line">y_true = (<span class="number">0.5</span> * x_data) + <span class="number">5</span> + noise</span><br><span class="line"></span><br><span class="line">x_df = pd.DataFrame(data = x_data, columns = [<span class="string">'X Data'</span>])</span><br><span class="line">y_df = pd.DataFrame(data = y_true, columns = [<span class="string">'Y'</span>])</span><br><span class="line">my_data = pd.concat([x_df, y_df], axis = <span class="number">1</span>)  <span class="comment"># axis = 1 column concatenate</span></span><br><span class="line">my_data.sample(n = <span class="number">25</span>) <span class="comment"># random 25 samples</span></span><br><span class="line">my_data.sample(n=<span class="number">250</span>).plot(kind = <span class="string">'scatter'</span>, x = <span class="string">'X Data'</span>, y = <span class="string">'Y'</span>)</span><br></pre></td></tr></table></figure></p><p><img src="/images/2018/01/TF3.png" alt="Sample Image Added via Markdown"><br>For real cases, usually the datasets are humongous and we do not use all of them at once. Instead, we use batch method to feed the data batch by batch into the network to save the time. The number of batches depends on the size of the data. Here we feed 1000 batches of data, with each batch has 8 corresponding data points (x data points with corresponding y labels). To make batches useful, we grab 8 random data points via <code>rand_ind = np.random.randint(len(x_data), size = batch_size)</code>, then train the optimizer.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">8</span></span><br><span class="line">m = tf.Variable(<span class="number">0.81</span>)</span><br><span class="line">b = tf.Variable(<span class="number">0.17</span>)</span><br><span class="line"></span><br><span class="line">xph = tf.placeholder(tf.float32,[batch_size])</span><br><span class="line">yph = tf.placeholder(tf.float32,[batch_size])</span><br><span class="line"></span><br><span class="line">y_model = m * xph + b    <span class="comment"># the Graph I want to compute</span></span><br><span class="line"><span class="comment"># Loss function</span></span><br><span class="line">error = tf.reduce_sum(tf.square(yph - y_model)) <span class="comment"># tf.reduce_sum: Computes the sum of elements across dimensions of a tensor.</span></span><br><span class="line"><span class="comment"># Optimizer</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate = <span class="number">0.001</span>)     <span class="comment"># This is gradient decent optimizer, there are many other optimizers</span></span><br><span class="line">train = optimizer.minimize(error)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    batches = <span class="number">1000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(batches):</span><br><span class="line">        rand_ind = np.random.randint(len(x_data), size = batch_size)    <span class="comment"># random index</span></span><br><span class="line">        feed = &#123;xph: x_data[rand_ind], yph: y_true[rand_ind]&#125;</span><br><span class="line">        sess.run(train, feed_dict = feed)</span><br><span class="line">    model_m, model_b = sess.run([m, b])</span><br><span class="line"></span><br><span class="line">y_hat = x_data * model_m + model_b</span><br><span class="line">my_data.sample(<span class="number">250</span>).plot(kind = <span class="string">'scatter'</span>, x = <span class="string">'X Data'</span>, y = <span class="string">'Y'</span>)</span><br><span class="line">plt.plot(x_data, y_hat, <span class="string">'r'</span>)</span><br></pre></td></tr></table></figure></p><p><img src="/images/2018/01/TF4.png" alt="Sample Image Added via Markdown"></p><h1 id="TensorFlow-Estimator-API"><a href="#TensorFlow-Estimator-API" class="headerlink" title="TensorFlow Estimator API"></a>TensorFlow Estimator API</h1><p>This section is to solve the regression task with TensorFlow estimator API. Wait a minute, what is API?  Technically, API stands for <em>Application Programming Interface</em>, but still, what is that? Basically, it is part of the server, that can be distinctively separated from its environment, that receives requests and sends responses. Here is an excellent article from Petr Gazarov explaining <a href="https://medium.freecodecamp.org/what-is-an-api-in-english-please-b880a3214a82" target="_blank" rel="noopener"><strong>API</strong></a>.<br>There are many other higher levels of APIs (Keras, Layersâ€¦). The tf.estimator API has several model types to choose from:</p><ul><li>tf.estimator.LinearClassifier: Constructs a linear classification model</li><li>tf.estimator.LinearRegressor: Constructs a linear regression model</li><li>tf.estimator.DNNClassifier: Constructs a neural network classification model</li><li>tf.estimator.DNNRegressor: Constructs a neural network regression model</li><li>tf.estimator.DNNLinearCombinedRegressor: Constructs a neural network and linear combined regression model</li></ul><p><br><br>Steps for using the Estimator API:</p><ul><li>Define a list of feature columns</li><li>Create the Estimator Model</li><li>Create a Data Input Function</li><li>Call train, evaluate, and predict methods on the estimator object</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">feat_cols = [tf.feature_column.numeric_column(<span class="string">'x'</span>, shape = <span class="number">1</span>)]  <span class="comment"># set all feature columns</span></span><br><span class="line">estimator = tf.estimator.LinearRegressor(feature_columns = feat_cols)  <span class="comment"># set estimator</span></span><br><span class="line"><span class="comment"># Split the data</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x_data, y_true, test_size = <span class="number">0.3</span>, random_state = <span class="number">101</span>)</span><br><span class="line"><span class="comment"># Set up Estimator Inputs</span></span><br><span class="line">input_func = tf.estimator.inputs.numpy_input_fn(&#123;<span class="string">'x'</span>: x_train&#125;, y_train, batch_size = <span class="number">8</span>, num_epochs = <span class="keyword">None</span>, shuffle = <span class="keyword">True</span>)     <span class="comment"># we can also use "tf.estimator.inputs.pandas_input_fn"</span></span><br><span class="line">train_input_func = tf.estimator.inputs.numpy_input_fn(&#123;<span class="string">'x'</span>: x_train&#125;, y_train, batch_size = <span class="number">8</span>, num_epochs = <span class="number">1000</span>, shuffle = <span class="keyword">False</span>)     <span class="comment"># change num_epochs and shuffle</span></span><br><span class="line">test_input_func = tf.estimator.inputs.numpy_input_fn(&#123;<span class="string">'x'</span>: x_test&#125;, y_test, batch_size = <span class="number">8</span>, num_epochs = <span class="number">1000</span>, shuffle = <span class="keyword">False</span>)     <span class="comment"># change num_epochs and shuffle, and the dataset</span></span><br><span class="line"><span class="comment"># Train the estimator</span></span><br><span class="line">estimator.train(input_fn = input_func, steps = <span class="number">1000</span>)</span><br><span class="line"><span class="comment"># Evaluation</span></span><br><span class="line">train_matrics = estimator.evaluate(input_fn = train_input_func, steps = <span class="number">1000</span>)</span><br><span class="line">test_matrics = estimator.evaluate(input_fn = test_input_func, steps = <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Training data matrics'</span>)</span><br><span class="line">print(train_matrics)</span><br><span class="line">print(<span class="string">'Test data matrics'</span>)</span><br><span class="line">print(test_matrics)</span><br></pre></td></tr></table></figure><p>Output:<br><code>Training data matrics{&#39;loss&#39;: 8.7310658, &#39;average_loss&#39;: 1.0913832, &#39;global_step&#39;: 1000}Test data matrics{&#39;loss&#39;: 8.6690454, &#39;average_loss&#39;: 1.0836307, &#39;global_step&#39;: 1000}</code><br>This is a good way to check if the model is overfitting (very low loss on training data but very high loss on test data). We want the loss of training data and test data are very close to each other.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get predict value</span></span><br><span class="line">brand_new_data = np.linspace(<span class="number">0</span>,<span class="number">10</span>,<span class="number">10</span>)</span><br><span class="line">input_fn_predict = tf.estimator.inputs.numpy_input_fn(&#123;<span class="string">'x'</span>: brand_new_data&#125;, shuffle = <span class="keyword">False</span>)</span><br><span class="line">estimator.predict(input_fn = input_fn_predict)</span><br><span class="line">list(estimator.predict(input_fn = input_fn_predict))</span><br></pre></td></tr></table></figure></p><p>Output:<br><code>[{&#39;predictions&#39;: array([ 4.43396044], dtype=float32)}, {&#39;predictions&#39;: array([ 5.06833887], dtype=float32)}, {&#39;predictions&#39;: array([ 5.7027173], dtype=float32)}, {&#39;predictions&#39;: array([ 6.33709526], dtype=float32)}, {&#39;predictions&#39;: array([ 6.97147369], dtype=float32)}, {&#39;predictions&#39;: array([ 7.60585213], dtype=float32)}, {&#39;predictions&#39;: array([ 8.24023056], dtype=float32)}, {&#39;predictions&#39;: array([ 8.87460899], dtype=float32)}, {&#39;predictions&#39;: array([ 9.50898743], dtype=float32)}, {&#39;predictions&#39;: array([ 10.14336586], dtype=float32)}]</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plot the prediction</span></span><br><span class="line">predictions = []</span><br><span class="line"><span class="keyword">for</span> pred <span class="keyword">in</span> estimator.predict(input_fn = input_fn_predict):</span><br><span class="line">    predictions.append(pred[<span class="string">'predictions'</span>])</span><br><span class="line">my_data.sample(<span class="number">250</span>).plot(kind = <span class="string">'scatter'</span>, x = <span class="string">'X Data'</span>, y = <span class="string">'Y'</span>)</span><br><span class="line">plt.plot(brand_new_data, predictions, <span class="string">'r'</span>)</span><br></pre></td></tr></table></figure></p><p><img src="/images/2018/01/TF5.png" alt="Sample Image Added via Markdown"></p><h1 id="TensorFlow-Classification"><a href="#TensorFlow-Classification" class="headerlink" title="TensorFlow Classification"></a>TensorFlow Classification</h1><p>Use real dataset â€œPima Indians Diabetes Datasetâ€, including both categorical and continuous features, to use tf.estimator switching models â€“ from linear classifier to dense neural network classifier. </p><h2 id="Linear-Classifier"><a href="#Linear-Classifier" class="headerlink" title="Linear Classifier"></a>Linear Classifier</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">diabetes = pd.read_csv(<span class="string">'pima-indians-diabetes.csv'</span>)</span><br><span class="line">diabetes.head()</span><br></pre></td></tr></table></figure><p><img src="/images/2018/01/TF6.png" alt="Sample Image Added via Markdown"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">diabetes.columns</span><br></pre></td></tr></table></figure></p><p>Output: <code>Index([&#39;Number_pregnant&#39;, &#39;Glucose_concentration&#39;, &#39;Blood_pressure&#39;, &#39;Triceps&#39;,       &#39;Insulin&#39;, &#39;BMI&#39;, &#39;Pedigree&#39;, &#39;Age&#39;, &#39;Class&#39;, &#39;Group&#39;],      dtype=&#39;object&#39;)</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># normalize the data except the catagorical part (Age, Class, Group)</span></span><br><span class="line">cols_to_norm = [<span class="string">'Number_pregnant'</span>, <span class="string">'Glucose_concentration'</span>, <span class="string">'Blood_pressure'</span>, <span class="string">'Triceps'</span>,</span><br><span class="line">       <span class="string">'Insulin'</span>, <span class="string">'BMI'</span>, <span class="string">'Pedigree'</span>]</span><br><span class="line">diabetes[cols_to_norm] = diabetes[cols_to_norm].apply(<span class="keyword">lambda</span> x: (x - x.min())/(x.max() - x.min()))</span><br><span class="line">diabetes.head()</span><br></pre></td></tr></table></figure></p><p><img src="/images/2018/01/TF7.png" alt="Sample Image Added via Markdown"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create feature columns</span></span><br><span class="line"><span class="comment"># continuous value</span></span><br><span class="line">num_preg = tf.feature_column.numeric_column(<span class="string">'Number_pregnant'</span>)</span><br><span class="line">plasma_gluc = tf.feature_column.numeric_column(<span class="string">'Glucose_concentration'</span>)</span><br><span class="line">dias_press = tf.feature_column.numeric_column(<span class="string">'Blood_pressure'</span>)</span><br><span class="line">tricep = tf.feature_column.numeric_column(<span class="string">'Triceps'</span>)</span><br><span class="line">insulin = tf.feature_column.numeric_column(<span class="string">'Insulin'</span>)</span><br><span class="line">bmi = tf.feature_column.numeric_column(<span class="string">'BMI'</span>)</span><br><span class="line">diabetes_pedigree = tf.feature_column.numeric_column(<span class="string">'Pedigree'</span>)</span><br><span class="line">age = tf.feature_column.numeric_column(<span class="string">'Age'</span>)</span><br><span class="line"><span class="comment"># catagorical value</span></span><br><span class="line">assigned_group = tf.feature_column.categorical_column_with_vocabulary_list(<span class="string">'Group'</span>, [<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'C'</span>,<span class="string">'D'</span>])     </span><br><span class="line"><span class="comment"># assigned_group = tf.feature_column.categorical_column_with_hash_bucket('Group', hash_bucket_size = 10)  </span></span><br><span class="line"><span class="comment"># For many groups scenario we can use hash_bucket, as long as we set the hash_bucket_size &gt; # of groups</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line">diabetes[<span class="string">'Age'</span>].hist(bins = <span class="number">20</span>)</span><br></pre></td></tr></table></figure></p><p><img src="/images/2018/01/TF8.png" alt="Sample Image Added via Markdown"><br>The distribution of Age from the histgram shows that most of the people are 20s. And instead of treating this variable as a continuous variable, we can bucket these values together, making boundary for each decade or so. <strong>Basically we use bucket system to transform continuous variable into categorial variable.</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">age_bucket = tf.feature_column.bucketized_column(age, boundaries = [<span class="number">20</span>,<span class="number">30</span>,<span class="number">40</span>,<span class="number">50</span>,<span class="number">60</span>,<span class="number">70</span>,<span class="number">80</span>])</span><br><span class="line"><span class="comment"># feature columns</span></span><br><span class="line">feat_cols = [num_preg ,plasma_gluc,dias_press ,tricep ,insulin,bmi,diabetes_pedigree ,assigned_group, age_bucket]</span><br><span class="line"><span class="comment"># train test split</span></span><br><span class="line">x_data = diabetes.drop(<span class="string">'Class'</span>, axis = <span class="number">1</span>)   <span class="comment"># Class column is Y</span></span><br><span class="line">labels = diabetes[<span class="string">'Class'</span>]</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(x_data, labels, test_size = <span class="number">0.3</span>, random_state = <span class="number">101</span>)</span><br></pre></td></tr></table></figure></p><p>Create the model!<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Create the model</span></span><br><span class="line">input_func = tf.estimator.inputs.pandas_input_fn(x = X_train, y = y_train, batch_size = <span class="number">10</span>, num_epochs = <span class="number">1000</span>, shuffle = <span class="keyword">True</span>)</span><br><span class="line">model = tf.estimator.LinearClassifier(feature_columns = feat_cols, n_classes = <span class="number">2</span>)</span><br><span class="line">model.train(input_fn = input_func, steps = <span class="number">1000</span>)</span><br><span class="line"><span class="comment">## Evaluate the model</span></span><br><span class="line">eval_input_func = tf.estimator.inputs.pandas_input_fn(x = X_test, y = y_test, batch_size = <span class="number">10</span>, num_epochs = <span class="number">1</span>, shuffle = <span class="keyword">False</span>)</span><br><span class="line">results = model.evaluate(eval_input_func)</span><br><span class="line">results</span><br></pre></td></tr></table></figure></p><p>Output:<br><code>{&#39;accuracy&#39;: 0.73160172, &#39;accuracy_baseline&#39;: 0.64935064, &#39;auc&#39;: 0.79658431, &#39;auc_precision_recall&#39;: 0.6441071, &#39;average_loss&#39;: 0.52852213, &#39;global_step&#39;: 1000, &#39;label/mean&#39;: 0.35064936, &#39;loss&#39;: 5.0870256, &#39;prediction/mean&#39;: 0.35784912}</code><br>The accuracy is 73.16%.</p><p> Prediction!<br> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># No y value in prediction, put new data set in x</span></span><br><span class="line">pred_input_func = tf.estimator.inputs.pandas_input_fn(x = X_test, batch_size = <span class="number">10</span>, num_epochs = <span class="number">1</span>, shuffle = <span class="keyword">False</span>)</span><br><span class="line">predictions = model.predict(pred_input_func)</span><br><span class="line">my_pred = list(predictions)</span><br><span class="line">my_pred</span><br></pre></td></tr></table></figure></p><p><img src="/images/2018/01/TF9.png" alt="Sample Image Added via Markdown"></p><h2 id="Dense-Neural-Network-Classifier"><a href="#Dense-Neural-Network-Classifier" class="headerlink" title="Dense Neural Network Classifier"></a>Dense Neural Network Classifier</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a three-layer neural network, with 10 neurons in each layer </span></span><br><span class="line">dnn_model = tf.estimator.DNNClassifier(hidden_units = [<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>], feature_columns = feat_cols, n_classes = <span class="number">2</span>)</span><br><span class="line"><span class="comment">### If use the same of the previous step error will show up -- because of the categorial columns, so we need to use embeded group </span></span><br><span class="line">embedded_group_col = tf.feature_column.embedding_column(assigned_group, dimension = <span class="number">4</span>)</span><br><span class="line"><span class="comment">### Reset the feature columns</span></span><br><span class="line">feat_cols = [num_preg ,plasma_gluc,dias_press ,tricep ,insulin,bmi,diabetes_pedigree , embedded_group_col, age_bucket]</span><br><span class="line"><span class="comment">### Now do as the previous -- train the model</span></span><br><span class="line">input_func = tf.estimator.input.pandas_inputs_fn(X_train, y_train, batch_size = <span class="number">10</span>, num_epochs = <span class="number">1000</span>, shuffle = <span class="keyword">True</span>)</span><br><span class="line">dnn_model = tf.estimator.DNNClassifier(hidden_units = [<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>], feature_columns = feat_cols, n_classes = <span class="number">2</span>)</span><br><span class="line">dnn_model.train(input_fn = input_func, steps = <span class="number">1000</span>)</span><br><span class="line"><span class="comment"># Evaluate the model</span></span><br><span class="line">eval_input_func = tf.estimator.inputs.pandas_input_fn(x = X_test, y = y_test, batch_size = <span class="number">10</span>, num_epochs = <span class="number">1</span>, shuffle = <span class="keyword">False</span>)</span><br><span class="line">dnn_model.evaluate(eval_input_func)</span><br></pre></td></tr></table></figure><p>Output:<br><code>{&#39;accuracy&#39;: 0.75757575, &#39;accuracy_baseline&#39;: 0.64935064, &#39;auc&#39;: 0.82814813, &#39;auc_precision_recall&#39;: 0.67539084, &#39;average_loss&#39;: 0.49014509, &#39;global_step&#39;: 1000, &#39;label/mean&#39;: 0.35064936, &#39;loss&#39;: 4.7176466, &#39;prediction/mean&#39;: 0.36287409}</code></p>]]></content>
      
      
      <categories>
          
          <category> TensorFlow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TensorFlow </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LeetCode in Python3 - Two Sum</title>
      <link href="/2018/01/09/LeetCode1/"/>
      <url>/2018/01/09/LeetCode1/</url>
      
        <content type="html"><![CDATA[<p>As a graduate student in Statistics, I profoundly believe the power of programming skills to Statistics/ Data Science. I used R or Matlab as my default programming tools and they are pretty powerful to help me solve most mathematical related tasks. However I realize in Artificial Intelligence/ Machine Learning realm, Python is more prevelent. In order to better gain the gist of this mighty programming language, in the process of learning it, I will show my LeetCode practice in Python3 with relative explanations as well as pitfalls I encounter.<br><a id="more"></a></p><h3 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h3><p>Given an array of integers, return indices of the two numbers such that they add up to a specific target. You may assume that each input would have exactly one solution, and you may not use the same element twice.</p><h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><p>Given nums = [2, 7, 11, 15], target = 9,<br>Because nums[0] + nums[1] = 2 + 7 = 9,<br>return [0, 1].</p><h3 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">twoSum</span><span class="params">(self, nums, target)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)):</span><br><span class="line">            <span class="keyword">if</span> (target - nums[i])  <span class="keyword">in</span> nums[i+<span class="number">1</span>:]:</span><br><span class="line">                <span class="keyword">return</span> (i, nums.index(target - nums[i], i+<span class="number">1</span>) )</span><br></pre></td></tr></table></figure><h3 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h3><ul><li><p>It is easily to ignore the condition â€œnot use the same element twiceâ€ in the question. Initially I used<br><span style="color:blue"> <strong>return (i, nums.index(target - nums[i] )</strong></span> instead of<br><span style="color:blue"> <strong>return (i, nums.index(target - nums[i], i+1 )</strong></span>,<br>it works for most cases, but for example such as <strong>Given nums = [3,3], target = 6</strong>, it failed to give me [0,1], it returned [0,0]. So basically <strong>.index(A,B)</strong> is to find the position of A after Bth index.</p></li><li><p>Another point worthy of mentioning is <span style="color:red"> <strong>self</strong></span> in the definition part <strong>def twoSum( <span style="color:red"> self</span>, nums, target)</strong>.  <span style="color:red"> <strong>self</strong></span> is used to refer class in python. In this case the class is  <span style="color:blue"> <strong>Solution</strong></span> </p></li></ul><h3 id="Difficulty"><a href="#Difficulty" class="headerlink" title="Difficulty"></a>Difficulty</h3><p>Easy</p>]]></content>
      
      
      <categories>
          
          <category> LeetCode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TwoSum </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Data Mining Note 3 - K Nearest Neighbors</title>
      <link href="/2018/01/06/Data-Mining-Note3/"/>
      <url>/2018/01/06/Data-Mining-Note3/</url>
      
        <content type="html"><![CDATA[<p>This chapter talks about k Nearest Neighbors (kNN) as a metholodogy for both classification and regression problems. The kNN method serves a basic and easy to understand foundational machine learning and data mining technique. The kNN method is an excellent baseline machine learning technique, and also allows many extensions. It usually performs reasonable well or sometimes very well when compared to more sophisticated techniques.</p><p>kNN classification basically means the estimated class of a vector $\mathbf{x}$ is the most frequent class label in the neighborhood of $\mathbf{x}$. kNN classifiers are inherently naturally multi-class, and are used extensively in applications such as image processing, character recognition and general pattern recognition tasks. For kNN regression, the estimated response value of a vector $\mathbf{x}$ is the average of the response values in the neighborhood of $\mathbf{x}$.<br><a id="more"></a></p><p><strong>Principle for kNN Classification</strong>: The reasonable class/category for a given object is the most prevalent class among its nearest neighbors.<br><strong>Principle k-Nearest Neighbor Regression</strong>: The reasonable prediction of the response value for a given object is the average of the response values of its nearest neighbors.</p><p><strong>General Steps for kNN</strong><br>Comparison of Classification and Regression</p><ol><li>Choose a distance for measuring how far a given point is from another</li><li>Set the <strong>size</strong> of the neighborhood k</li><li>Compute the distance from each existing point to the new point</li><li>Identify the class labels of the k points closest/nearest to the new point (classification)<br>Identify the response values of the k points closest/nearest to the new point (regression)</li><li>Assign the most frequent label to the new point (classification)<br>Compute the average of the response values of those k neighbors as the best estimate of the new point (regression)</li></ol><p><br></p><h3 id="Distance"><a href="#Distance" class="headerlink" title="Distance"></a>Distance</h3><p>First introduce some most commonly used distance below, which would be applied later in kNN.</p><ul><li><p>Euclidean distance: also known as the $l_2$ distance</p><span>$$\begin{align*}d(\mathbf{x}_i,\mathbf{x}_j)  = \sqrt{\sum_{l=1}^q(x_{il}-x_{jl})^2} = ||\mathbf{x}_i-\mathbf{x}_j||_2 \end{align*}$$</span><!-- Has MathJax --></li><li><p>Manhattan distance (city block): also known as $l_2$ distance</p><span>$$\begin{align*}d(\mathbf{x}_i,\mathbf{x}_j)  = \sum_{l=1}^q|x_{il}-x_{jl}| = ||\mathbf{x}_i-\mathbf{x}_j||_1 \end{align*}$$</span><!-- Has MathJax --></li><li><p>Maximum distance: also known as the infinity distance</p><span>$$\begin{align*}d(\mathbf{x}_i,\mathbf{x}_j)  = \underset{l=1,...,q}{\text{max}}|x_{il}-x_{jl}| = ||\mathbf{x}_i-\mathbf{x}_j||_\infty\end{align*}$$</span><!-- Has MathJax --></li><li><p>Minkowski distance: also known as $l_p$ distance</p><span>$$\begin{align*}d(\mathbf{x}_i,\mathbf{x}_j)  = \Big\{\sum_{l=1}^q|x_{il}-x_{jl}| \Big\}^{1/p}\end{align*}$$</span><!-- Has MathJax --></li><li><p>Canberra distance:</p><span>$$\begin{align*}d(\mathbf{x}_i,\mathbf{x}_j)  =\sum_{l=1}^q \frac{ |x_{il}-x_{jl}| }{ |x_{il}+x_{jl}| }  \end{align*}$$</span><!-- Has MathJax --></li><li><p>Jaccard/Tanimoto distance: For binary vectors ie $\mathbf{x}_i\in {0,1}^q$</p><span>$$\begin{align*}d(\mathbf{x}_i,\mathbf{x}_j)  =1- \frac{ \mathbf{x}_i\cdot \mathbf{x}_j }{ |\mathbf{x}_i|^2 + |\mathbf{x}_j|^2  - \mathbf{x}_i\cdot \mathbf{x}_j}  \end{align*}$$</span><!-- Has MathJax --></li></ul><p><br></p><h3 id="kNN-Classification"><a href="#kNN-Classification" class="headerlink" title="kNN Classification"></a>kNN Classification</h3><h4 id="Detailed-Steps-for-kNN-Classification"><a href="#Detailed-Steps-for-kNN-Classification" class="headerlink" title="Detailed Steps for kNN Classification"></a>Detailed Steps for kNN Classification</h4><p>$\mathcal{D} = \Big\{(x_1, Y_1),â€¦, (x_n, Y_n) \Big\}$, with $x_i\in \mathcal{X}^q, Y_i\in \{1,â€¦,g\}$</p><ol><li>Choose the value of k and the distance to be used</li><li>Let $\mathbf{x}^*$ be a new point. Compute  $d(\mathbf{x}^*,\mathbf{x}_i)  \quad i=1,2,â€¦n $</li><li>Rank all the distances $d^âˆ—_i$ in increasing order: <span>$$\begin{align*} d^&lowast;_{(1)} \leq  d^&lowast;_{(2)} \leq ...\leq  d^&lowast;_{(k)} \leq  d^&lowast;_{(k+1)} \leq ... d^&lowast;_{(n)} \end{align*}$$</span><!-- Has MathJax --></li><li>Form $\mathcal{V}_k(\mathbf{x}^âˆ—)$, the k-Neighborhood of $\mathbf{x}^âˆ—$<span>$$\begin{align*} \mathcal{V}_k(\mathbf{x}^&lowast;) = \Big\{ \mathbf{x}_i:  d(\mathbf{x}^*,\mathbf{x}_i)\leq d^*_{(k)}  \Big\}\end{align*}$$</span><!-- Has MathJax --></li><li>Compute the predicted response $\hat{Y}^*$ as <span>$$\begin{align*}\hat{Y}^*_{kNN} &amp;= \text{Most frequent label in } \mathcal{V}_k(\mathbf{x}^&lowast;) \\  &amp;=  \hat{f}^*_{kNN}(\mathbf{x}^*) = \underset{j\in\{1,...,g\}}{\texttt{argmax}} \Big\{  p_j^{(k)}(\mathbf{x}^*)  \Big\}\\ \text{where }  p_j^{(k)}(\mathbf{x}^*)= \frac{1}{k}\sum_{\mathbf{x}^* \in  \mathcal{V}_k(\mathbf{x}^&lowast;) } I(Y_i=j) &amp;\text{ estimates the probability that } \mathbf{x}^* \text{ belongs to class j based on} \mathcal{V}_k(\mathbf{x}^&lowast;)\end{align*}$$</span><!-- Has MathJax --></li></ol><ul><li>Note: <strong>[Posterior probability estimate]</strong> $ \frac{1}{k}\sum_{\mathbf{x}^* \in  \mathcal{V}_k(\mathbf{x}^âˆ—) } I(Y_i=j) $ can be regarded as a rough estimate of $ \pi_j (\mathbf{x}^*) = Pr[Y^*=j| \mathbf{x}^* ] $ , the posterior probability of class membership of $ \mathbf{x}^* $</li></ul><h4 id="Comments"><a href="#Comments" class="headerlink" title="Comments:"></a>Comments:</h4><ul><li>kNearest Neighbors (kNN) essentially performs classification by voting for the most popular response among the k nearest neighbors of $\mathbf{x}^*$.</li><li>kNN provides the most basic form of nonparametric classification.</li><li>Since the fundamental building block of kNN is the distance measure, one can easily perform classification beyond the traditional setting where the predictors are numeric. For instance, classification with kNN can be readily performed on indicator attributes<br>$$  \mathbf{x}^* = (x_{i1},â€¦, x_{ip}  )^T \in \{ 0,1\}^p $$</li><li>kNN classifiers are inherently naturally <strong>multi-class</strong>, and are used in many applications.</li></ul><p><br></p><h3 id="kNN-Regression"><a href="#kNN-Regression" class="headerlink" title="kNN Regression"></a>kNN Regression</h3><h4 id="Detailed-Steps-for-kNN-Regression"><a href="#Detailed-Steps-for-kNN-Regression" class="headerlink" title="Detailed Steps for kNN Regression"></a>Detailed Steps for kNN Regression</h4><p>$\mathcal{D} = \Big\{(x_1, Y_1),â€¦, (x_n, Y_n) \Big\}$, with $x_i\in \mathcal{X}^q, Y_i\in \mathbb{R}$</p><ol><li>Choose the value of k and the distance to be used</li><li>Let $\mathbf{x}^*$ be a new point. Compute  $d(\mathbf{x}^*,\mathbf{x}_i)  \quad i=1,2,â€¦n $</li><li>Rank all the distances $d^âˆ—_i$ in increasing order: <span>$$\begin{align*} d^&lowast;_{(1)} \leq  d^&lowast;_{(2)} \leq ...\leq  d^&lowast;_{(k)} \leq  d^&lowast;_{(k+1)} \leq ... d^&lowast;_{(n)} \end{align*}$$</span><!-- Has MathJax --></li><li>Form $\mathcal{V}_k(\mathbf{x}^âˆ—)$, the k-Neighborhood of $\mathbf{x}^âˆ—$<span>$$\begin{align*} \mathcal{V}_k(\mathbf{x}^&lowast;) = \Big\{ \mathbf{x}_i:  d(\mathbf{x}^*,\mathbf{x}_i)\leq d^*_{(k)}  \Big\}\end{align*}$$</span><!-- Has MathJax --></li><li>Compute the predicted response $\hat{Y}^*$ as <span>$$\begin{align*}\hat{Y}^*_{kNN} &amp;=  \hat{f}^*_{kNN}(\mathbf{x}^*)\\ &amp;= \frac{1}{k}\sum_{\mathbf{x}^* \in  \mathcal{V}_k(\mathbf{x}^&lowast;) } Y_i \\  &amp;=  \frac{1}{k}\sum_{i=1}^n Y_i I( \mathbf{x}^* \in \mathcal{V}_k(\mathbf{x}^&lowast;) )\end{align*}$$</span><!-- Has MathJax --></li></ol><h4 id="Comments-1"><a href="#Comments-1" class="headerlink" title="Comments:"></a>Comments:</h4><ul><li>kNearest Neighbors (kNN) essentially performs regression by averaging the responses of the nearest neighbors of $\mathbf{x}^*$.</li><li>kNN provides the most basic form of nonparametric regression</li><li>Since the fundamental building block of kNN is the distance measure, one can easily perform regression beyond the traditional setting where the predictors are numeric. For instance, Regression vectors of binary with kNN can be readily performed on indicator attributes<br>$$  \mathbf{x}^* = (x_{i1},â€¦, x_{ip}  )^T \in \{ 0,1\}^p $$ </li><li>kNN somewhat performs smoothing (filtering).</li><li>The estimated response kNN for $\mathbf{x}^*$ is estimator of the average response which is the <strong>conditional expectation of Y given $\mathbf{x}^*$</strong><br>$$ \hat{Y}^*_{kNN} =\mathbb{E}\widehat{[Y^*|\mathbf{x}^*]}  $$</li></ul><p><br></p><h3 id="Basic-kNN-amp-Weighted-kNN"><a href="#Basic-kNN-amp-Weighted-kNN" class="headerlink" title="Basic kNN &amp; Weighted kNN"></a>Basic kNN &amp; Weighted kNN</h3><h4 id="Limitation-of-Basic-kNN"><a href="#Limitation-of-Basic-kNN" class="headerlink" title="Limitation of Basic kNN"></a>Limitation of Basic kNN</h4><ul><li><p><strong>Equidistance</strong>: All neighbors are given the same contribution to the estimate of the response; In the estimated probability</p><span>$$\begin{align*} p_j^{(k)}(\mathbf{x}^*) &amp; = \frac{1}{k}\sum_{\mathbf{x}^* \in  \mathcal{V}_k(\mathbf{x}^&lowast;) } I(Y_i=j) =\sum_{\mathbf{x}^* \in  \mathcal{V}_k(\mathbf{x}^&lowast;) } w_i I(Y_i=j) \\ \text{the weight } w_i = \frac{1}{k}=\texttt{constant }&amp;\text{for all points in }  \mathcal{V}_k(\mathbf{x}^&lowast;) \text{ regardlesslyof how far they are from } \mathbf{x}^&lowast;\end{align*}$$</span><!-- Has MathJax --></li><li><p><strong>No model, weak interpretability</strong>: There is no underlying model, therefore no interpretation of the response relative to the predictor variables. There is no training set, since all happens at prediction. For this reason, kNN is referred to as <strong>lazy method</strong>.</p></li><li><p><strong>Computationally intensive</strong>: Predictions are computationally very intensive, due to the fact that for each new observation, the whole dataset must be traversed to compute the response</p></li></ul><p><br></p><h4 id="Extension-Weighted-kNN"><a href="#Extension-Weighted-kNN" class="headerlink" title="Extension: Weighted kNN"></a>Extension: Weighted kNN</h4><p>kNN classification can be improved by weighting the votes as a function of the distance from $\mathbf{x}^âˆ—$. The weights are defined so as to preserve convexity $ \sum_{i=1}^k w_i = 1$. Some of the common weighting schemes include:</p><ul><li>Exponential Decay:<span>$$\begin{align*}w_i = \frac{e^{-d_i^*}}{\sum_{l=1}^k e^{-d_l^*}  } \end{align*}$$</span><!-- Has MathJax --></li><li>Inverse Distance:<span>$$\begin{align*}w_i = \frac{ \frac{1}{1+d_i^*}}{\sum_{l=1}^k  \frac{1}{1+d_l^*} } \end{align*}$$</span><!-- Has MathJax --></li></ul><p><br></p><h3 id="Effect-of-kNN"><a href="#Effect-of-kNN" class="headerlink" title="Effect of kNN"></a>Effect of kNN</h3><h4 id="Effect-of-k"><a href="#Effect-of-k" class="headerlink" title="Effect of k"></a>Effect of k</h4><ul><li><p>k controls the complexity of the underlying classifier, with small k yielding very complex classifiers and large k yielding rather simple ones.</p></li><li><p>If k is <strong>small</strong>, the estimated class is determined based on very few neighbors, the resulting kNN classifier will have very <strong>low bias</strong>, but very <strong>high variance</strong>. Take the example of k=1, the decision boundary will perfectly separate the classes on the training set, but will perform poorly on the test set.</p></li><li><p>If k is <strong>large</strong>, the estimated class is determined based on very many neighbors from far and wide, the resulting kNN classifier will<br>have very <strong>large bias</strong>, but very <strong>low variance</strong>. In fact, for truly large k, the decision boundary will be a constant hyperplane.</p></li><li><p>The determination of an optimal k desires the trade-off between bias and variance:<br>Determine k by cross validation<br>Determine k by direct minimization of the estimated prediction error via a suitably chosen test set</p><h4 id="Effect-of-n"><a href="#Effect-of-n" class="headerlink" title="Effect of n"></a>Effect of n</h4><p>As stated before, kNN is a lazy method, everything happens at prediction step. Thus the sample size n plays a crucial role, and <strong>large n</strong> would lead to <strong>intense</strong> prediction.</p><h4 id="Effect-of-p"><a href="#Effect-of-p" class="headerlink" title="Effect of p"></a>Effect of p</h4><p>The dimensionality p of the input space is <strong>only</strong> felt by the function that computes the <strong>distances</strong>. If the function is optimized, kNN should be <strong>unaffected</strong> by this dimensionality</p><h4 id="Effect-of-distance"><a href="#Effect-of-distance" class="headerlink" title="Effect of distance"></a>Effect of distance</h4><p>Some distances are more robust to extreme observations.</p></li></ul><h3 id="Pros-amp-Cons-of-kNN"><a href="#Pros-amp-Cons-of-kNN" class="headerlink" title="Pros &amp; Cons of kNN"></a>Pros &amp; Cons of kNN</h3><h4 id="Strength"><a href="#Strength" class="headerlink" title="Strength"></a>Strength</h4><ul><li>The kNN method is intuitively appealing and very easy to understand, explain, program/code and interpret</li><li>The kNN method provides a decent estimate of $Pr[Y = j|x]$, the posterior probability of class membership</li><li><span style="color:red"> <em>The kNN method easily handles missing values (by restricting distance calculations to subspace)</em></span></li><li><span style="color:red"> <em>As the number of training samples grows larger, the asymptotic misclassification error rate is bounded by twice the Bayes risk.</em></span><br>$$ \underset{n\rightarrow \infty}{lim} R(\hat{f}_n^{(kNN)}) \leq 2R^* $$</li><li><span style="color:red"> <em>The kNN method is naturally suitable for sequential/incremental machine learning.</em></span></li><li><span style="color:red"> <em>The kNN method is also suitable where the hypothesis space is variable in size.</em></span></li><li>The kNN method can handle <strong>non-numeric</strong> data as long as the <strong>distance</strong> can be defined.</li><li>The kNN methods can handle <strong>mixed types</strong> of data as long as the <strong>distance</strong> are computed as <strong>hybrid or combinations</strong>.</li><li>The kNN method is inherently multi-class. This is very important because for some other methods, going beyond binary classification requires some sophisticated mathematics. It also handles very flexible decision boundaries.</li></ul><h4 id="Weakness"><a href="#Weakness" class="headerlink" title="Weakness"></a>Weakness</h4><ul><li>The <strong>computational complexity</strong> of kNN is very high in prediction. Specifically, it is $\mathcal{O}(nmp)$ where n is the training set size, m is the test set size and p is the number of predictor variables. This means that kNN requires <strong>large amount of memory</strong>, and therefore does NOT scale well. This <strong>failure in scalability</strong> is addressed using various heuristics and strategies.</li><li>kNN methods <strong>suffer from the Curse of Dimensionality (COD)</strong>. When p is large and n is small (short fat data/ dimension of the space very high), the concept of nearness becomes meaningless to the point of being ill-defined, because the â€neighborhoodâ€ becomes very large. Thus, the nearest neighbor could be very far when the space is high dimensional and there are very few observations.</li><li>The kNN method does not yield a model, and therefore no parameter to help explain why the method performs as it does.</li><li>The kNN method is heavily affected by the local structure, and it is very sensitive to both irrelevant and correlated features. Unless the distance is well chosen and properly calibrated, kNN methods will be sensitive to outliers and all sorts of noise in the data.</li><li>Unless the distance is used in some way to weight the neighbors, more <strong>frequent classes will dominate</strong> in the determination of the estimated label. This means one has to be careful with kNN when one class has a far larger proportion of observations than the others.</li><li>The measurement scale of each variable affect the kNN method more than most methods. (measurement scale: standardizing, unitizing or cubizing/squeezing the data).</li></ul><p><br></p><h3 id="Application-of-kNN"><a href="#Application-of-kNN" class="headerlink" title="Application of kNN"></a>Application of kNN</h3><ul><li><strong>Handwritten Digit Recognition</strong> is usually the first task in some Data Analytics competitions.</li><li><strong>Text Mining</strong> and specific topic of text categorization/classification has made successful use of kNearest Neighbors approach</li><li><strong>Credit Scoring</strong> is another application that has been connected with k Nearest Neighbors Classification</li><li><strong>Disease Diagnostics</strong> also has been tackled using k Nearest Neighbors Classifiers</li></ul>]]></content>
      
      
      <categories>
          
          <category> Data Mining </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kNN </tag>
            
            <tag> Classification </tag>
            
            <tag> Regression </tag>
            
            <tag> Distance </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Data Mining Note 2 - Binary Classification</title>
      <link href="/2018/01/05/Data-Mining-Note2/"/>
      <url>/2018/01/05/Data-Mining-Note2/</url>
      
        <content type="html"><![CDATA[<p>This chapter gives basic introduction of Vapnik-Chervonenkis Theory on Binary Classification. When it comes to binary classification task, we always want to know the functional relationship between $\mathbf{x}$ and $y$, or how to determine the â€œbestâ€ approach to determining from the available observaions such that given a new observation $\mathbf{x}^{new}$, we can predict its class $y^{new}$ as accurately as possible.</p><h3 id="Universal-Best"><a href="#Universal-Best" class="headerlink" title="Universal Best"></a>Universal Best</h3><p>Constructing a classification rule that puts all the points in their corresponding classes could be dangerous for classifying new observations not present in the current collection of observations. So to find an classification rule that achieves the absolute very best on the present data is not enough since infinitely many more observations can be generated. <strong>Even the universally best classifier will make mistakes.</strong><br><a id="more"></a></p><p>Of all the functions in $\mathcal{Y}^{\mathcal{X}}$, assume that there is a function $f^*$ that maps any $\mathbf{x}\in\mathcal{X}$ to the corresponding $y\in\mathcal{Y}$ with the minimum number of mistakes. $f^*$ is denoted as <strong>universal best</strong><br><span>$$\begin{align*}f^* :\quad &amp; \mathcal{X} \rightarrow \mathcal{Y}\\     &amp;  \mathbf{x} \rightarrow f^*(\mathbf{x})\end{align*}$$</span><!-- Has MathJax --></p><p>$f$ denote any generic function mapping an element $\mathbf{x}$ of $\mathcal{X}$ to its corresponding image $f(\mathbf{x})$ in $\mathcal{Y}$. Each time $\mathbf{x}$ is drawn from $\mathbb{P}(\mathbf{x})$, the disagreement between the image $f(\mathbf{x})$ and the true image y is called the loss, denoted by $l(y, f(\mathbf{x}))$. The expected value of this loss function with respect to the distribution $\mathbb{P}(\mathbf{x})$ is called <strong>the risk functional</strong> of $f$, denoted as $R(f)$.<br>$$ R(f) = \mathbb{E}[l(Y, f(\mathbf{X}))] = \int l(y, f(\mathbf{x})) d \mathbb{P}(\mathbf{x})  $$<br>The best function $f^*$ over the space $\mathcal{Y}^{\mathcal{X}}$ of all measurable functions from $\mathcal{X}$ to $\mathcal{Y}$ is therefore<br><span>$$\begin{align*}f^* &amp;=\texttt{arg} \underset{f}{inf} R(f)  \\R(f^*) &amp;= R^* = \underset{f}{inf} R(f)\end{align*}$$</span><!-- Has MathJax --><br>Note: Finding $f^*$ without the knowledge of $\mathbb{P}(\mathbf{x})$ means having to search the infinite dimensional function space $\mathcal{Y}^{\mathcal{X}}$ of all mappings from $\mathcal{X}$ to $\mathcal{Y}$, which is an ill-posed and computationally nasty problem (No way to get the universal best).</p><p>Thus, we seek a more reasonable way to solve the problem via choosing from a function space $F\subset \mathcal{Y}^{\mathcal{X}} $ , a function $f^+ \in F$ that best estimates the dependencies between $\mathbf{x}$ and $y$. The <strong>goal</strong> of statistical function estimation is to devise a technique (strategy) that chooses from the function class $\mathcal{F}$, the one function whose true risk is as close as possible to the lowest risk in class $\mathcal{F}$. So it is crucial to define what is meant to be best estimates. Hence we need to know what is loss function and risk functional.</p><h3 id="Loss-Functioin-and-Risk-Functional"><a href="#Loss-Functioin-and-Risk-Functional" class="headerlink" title="Loss Functioin and Risk Functional"></a>Loss Functioin and Risk Functional</h3><p>For classification task, the so-called 0-1 loss function defined below is used.<br>$$ l(y, f(\mathbf{x})) = \mathbb{1}_{Y\neq f(X)} = \begin{cases} 0, &amp;\text{if } y = f(\mathbf{x}) \\  1, &amp;\text{if } y \neq f(\mathbf{x}) \end{cases} $$</p><p>And the corresponding risk functional, also named cost function showed below confirms the intuition because it is estimated in practice by simply computng the proportion of misclassfied entities.<br>$$ R(f)= \int l(y, f(\mathbf{x})) d \mathbb{P}(\mathbf{x})   = \mathbb{E}[\mathbb{1}_{Y\neq f(X)}] = \underset{(X,Y)\sim \mathbb{P}}{Pr}[Y\neq f(X)]$$</p><p>The minimizer of the 0-1 risk functional over all possible classifiers is the so-called Bayes classifier which we shall denote here by $f^*$. It minimizes the rate of misclassifications.<br>$$  f^* =\texttt{arg} \underset{f}{inf} R(f) =  \texttt{arg} \underset{f}{inf} [\underset{(X,Y)\sim \mathbb{P}}{Pr}[Y\neq f(X)]  ]$$</p><p>Specifically, the Bayesâ€™ classifier $f^*$ is given by the posterior probability of class membership, namely<br>$$  f^* =\texttt{arg} \underset{y\in \mathcal{Y}}{max} [Pr[Y=y | \mathbf{x}]  ]$$</p><p>Because it is impossible to find the universal best $f^*$, We need to select a reasonable function space $F\subset \mathcal{Y}^{\mathcal{X}} $, and then choose the best estimator $f^+$ from $\mathcal{F}$. For the binary pattern recognition problem, one may consider finding the best linear separating hyperplane.</p><p>For empirical risk minimization,<br><span>$$\begin{align*} \hat{R}(f) &amp;=  \frac{1}{n}\sum_{i=1}^n \mathbb{1}_{Y\neq f(X)}\\  \hat{f} &amp;=     \underset{f\in \mathcal{F}}{\texttt{argmin}} [  \frac{1}{n}\sum_{i=1}^n \mathbb{1}_{Y\neq f(X)} ]\end{align*}$$</span><!-- Has MathJax --></p><h3 id="Bias-Variance-Trade-Off"><a href="#Bias-Variance-Trade-Off" class="headerlink" title="Bias-Variance Trade-Off"></a>Bias-Variance Trade-Off</h3><p>In statistical estimation, the blow important issues needs to be taken into account:</p><ul><li>Bias of the estimator</li><li>Variance of the estimator</li><li>Consistency of the estimator<br>In point estimation, given $\theta$ as the true value of the parameter, $\hat{\theta}$ is a point estimator of $\theta$, then the total error can be decomposed as: <span>$$\begin{align*}  \hat{\theta}-\theta = \underbrace{ \hat{\theta}- \mathbb{E}[ \hat{\theta}]}_\text{Estimation error}  + \underbrace{\mathbb{E}[ \hat{\theta}]-\theta}_\text{Bias} \end{align*}$$</span><!-- Has MathJax -->Under the squared error loss, one seeks $\hat{\theta}$ that minimizes the mean squared error, rather than trying to find the minimum variance unbiased estimator (MVUE).<span>$$\begin{align*}  \hat{\theta}=\underset{\theta\in \Theta}{\texttt{argmin}}\mathbb{E}[ (\hat{\theta}-\theta)^2] = \underset{\theta\in \Theta}{\texttt{argmin}}MSE(\hat{\theta})\end{align*}$$</span><!-- Has MathJax -->Actually, the traditional bias-variance decomposition of the MSE reveals the bias-variance trade-off: <span>$$\begin{align*} MSE(\hat{\theta}) &amp;= \mathbb{E}[ (\hat{\theta}-\theta)^2] \\  &amp;= \mathbb{E}[ (\hat{\theta}- \mathbb{E}[ \hat{\theta}])^2] + \mathbb{E}[(\mathbb{E}[ \hat{\theta}]-\theta)^2 ] \\  &amp;= \text{Variance} + \text{Bias}^2\end{align*}$$</span><!-- Has MathJax -->Again becasue of the previous statement that we cannot get the true value of $\theta$, MVUE will not help, which means the estimation we get contains bias. If the bias is too small, variance would be very large. Vice versa. The best compromise is then to trade-off bias and variance. In functional terms we call it trade-off between <strong>approximation error</strong> and <strong>estimation error</strong>.</li></ul><p><img src="/images/2018/01/BVTradeoff.png" alt="Sample Image Added via Markdown"></p><h3 id="Statistical-Consistency"><a href="#Statistical-Consistency" class="headerlink" title="Statistical Consistency"></a>Statistical Consistency</h3><p>$\hat{\theta}_n$ is a consistent estimator of $\theta$ if $\forall \epsilon &gt;0$<br>$$ \underset{n\rightarrow \infty}{lim}Pr[|\hat{\theta}_n-\theta|&gt;\epsilon] = 0 $$</p><p>It turns out that for unbiased estimators $\hat{\theta}_n$, consistency is straightforward as direct consequence of a basic probabilistic inequality like Chebyshevâ€™s inequality. However, for biased estimators, one has to be more careful.<br>The <strong>ERM (Consistency of the Empirical Risk Minimization) principle</strong> is consistent if it provides a sequence of functions for which both the expected risk and the empirical risk converge to the minimal possible value of the risk in the function class under consideration.<br><span>$$\begin{align*}R(\hat{f}_n) &amp; \xrightarrow[n \rightarrow \infty]{P} \underset{f\in \mathcal{F}}{inf}R(f) = R(f^+) \\ \underset{n\rightarrow \infty}{lim} Pr&amp;[\underset{f\in\mathcal{F}}{sup}|R(f)-\hat{R}_n(f)|&gt;\epsilon] = 0\end{align*}$$</span><!-- Has MathJax --><br>As $\hat{R}_n(f) $ reveals the disagreement between classifier $f$ and the truth about the label $y$ of $\mathbf{x}$ based on information contained in the sample $\mathcal{D}$. So for a given (fixed) function (classifier) $f$,<br>$$ \mathcal{E}[\hat{R}_n(f)] = R(f) $$</p><p><strong>Key Question:</strong><br>Since one cannot calculate the true error, how can one devise a learning strategy for choosing classifiers based on it?<br><strong>Tentative Answer:</strong><br>At least devise strategies that yield functions for which the upper bound on the theoretical risk is as tight as possible:</p><p>With probability $1 âˆ’ \delta$ over an i.i.d. draw of some sample according to the distribution $\mathcal{P}$, the expected future error rate of some classifier is bounded by a function $g$ ($\delta$, error rate on sample) of $\delta$ and the error rate on sample. $\Downarrow\Downarrow\Downarrow$<br><span>$$\begin{align*}\text{Pr}\Big\{ \texttt{TestError} \leq \texttt{TrainError} + \phi(n,\delta, \kappa (\mathcal{F}))  \Big\} \leq 1-\delta\end{align*}$$</span><!-- Has MathJax --></p><p><strong>Thorem: (Vapnik and Chervonenkis, 1971)</strong><br>Let $\mathcal{F}$ be a class of functions implementing so learning machines, and let $\xi = VCdim(\mathcal{F})$ be the VC dimension of F. Let the theoretical and the empirical risks be defined as earlier and consider any data distribution in the population of interest. Then $\forall f\in \mathcal{F}$, the prediction error (theoretical risk) is bounded by<br>$$  R(f) \leq \hat{R}_n(f) + \sqrt{  \frac{\zeta(\text{log}\frac{2n}{\zeta}+1)-\text{log}\frac{\eta}{4} }{n}} $$<br>with probability of at least $1-\eta$. or<br>$$\text{Pr} \Big( \texttt{TestError} \leq \texttt{TrainError} +     \sqrt{  \frac{\zeta(\text{log}\frac{2n}{\zeta}+1)-\text{log}\frac{\eta}{4} }{n}} \Big) \leq 1-\eta  $$</p><p><strong>VC Bound</strong></p><ul><li>From the expression of the VC Bound, to improve the predictive performance (reduce prediction error) of a class of machines is to achieve a<br>trade-off (compromise) between small VC dimension and minimization of the empirical risk.</li><li>One of the greatest appeals of the VC bound is that, though applicable to function classes of infinite dimension, it preserves the same intuitive form as the bound derived for finite dimensional $\mathcal{F}$</li><li>VC bound is acting in a way similar to the number of parameters, since it serves as a measure of the complexity of $\mathcal{F}$. </li><li>One should seek to construct a classifier that achieves the best trade-off between complexity of function class (measured by VC dimension) and fit to the training data (measured by empirical risk).</li></ul>]]></content>
      
      
      <categories>
          
          <category> Data Mining </category>
          
      </categories>
      
      
        <tags>
            
            <tag> classification </tag>
            
            <tag> Vapnik-Chervonenkis </tag>
            
            <tag> binary </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Data Mining Note 1 - Introduction</title>
      <link href="/2018/01/05/Data-Mining-Note1/"/>
      <url>/2018/01/05/Data-Mining-Note1/</url>
      
        <content type="html"><![CDATA[<p>I took Dr. Ernest Fokoueâ€™s course Data Mining (STAT 747) in my Master study in RIT and gained tremendous fascinating modern Statistical Machine Learning technique skills. I want to share the marvelous essence of this course and my self-learning and self-reflection towards this course.</p><p>This course covers topics such as clustering, classification and regression trees, multiple linear regression under various conditions, logistic regression, PCA and kernel PCA, model-based clustering via mixture of gaussians, spectral clustering, text mining, neural networks, support vector machines, multidimensional scaling, variable selection, model selection, k-means clustering, k-nearest neighbors classifiers, statistical tools for modern machine learning and data mining, naÃ¯ve Bayes classifiers, variance reduction methods (bagging) and ensemble methods for predictive optimality.<br><a id="more"></a></p><p>I will show the roadmap of this note in this post and follow the order. Basically, each post contains one essential data mining technique and later I will show some relative examples and exercises based on these methods.</p><ul><li>Supervised Learning<br>   Classification<br>   Regression</li><li>Unsupervised Learning<br>  Clustering Analysis<br>  Factor Analysis<br>  Topic Modeling<br>  Recommender System</li></ul><h3 id="Application-in-Statistical-Machine-Learning"><a href="#Application-in-Statistical-Machine-Learning" class="headerlink" title="Application in Statistical Machine Learning"></a>Application in Statistical Machine Learning</h3><ul><li>Handwritten Digit Recognition (MNIST)</li><li>Text Mining</li><li>Credit Scoring</li><li>Disease Diagonostics</li><li>Audio Processing</li><li>Speaker Recognition &amp; Speaker Identification</li></ul><h3 id="Computing-Tools-in-R"><a href="#Computing-Tools-in-R" class="headerlink" title="Computing Tools in R"></a>Computing Tools in R</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(ctv)</span><br><span class="line"><span class="keyword">library</span>(MachineLearning)</span><br><span class="line"><span class="keyword">library</span>(HighPerformanceComputing)</span><br><span class="line"><span class="keyword">library</span>(TimeSeries)</span><br><span class="line"><span class="keyword">library</span>(Bayesian)</span><br><span class="line"><span class="keyword">library</span>(Robust)</span><br><span class="line"><span class="keyword">library</span>(biglm)</span><br><span class="line"><span class="keyword">library</span>(foreach)</span><br><span class="line"><span class="keyword">library</span>(glmnet)</span><br><span class="line"><span class="keyword">library</span>(kernlab)</span><br><span class="line"><span class="keyword">library</span>(randomForest)</span><br><span class="line"><span class="keyword">library</span>(ada)</span><br><span class="line"><span class="keyword">library</span>(audio)</span><br><span class="line"><span class="keyword">library</span>(rpart)</span><br><span class="line"><span class="keyword">library</span>(e1071)</span><br><span class="line"><span class="keyword">library</span>(MASS)</span><br><span class="line"><span class="keyword">library</span>(kernlab)</span><br></pre></td></tr></table></figure><h1 id="Important-Aspects-of-Machine-Learning"><a href="#Important-Aspects-of-Machine-Learning" class="headerlink" title="Important Aspects of Machine Learning"></a>Important Aspects of Machine Learning</h1><h5 id="Machines-Inherently-designed-to-handle-p-larger-than-n-problems"><a href="#Machines-Inherently-designed-to-handle-p-larger-than-n-problems" class="headerlink" title="Machines Inherently designed to handle p larger than n problems"></a>Machines Inherently designed to handle p larger than n problems</h5><ul><li>Classification and Regression Trees</li><li>Support Vector Machines</li><li>Relevance Vector Machines (n &lt; 500)</li><li>Gaussian Process Learning Machines (n &lt; 500)</li><li>k-Nearest Neighbors Learning Machines (Watch for the curse of dimensionality)</li><li>Kernel Machines in general<h5 id="Machines-can-handle-p-larger-than-n-problems-if-regularized-with-suitable-constraints"><a href="#Machines-can-handle-p-larger-than-n-problems-if-regularized-with-suitable-constraints" class="headerlink" title="Machines can handle p larger than n problems if regularized with suitable constraints"></a>Machines can handle p larger than n problems if regularized with suitable constraints</h5></li><li>Multiple Linear Regression Models</li><li>Generalized Linear Models</li><li>Discriminant Analysis<h5 id="Ensemble-Learning-Machines"><a href="#Ensemble-Learning-Machines" class="headerlink" title="Ensemble Learning Machines"></a>Ensemble Learning Machines</h5></li><li>Random Subspace Learning Ensembles (Random Forest)</li><li>Boosting and its extensions</li></ul><p><br><br> <span style="color:red"> <strong>Note:</strong> <em>Red parts in this Note Series remain questionable and I will update and add explanations for those parts as soon as I figure them out.</em></span></p>]]></content>
      
      
      <categories>
          
          <category> Data Mining </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SVM </tag>
            
            <tag> KNN </tag>
            
            <tag> KMeans </tag>
            
            <tag> Ensemble </tag>
            
            <tag> Neural Network </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
