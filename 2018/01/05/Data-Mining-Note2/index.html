<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  

  
  <title>Data Mining Note 2 - Binary Classification | Qiuyi&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="This chapter gives basic introduction of Vapnik-Chervonenkis Theory on Binary Classification. When it comes to binary classification task, we always want to know the functional relationship between $\">
<meta name="keywords" content="classification,Vapnik-Chervonenkis,binary">
<meta property="og:type" content="article">
<meta property="og:title" content="Data Mining Note 2 - Binary Classification">
<meta property="og:url" content="https://qiuyiwu.github.io/2018/01/05/Data-Mining-Note2/index.html">
<meta property="og:site_name" content="Qiuyi&#39;s Blog">
<meta property="og:description" content="This chapter gives basic introduction of Vapnik-Chervonenkis Theory on Binary Classification. When it comes to binary classification task, we always want to know the functional relationship between $\">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://qiuyiwu.github.io/images/2018/01/BVTradeoff.png">
<meta property="og:updated_time" content="2019-01-26T15:12:21.039Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Data Mining Note 2 - Binary Classification">
<meta name="twitter:description" content="This chapter gives basic introduction of Vapnik-Chervonenkis Theory on Binary Classification. When it comes to binary classification task, we always want to know the functional relationship between $\">
<meta name="twitter:image" content="https://qiuyiwu.github.io/images/2018/01/BVTradeoff.png">
  
    <link rel="alternate" href="/atom.xml" title="Qiuyi&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
</html>
<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Qiuyi&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Researcher✨Qiuyi Wu</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://qiuyiwu.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Data-Mining-Note2" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/05/Data-Mining-Note2/" class="article-date">
  <time datetime="2018-01-06T01:07:53.000Z" itemprop="datePublished">2018-01-05</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Data-Mining/">Data Mining</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Data Mining Note 2 - Binary Classification
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>This chapter gives basic introduction of Vapnik-Chervonenkis Theory on Binary Classification. When it comes to binary classification task, we always want to know the functional relationship between $\mathbf{x}$ and $y$, or how to determine the “best” approach to determining from the available observaions such that given a new observation $\mathbf{x}^{new}$, we can predict its class $y^{new}$ as accurately as possible.</p>
<h3 id="Universal-Best"><a href="#Universal-Best" class="headerlink" title="Universal Best"></a>Universal Best</h3><p>Constructing a classification rule that puts all the points in their corresponding classes could be dangerous for classifying new observations not present in the current collection of observations. So to find an classification rule that achieves the absolute very best on the present data is not enough since infinitely many more observations can be generated. <strong>Even the universally best classifier will make mistakes.</strong><br><a id="more"></a></p>
<p>Of all the functions in $\mathcal{Y}^{\mathcal{X}}$, assume that there is a function $f^*$ that maps any $\mathbf{x}\in\mathcal{X}$ to the corresponding $y\in\mathcal{Y}$ with the minimum number of mistakes. $f^*$ is denoted as <strong>universal best</strong><br><span>$$\begin{align*}
f^* :\quad &amp; \mathcal{X} \rightarrow \mathcal{Y}\\ 
    &amp;  \mathbf{x} \rightarrow f^*(\mathbf{x})
\end{align*}$$</span><!-- Has MathJax --></p>
<p>$f$ denote any generic function mapping an element $\mathbf{x}$ of $\mathcal{X}$ to its corresponding image $f(\mathbf{x})$ in $\mathcal{Y}$. Each time $\mathbf{x}$ is drawn from $\mathbb{P}(\mathbf{x})$, the disagreement between the image $f(\mathbf{x})$ and the true image y is called the loss, denoted by $l(y, f(\mathbf{x}))$. The expected value of this loss function with respect to the distribution $\mathbb{P}(\mathbf{x})$ is called <strong>the risk functional</strong> of $f$, denoted as $R(f)$.<br>$$ R(f) = \mathbb{E}[l(Y, f(\mathbf{X}))] = \int l(y, f(\mathbf{x})) d \mathbb{P}(\mathbf{x})  $$<br>The best function $f^*$ over the space $\mathcal{Y}^{\mathcal{X}}$ of all measurable functions from $\mathcal{X}$ to $\mathcal{Y}$ is therefore<br><span>$$\begin{align*}
f^* &amp;=\texttt{arg} \underset{f}{inf} R(f)  \\
R(f^*) &amp;= R^* = \underset{f}{inf} R(f)
\end{align*}$$</span><!-- Has MathJax --><br>Note: Finding $f^*$ without the knowledge of $\mathbb{P}(\mathbf{x})$ means having to search the infinite dimensional function space $\mathcal{Y}^{\mathcal{X}}$ of all mappings from $\mathcal{X}$ to $\mathcal{Y}$, which is an ill-posed and computationally nasty problem (No way to get the universal best).</p>
<p>Thus, we seek a more reasonable way to solve the problem via choosing from a function space $F\subset \mathcal{Y}^{\mathcal{X}} $ , a function $f^+ \in F$ that best estimates the dependencies between $\mathbf{x}$ and $y$. The <strong>goal</strong> of statistical function estimation is to devise a technique (strategy) that chooses from the function class $\mathcal{F}$, the one function whose true risk is as close as possible to the lowest risk in class $\mathcal{F}$. So it is crucial to define what is meant to be best estimates. Hence we need to know what is loss function and risk functional.</p>
<h3 id="Loss-Functioin-and-Risk-Functional"><a href="#Loss-Functioin-and-Risk-Functional" class="headerlink" title="Loss Functioin and Risk Functional"></a>Loss Functioin and Risk Functional</h3><p>For classification task, the so-called 0-1 loss function defined below is used.<br>$$ l(y, f(\mathbf{x})) = \mathbb{1}_{Y\neq f(X)} = \begin{cases} 0, &amp;\text{if } y = f(\mathbf{x}) \\  1, &amp;\text{if } y \neq f(\mathbf{x}) \end{cases} $$</p>
<p>And the corresponding risk functional, also named cost function showed below confirms the intuition because it is estimated in practice by simply computng the proportion of misclassfied entities.<br>$$ R(f)= \int l(y, f(\mathbf{x})) d \mathbb{P}(\mathbf{x})   = \mathbb{E}[\mathbb{1}_{Y\neq f(X)}] = \underset{(X,Y)\sim \mathbb{P}}{Pr}[Y\neq f(X)]$$</p>
<p>The minimizer of the 0-1 risk functional over all possible classifiers is the so-called Bayes classifier which we shall denote here by $f^*$. It minimizes the rate of misclassifications.<br>$$  f^* =\texttt{arg} \underset{f}{inf} R(f) =  \texttt{arg} \underset{f}{inf} [\underset{(X,Y)\sim \mathbb{P}}{Pr}[Y\neq f(X)]  ]$$</p>
<p>Specifically, the Bayes’ classifier $f^*$ is given by the posterior probability of class membership, namely<br>$$  f^* =\texttt{arg} \underset{y\in \mathcal{Y}}{max} [Pr[Y=y | \mathbf{x}]  ]$$</p>
<p>Because it is impossible to find the universal best $f^*$, We need to select a reasonable function space $F\subset \mathcal{Y}^{\mathcal{X}} $, and then choose the best estimator $f^+$ from $\mathcal{F}$. For the binary pattern recognition problem, one may consider finding the best linear separating hyperplane.</p>
<p>For empirical risk minimization,<br><span>$$\begin{align*}
 \hat{R}(f) &amp;=  \frac{1}{n}\sum_{i=1}^n \mathbb{1}_{Y\neq f(X)}\\ 
 \hat{f} &amp;=     \underset{f\in \mathcal{F}}{\texttt{argmin}} [  \frac{1}{n}\sum_{i=1}^n \mathbb{1}_{Y\neq f(X)} ]
\end{align*}$$</span><!-- Has MathJax --></p>
<h3 id="Bias-Variance-Trade-Off"><a href="#Bias-Variance-Trade-Off" class="headerlink" title="Bias-Variance Trade-Off"></a>Bias-Variance Trade-Off</h3><p>In statistical estimation, the blow important issues needs to be taken into account:</p>
<ul>
<li>Bias of the estimator</li>
<li>Variance of the estimator</li>
<li>Consistency of the estimator<br>In point estimation, given $\theta$ as the true value of the parameter, $\hat{\theta}$ is a point estimator of $\theta$, then the total error can be decomposed as: <span>$$\begin{align*}
  \hat{\theta}-\theta = \underbrace{ \hat{\theta}- \mathbb{E}[ \hat{\theta}]}_\text{Estimation error}  + \underbrace{\mathbb{E}[ \hat{\theta}]-\theta}_\text{Bias} 
\end{align*}$$</span><!-- Has MathJax -->
Under the squared error loss, one seeks $\hat{\theta}$ that minimizes the mean squared error, rather than trying to find the minimum variance unbiased estimator (MVUE).<span>$$\begin{align*}
  \hat{\theta}=\underset{\theta\in \Theta}{\texttt{argmin}}\mathbb{E}[ (\hat{\theta}-\theta)^2] = \underset{\theta\in \Theta}{\texttt{argmin}}MSE(\hat{\theta})
\end{align*}$$</span><!-- Has MathJax -->
Actually, the traditional bias-variance decomposition of the MSE reveals the bias-variance trade-off: <span>$$\begin{align*}
 MSE(\hat{\theta}) &amp;= \mathbb{E}[ (\hat{\theta}-\theta)^2] \\ 
 &amp;= \mathbb{E}[ (\hat{\theta}- \mathbb{E}[ \hat{\theta}])^2] + \mathbb{E}[(\mathbb{E}[ \hat{\theta}]-\theta)^2 ] \\ 
 &amp;= \text{Variance} + \text{Bias}^2
\end{align*}$$</span><!-- Has MathJax -->
Again becasue of the previous statement that we cannot get the true value of $\theta$, MVUE will not help, which means the estimation we get contains bias. If the bias is too small, variance would be very large. Vice versa. The best compromise is then to trade-off bias and variance. In functional terms we call it trade-off between <strong>approximation error</strong> and <strong>estimation error</strong>.</li>
</ul>
<p><img src="/images/2018/01/BVTradeoff.png" alt="Sample Image Added via Markdown"></p>
<h3 id="Statistical-Consistency"><a href="#Statistical-Consistency" class="headerlink" title="Statistical Consistency"></a>Statistical Consistency</h3><p>$\hat{\theta}_n$ is a consistent estimator of $\theta$ if $\forall \epsilon &gt;0$<br>$$ \underset{n\rightarrow \infty}{lim}Pr[|\hat{\theta}_n-\theta|&gt;\epsilon] = 0 $$</p>
<p>It turns out that for unbiased estimators $\hat{\theta}_n$, consistency is straightforward as direct consequence of a basic probabilistic inequality like Chebyshev’s inequality. However, for biased estimators, one has to be more careful.<br>The <strong>ERM (Consistency of the Empirical Risk Minimization) principle</strong> is consistent if it provides a sequence of functions for which both the expected risk and the empirical risk converge to the minimal possible value of the risk in the function class under consideration.<br><span>$$\begin{align*}
R(\hat{f}_n) &amp; \xrightarrow[n \rightarrow \infty]{P} \underset{f\in \mathcal{F}}{inf}R(f) = R(f^+) \\ 
\underset{n\rightarrow \infty}{lim} Pr&amp;[\underset{f\in\mathcal{F}}{sup}|R(f)-\hat{R}_n(f)|&gt;\epsilon] = 0
\end{align*}$$</span><!-- Has MathJax --><br>As $\hat{R}_n(f) $ reveals the disagreement between classifier $f$ and the truth about the label $y$ of $\mathbf{x}$ based on information contained in the sample $\mathcal{D}$. So for a given (fixed) function (classifier) $f$,<br>$$ \mathcal{E}[\hat{R}_n(f)] = R(f) $$</p>
<p><strong>Key Question:</strong><br>Since one cannot calculate the true error, how can one devise a learning strategy for choosing classifiers based on it?<br><strong>Tentative Answer:</strong><br>At least devise strategies that yield functions for which the upper bound on the theoretical risk is as tight as possible:</p>
<p>With probability $1 − \delta$ over an i.i.d. draw of some sample according to the distribution $\mathcal{P}$, the expected future error rate of some classifier is bounded by a function $g$ ($\delta$, error rate on sample) of $\delta$ and the error rate on sample. $\Downarrow\Downarrow\Downarrow$<br><span>$$\begin{align*}
\text{Pr}\Big\{ \texttt{TestError} \leq \texttt{TrainError} + \phi(n,\delta, \kappa (\mathcal{F}))  \Big\} \leq 1-\delta
\end{align*}$$</span><!-- Has MathJax --></p>
<p><strong>Thorem: (Vapnik and Chervonenkis, 1971)</strong><br>Let $\mathcal{F}$ be a class of functions implementing so learning machines, and let $\xi = VCdim(\mathcal{F})$ be the VC dimension of F. Let the theoretical and the empirical risks be defined as earlier and consider any data distribution in the population of interest. Then $\forall f\in \mathcal{F}$, the prediction error (theoretical risk) is bounded by<br>$$  R(f) \leq \hat{R}_n(f) + \sqrt{  \frac{\zeta(\text{log}\frac{2n}{\zeta}+1)-\text{log}\frac{\eta}{4} }{n}} $$<br>with probability of at least $1-\eta$. or<br>$$\text{Pr} \Big( \texttt{TestError} \leq \texttt{TrainError} +     \sqrt{  \frac{\zeta(\text{log}\frac{2n}{\zeta}+1)-\text{log}\frac{\eta}{4} }{n}} \Big) \leq 1-\eta  $$</p>
<p><strong>VC Bound</strong></p>
<ul>
<li>From the expression of the VC Bound, to improve the predictive performance (reduce prediction error) of a class of machines is to achieve a<br>trade-off (compromise) between small VC dimension and minimization of the empirical risk.</li>
<li>One of the greatest appeals of the VC bound is that, though applicable to function classes of infinite dimension, it preserves the same intuitive form as the bound derived for finite dimensional $\mathcal{F}$</li>
<li>VC bound is acting in a way similar to the number of parameters, since it serves as a measure of the complexity of $\mathcal{F}$. </li>
<li>One should seek to construct a classifier that achieves the best trade-off between complexity of function class (measured by VC dimension) and fit to the training data (measured by empirical risk).</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qiuyiwu.github.io/2018/01/05/Data-Mining-Note2/" data-id="cjt38hpbe000bldy564vunywk" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Vapnik-Chervonenkis/">Vapnik-Chervonenkis</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/binary/">binary</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/classification/">classification</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/01/06/Data-Mining-Note3/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Data Mining Note 3 - K Nearest Neighbors
        
      </div>
    </a>
  
  
    <a href="/2018/01/05/Data-Mining-Note1/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Data Mining Note 1 - Introduction</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Astrostatistics/">Astrostatistics</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Mining/">Data Mining</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/JUJUs/">JUJUs</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/LeetCode/">LeetCode</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Philosophy/">Philosophy</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Research/">Research</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Statistics/">Statistics</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/TensorFlow/">TensorFlow</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/API/">API</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Astronomy/">Astronomy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Astrostatistics/">Astrostatistics</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Birthday/">Birthday</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/">CNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Chinese-Restaurant/">Chinese Restaurant</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Classification/">Classification</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DNN/">DNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Fusion/">Data Fusion</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Mining/">Data Mining</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deep-Learning/">Deep Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Disqus/">Disqus</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Distance/">Distance</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ensemble/">Ensemble</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Firebase/">Firebase</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Firestore/">Firestore</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gaussian-Process/">Gaussian Process</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/">Hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Indian-Buffet/">Indian Buffet</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KMeans/">KMeans</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KNN/">KNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LeanCloud/">LeanCloud</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Network-Analysis/">Network Analysis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neural-Network/">Neural Network</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Nietzsche/">Nietzsche</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Nonparametric/">Nonparametric</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Philosophy/">Philosophy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/R/">R</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/">RNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Regression/">Regression</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/">SVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spatial-Statistics/">Spatial Statistics</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Statistics/">Statistics</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TensorFlow/">TensorFlow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Topic-Modeling/">Topic Modeling</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TwoSum/">TwoSum</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Valine/">Valine</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Vapnik-Chervonenkis/">Vapnik-Chervonenkis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/binary/">binary</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/blog/">blog</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/classification/">classification</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/comment/">comment</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hurrican-tracks/">hurrican tracks</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kNN/">kNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/local-search/">local search</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/music/">music</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/storm-surge/">storm surge</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/visitor-counts/">visitor counts</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/尼采/">尼采</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/API/" style="font-size: 10px;">API</a> <a href="/tags/Astronomy/" style="font-size: 10px;">Astronomy</a> <a href="/tags/Astrostatistics/" style="font-size: 10px;">Astrostatistics</a> <a href="/tags/Birthday/" style="font-size: 10px;">Birthday</a> <a href="/tags/CNN/" style="font-size: 15px;">CNN</a> <a href="/tags/Chinese-Restaurant/" style="font-size: 10px;">Chinese Restaurant</a> <a href="/tags/Classification/" style="font-size: 10px;">Classification</a> <a href="/tags/DNN/" style="font-size: 10px;">DNN</a> <a href="/tags/Data-Fusion/" style="font-size: 10px;">Data Fusion</a> <a href="/tags/Data-Mining/" style="font-size: 10px;">Data Mining</a> <a href="/tags/Deep-Learning/" style="font-size: 10px;">Deep Learning</a> <a href="/tags/Disqus/" style="font-size: 10px;">Disqus</a> <a href="/tags/Distance/" style="font-size: 10px;">Distance</a> <a href="/tags/Ensemble/" style="font-size: 10px;">Ensemble</a> <a href="/tags/Firebase/" style="font-size: 10px;">Firebase</a> <a href="/tags/Firestore/" style="font-size: 10px;">Firestore</a> <a href="/tags/Gaussian-Process/" style="font-size: 10px;">Gaussian Process</a> <a href="/tags/Hexo/" style="font-size: 20px;">Hexo</a> <a href="/tags/Indian-Buffet/" style="font-size: 10px;">Indian Buffet</a> <a href="/tags/KMeans/" style="font-size: 10px;">KMeans</a> <a href="/tags/KNN/" style="font-size: 10px;">KNN</a> <a href="/tags/LeanCloud/" style="font-size: 10px;">LeanCloud</a> <a href="/tags/Network-Analysis/" style="font-size: 10px;">Network Analysis</a> <a href="/tags/Neural-Network/" style="font-size: 10px;">Neural Network</a> <a href="/tags/Nietzsche/" style="font-size: 10px;">Nietzsche</a> <a href="/tags/Nonparametric/" style="font-size: 15px;">Nonparametric</a> <a href="/tags/Philosophy/" style="font-size: 10px;">Philosophy</a> <a href="/tags/Python/" style="font-size: 10px;">Python</a> <a href="/tags/R/" style="font-size: 10px;">R</a> <a href="/tags/RNN/" style="font-size: 15px;">RNN</a> <a href="/tags/Regression/" style="font-size: 10px;">Regression</a> <a href="/tags/SVM/" style="font-size: 10px;">SVM</a> <a href="/tags/Spatial-Statistics/" style="font-size: 10px;">Spatial Statistics</a> <a href="/tags/Statistics/" style="font-size: 20px;">Statistics</a> <a href="/tags/TensorFlow/" style="font-size: 15px;">TensorFlow</a> <a href="/tags/Topic-Modeling/" style="font-size: 15px;">Topic Modeling</a> <a href="/tags/TwoSum/" style="font-size: 10px;">TwoSum</a> <a href="/tags/Valine/" style="font-size: 10px;">Valine</a> <a href="/tags/Vapnik-Chervonenkis/" style="font-size: 10px;">Vapnik-Chervonenkis</a> <a href="/tags/binary/" style="font-size: 10px;">binary</a> <a href="/tags/blog/" style="font-size: 20px;">blog</a> <a href="/tags/classification/" style="font-size: 10px;">classification</a> <a href="/tags/comment/" style="font-size: 10px;">comment</a> <a href="/tags/hurrican-tracks/" style="font-size: 10px;">hurrican tracks</a> <a href="/tags/kNN/" style="font-size: 10px;">kNN</a> <a href="/tags/local-search/" style="font-size: 10px;">local search</a> <a href="/tags/music/" style="font-size: 10px;">music</a> <a href="/tags/storm-surge/" style="font-size: 10px;">storm surge</a> <a href="/tags/visitor-counts/" style="font-size: 10px;">visitor counts</a> <a href="/tags/尼采/" style="font-size: 10px;">尼采</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/03/07/BayesianNonparametrics/">Bayesian Nonparametrics Notes</a>
          </li>
        
          <li>
            <a href="/2019/02/19/GaussianProcess/">Why are Gaussian Process Models called Nonparametric?</a>
          </li>
        
          <li>
            <a href="/2019/01/29/Birthday/">Make a Birthday Cake in R</a>
          </li>
        
          <li>
            <a href="/2019/01/27/BlogTheme/">Blog Theme 日神 x 酒神</a>
          </li>
        
          <li>
            <a href="/2019/01/26/Hexo-View/">Add Article Views to Your Hexo Blog</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Qiuyi Wu<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>