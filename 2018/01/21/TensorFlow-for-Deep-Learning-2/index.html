<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  

  
  <title>TensorFlow for Deep Learning 2 | Qiuyi&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Here I’ll give the theory part of Neural Networks, sepecifically three kinds of NN: Normal Neural Networks, Convolutional Neural Networks, Recurrent Neural Networks.  Neural NetworksIn single neuron:$">
<meta name="keywords" content="CNN,RNN,TensorFlow,DNN">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorFlow for Deep Learning 2">
<meta property="og:url" content="https://qiuyiwu.github.io/2018/01/21/TensorFlow-for-Deep-Learning-2/index.html">
<meta property="og:site_name" content="Qiuyi&#39;s Blog">
<meta property="og:description" content="Here I’ll give the theory part of Neural Networks, sepecifically three kinds of NN: Normal Neural Networks, Convolutional Neural Networks, Recurrent Neural Networks.  Neural NetworksIn single neuron:$">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://qiuyiwu.github.io/images/2018/01/activation1.png">
<meta property="og:image" content="https://qiuyiwu.github.io/images/2018/01/activation2.png">
<meta property="og:image" content="https://qiuyiwu.github.io/images/2018/01/activation3.png">
<meta property="og:image" content="https://qiuyiwu.github.io/images/2018/01/activation4.png">
<meta property="og:image" content="https://qiuyiwu.github.io/images/2018/01/GD.png">
<meta property="og:image" content="https://qiuyiwu.github.io/images/2018/01/BP.png">
<meta property="og:image" content="https://qiuyiwu.github.io/images/2018/01/DNNCNN.png">
<meta property="og:image" content="https://qiuyiwu.github.io/images/2018/01/CNN12.png">
<meta property="og:image" content="https://qiuyiwu.github.io/images/2018/01/CNN34.png">
<meta property="og:image" content="https://qiuyiwu.github.io/images/2018/01/2D.png">
<meta property="og:image" content="https://qiuyiwu.github.io/images/2018/01/pooling.png">
<meta property="og:image" content="https://qiuyiwu.github.io/images/2018/01/diagram.png">
<meta property="og:image" content="https://qiuyiwu.github.io/images/2018/01/RNN.png">
<meta property="og:updated_time" content="2019-01-26T15:18:30.832Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TensorFlow for Deep Learning 2">
<meta name="twitter:description" content="Here I’ll give the theory part of Neural Networks, sepecifically three kinds of NN: Normal Neural Networks, Convolutional Neural Networks, Recurrent Neural Networks.  Neural NetworksIn single neuron:$">
<meta name="twitter:image" content="https://qiuyiwu.github.io/images/2018/01/activation1.png">
  
    <link rel="alternate" href="/atom.xml" title="Qiuyi&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
</html>
<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Qiuyi&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Researcher✨Qiuyi Wu</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://qiuyiwu.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-TensorFlow-for-Deep-Learning-2" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/21/TensorFlow-for-Deep-Learning-2/" class="article-date">
  <time datetime="2018-01-21T18:50:21.000Z" itemprop="datePublished">2018-01-21</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/TensorFlow/">TensorFlow</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      TensorFlow for Deep Learning 2
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Here I’ll give the theory part of Neural Networks, sepecifically three kinds of NN: Normal Neural Networks, Convolutional Neural Networks, Recurrent Neural Networks. </p>
<h1 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks"></a>Neural Networks</h1><h2 id="In-single-neuron"><a href="#In-single-neuron" class="headerlink" title="In single neuron:"></a>In single neuron:</h2><span>$$\begin{align*}
z &amp;= Wx + b \\
a &amp;= \sigma (z)
\end{align*}$$</span><!-- Has MathJax -->
<h2 id="Activation-Function"><a href="#Activation-Function" class="headerlink" title="Activation Function"></a>Activation Function</h2><ul>
<li><strong>Perceptron:</strong> binary classifier, small changes are not reflected.<span>$$\begin{align*}
f(x) = \begin{cases}
1 &amp; \text{if } Wx+b&gt;0\\ 
0 &amp; \text{otherwise}
\end{cases}
\end{align*}$$</span><!-- Has MathJax -->
</li>
</ul>
<a id="more"></a>
<center><br><img src="/images/2018/01/activation1.png" width="50%" height="50%"><br></center><br><em> <strong>Sigmoid:</strong> special case of the logistic function with S shape curve, more dynamic<br><span>$$\begin{align*}
S(x) = \frac{1}{1 + e^{-x}} = \frac{e^x}{e^x +1} 
\end{align*}$$</span><!-- Has MathJax --><br><center><br><img src="/images/2018/01/activation2.png" width="50%" height="50%"><br></center>
</em> <strong>Hyperbolic Tangent</strong>: Tanh(z)<br><span>$$\begin{align*}
cosh x &amp;= \frac{e^x + e^{-x}}{2}\\ 
sinh x &amp;= \frac{e^x - e^{-x}}{2}\\ 
tanh x &amp;= \frac{cosh x}{sinh x }
\end{align*}$$</span><!-- Has MathJax --><br><center><br><img src="/images/2018/01/activation3.png" width="50%" height="50%"><br></center>

<ul>
<li><p><strong>ReLU (Rectified Linear Unit):</strong> max(0,z)<br><center><br><img src="/images/2018/01/activation4.png" width="50%" height="50%"><br></center><br>ReLU and tanh tend to have the best performance.</p>
</li>
<li><p><strong>Softmax Regression:</strong> </p>
<span>$$\begin{align*}
z_i &amp;= \sum_jW_{i,j}x_j + b_i  \\
softmax(z)_i &amp;= \frac{\text{exp}(z_i)}{\sum_j \text{exp}(z_j)}
\end{align*}$$</span><!-- Has MathJax -->
</li>
</ul>
<h2 id="Cost-Loss-Function"><a href="#Cost-Loss-Function" class="headerlink" title="Cost/Loss Function"></a>Cost/Loss Function</h2><p>Cost function is the measurement of the error.<br><span>$$\begin{align*}
z &amp;= Wx + b \\
a &amp;= \sigma (z)\\
C &amp;= \frac{1}{n}\sum(y_{true} - a)^2 &amp;\text{Quadratic Cost}\\
C &amp;= -\frac{1}{n}\sum(y_{true}\cdot ln(a) + (1-y_{true})\cdot ln(1-a) ) &amp;\text{Cross Entropy}
\end{align*}$$</span><!-- Has MathJax --><br><strong>Quadratic Cost:</strong><br>The larger errors are more prominent due to the squaring. It causes a slowdown in learning speed.<br><strong>Cross Entropy:</strong><br>It allows for faster learning. The larger the difference, the faster the neuron can learn.</p>
<h2 id="Gradient-Descent-amp-Backpropagation"><a href="#Gradient-Descent-amp-Backpropagation" class="headerlink" title="Gradient Descent &amp; Backpropagation"></a>Gradient Descent &amp; Backpropagation</h2><p><strong>Gradient Descent</strong> is an optimization algorithm for finding the minimum of a function. Here it minimizes the error to find the optiml value. 1-D example below shows the best parameter value (weights of the neuron inputs) we should choose to minimize the cost.</p>
<center><br><img src="/images/2018/01/GD.png" width="30%" height="30%"><br></center><br>For complicated cases more than 1 dimension, we use biilt-in algebra of Deep learning library to get the optimal parameters.<br><br>1. <em>Learning Rate:</em> defines the step size during gradient descent, too small - slow pace, too small - overshooting<br>2. <em>Batch Size:</em> batches allow us to use stochastic gradient descent, in case the datasets are large, if all the them are fed at once the computation would be very expensive. Too small - less representative of data, too large - longer training time<br>3. <em>Second-Order Behavior of Gradient Descent:</em> adjust the learning rate based on the rate of descent(second-order behavior: derivative),large learning rate at the beginning, adjust to slower learning rate as it get closer. Methods: AdaGrad, RMSProp, Adam<br><br>Vanishing Gradients: when increasing the number of layers in a network, the layers towards the input will be affected less by the error calculation occuring at the output as going backwards throught the network. Initialization and Normalization will help to mitigate the issue.<br><br><strong>Backpropagation</strong> is to calculate the error contribution of each neuron after a batch of data is processed. It works by calculating the error at the output and then distributes back through the network layers. It belongs to supervised learning as it requires a known output for each input value. The mathematical detail is showed below from Andrew Ng’s Neural Networks and Deep Learning in Coursera.<br><center><br><img src="/images/2018/01/BP.png" width="70%" height="70%"><br></center>



<h2 id="Initialization-of-Weights"><a href="#Initialization-of-Weights" class="headerlink" title="Initialization of Weights"></a>Initialization of Weights</h2><h3 id="Zeros"><a href="#Zeros" class="headerlink" title="Zeros"></a>Zeros</h3><p>No randomness (too subjective) so not a good choice</p>
<h3 id="Random-Distribution"><a href="#Random-Distribution" class="headerlink" title="Random Distribution"></a>Random Distribution</h3><p>Random distribution near zero is not optimal and results in activaion function distorition (distorted to large values)</p>
<h3 id="Xavier-Glorot-Initialization"><a href="#Xavier-Glorot-Initialization" class="headerlink" title="Xavier (Glorot) Initialization"></a>Xavier (Glorot) Initialization</h3><p>The weights drawn from uniform or normal distribution, with <strong>zero mean</strong> and <strong>specific variance</strong> $\text{Var}(W) =\frac{1}{n_{in}} $:<br><span>$$\begin{align*}
&amp;Y = W_1X_1 +W_2X_2 + ... + W_nX_n\\
&amp;\text{Var}(W_iX_i) = E[X_i]^2\text{Var}(W_i) + E[W_i]^2\text{Var}(X_i) + \text{Var}(W_i)\text{Var}(X_i)\\ 
&amp;\text{Var}(W_iX_i) = \text{Var}(W_i)\text{Var}(X_i) \qquad (\because E[X_i] = 0)\\ 
&amp;\text{Var}(Y) = \text{Var}(W_1X_1 + W_2X_2 +... + W_nX_n)  = n  \text{Var}(W_i)\text{Var}(X_i) \\ 
&amp;\because \text{Variance of the output is equal to the variance of the input}\\ 
&amp;\therefore n\text{Var}(W_i) = 1\\
&amp;\therefore \text{Var}(W_i) = \frac{1}{n} =  \frac{1}{n_{in}}  = \frac{2}{n_{in} + n_{out} }
\end{align*}$$</span><!-- Has MathJax --></p>
<h2 id="Overfitting-Issue"><a href="#Overfitting-Issue" class="headerlink" title="Overfitting Issue"></a>Overfitting Issue</h2><p>With potentially hundreds of parameters in a deep learning neural network, the possibility of overfitting is very high. We can mitigate this issue by the following ways:</p>
<ul>
<li><strong>$L_1/L_2$ Regularization</strong><br>Add a penalty for a larger weights in the model (not unique to neural networks)</li>
<li><strong>Dropout</strong><br>Remove neurons during training randomly so that the network does not over rely on any particular neuron (unique to neural networks)</li>
<li><strong>Expanding Data</strong><br>Artificially expand data via adding noise, tilting images </li>
</ul>
<h1 id="Convolutional-Neural-Networks"><a href="#Convolutional-Neural-Networks" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h1><h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h2><p><strong>Tensor:</strong> N-dimensional arrays:<br>Scalar: 3<br>Vector: [3,4,5]<br>Matrix: [[3,4],[5,6],[7,8]]<br>Tensor: [[[1,2], [3,4]],  [[5,6], [7,8]]]<br>We use tensors to feed in sets of images into the model - (I,H,W,C)<br>I: Images<br>H: Height of Image in Pixels<br>W: Width of Image in Pixels<br>C: Color Channels: 1 - Grayscale, 3 - RGB</p>
<h2 id="DNN-vs-CNN"><a href="#DNN-vs-CNN" class="headerlink" title="DNN vs CNN"></a>DNN vs CNN</h2><h3 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h3><p>The left figure is <strong>densely connected layer</strong>, every neuron in the layer is directly connected to the every neuron in the next layer. While each unit in the <strong>convolutional layer</strong> is connected to a smaller number of nearby units in the next layer. </p>
<center><br><img class="left" src="/images/2018/01/DNNCNN.png" width="60%" height="50%"><br></center><br>The reason for the idea of CNN is that most images are at least 256 by 256 pixels or greater (MNIST only 28 by 29 pixels, 784 total). So there are too many parameters unscalable to new images. Another merit of CNN is for image processing, pixels nearby to each other are much more correlated to each other for image detection. Each CNN layer looks at an increasingly larger part of the image. And having units only connected to nearby units helps invariance. CNN helps limit the search of weights to the size of the convolution. <em>Convolutional layers are only connected to pixels in their respective fields.</em> By <strong>adding a padding of zeros</strong> around the image we can fix the issue for edge neurons where there may not be an input for them.<br>Take the example of 1-D Convolution, we treat the weights as a filter for edge detection, then expand one filters to multiple filters. Stride 1 means 1 unit at a time. The top circle means the zero padding added to include more edge pixels. Each filter detects a different feature.<br><center><br><img class="left" src="/images/2018/01/CNN12.png" width="70%" height="60%"><br></center><br>Now for simplicity, the sets of neurons are visualized as blocks.<br><center><br><img class="left" src="/images/2018/01/CNN34.png" width="80%" height="70%"><br></center><br>For 2-D Images and Color Images:<br><center><br><img class="left" src="/images/2018/01/2D.png" width="80%" height="70%"><br></center>

<p>More Info: <a href="http://setosa.io/ev/image-kernels/" target="_blank" rel="noopener">Image Kernals</a></p>
<h3 id="Subsampling"><a href="#Subsampling" class="headerlink" title="Subsampling"></a>Subsampling</h3><p>Except <strong>convolutional layers</strong>, there’s another kind of layers called <strong>pooling layers</strong>. Pooling layers will subsample the input image to reduce the memory use and computer load as well as reducing the number of parameters.<br>Take example of MNIST, only select max value to the next layer, and move over by stride. So the pooling layer will finally remove a lot of information.</p>
<center><br><img class="left" src="/images/2018/01/pooling.png" width="60%" height="50%"><br></center>

<p>Another technique is “Dropout”. Dropout is regarded as a form of regularization as during training, units are randomly dropped with their connection to help prevent overfitting. </p>
<center><br><img class="left" src="/images/2018/01/diagram.png" width="80%" height="70%"><br></center>


<h1 id="Recurrent-Neural-Networks"><a href="#Recurrent-Neural-Networks" class="headerlink" title="Recurrent Neural Networks"></a>Recurrent Neural Networks</h1><p>Common Neural Networks can handle classification and regression problems, but for sequence information, we need Recurrent Neural Networks.<br>Normal Neural Networks just aggregation of inputs and pass the activation function to get the output. Recurrent Neural Networks send output back to itself.<br>Here I have to mention my previous research work – Echo State Networks, also belong to RNN: <a href="https://qiuyiwu.github.io/ESN/"><strong>ESN</strong></a> </p>
<p><center><br><img class="left" src="/images/2018/01/RNN.png" width="90%" height="80%"><br></center><br>Cells that are a function of inputs from previous time steps are also know as memory cells. </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qiuyiwu.github.io/2018/01/21/TensorFlow-for-Deep-Learning-2/" data-id="cjt39xdm4001hxly5449opwsa" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CNN/">CNN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DNN/">DNN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RNN/">RNN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/TensorFlow/">TensorFlow</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/01/22/TensorFlow-for-Deep-Learning-3/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          TensorFlow for Deep Learning 3
        
      </div>
    </a>
  
  
    <a href="/2018/01/16/Tensorflow-for-Deep-Learning-1/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">TensorFlow for Deep Learning 1</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Astrostatistics/">Astrostatistics</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Mining/">Data Mining</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/JUJUs/">JUJUs</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/LeetCode/">LeetCode</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Philosophy/">Philosophy</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Research/">Research</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Statistics/">Statistics</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/TensorFlow/">TensorFlow</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/API/">API</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Astronomy/">Astronomy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Astrostatistics/">Astrostatistics</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Birthday/">Birthday</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/">CNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Chinese-Restaurant/">Chinese Restaurant</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Classification/">Classification</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DNN/">DNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Fusion/">Data Fusion</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Mining/">Data Mining</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deep-Learning/">Deep Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Disqus/">Disqus</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Distance/">Distance</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ensemble/">Ensemble</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Firebase/">Firebase</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Firestore/">Firestore</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gaussian-Process/">Gaussian Process</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/">Hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Indian-Buffet/">Indian Buffet</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KMeans/">KMeans</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KNN/">KNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LeanCloud/">LeanCloud</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Network-Analysis/">Network Analysis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neural-Network/">Neural Network</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Nietzsche/">Nietzsche</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Nonparametric/">Nonparametric</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Philosophy/">Philosophy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/R/">R</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/">RNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Regression/">Regression</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/">SVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spatial-Statistics/">Spatial Statistics</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Statistics/">Statistics</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TensorFlow/">TensorFlow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Topic-Modeling/">Topic Modeling</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TwoSum/">TwoSum</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Valine/">Valine</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Vapnik-Chervonenkis/">Vapnik-Chervonenkis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/binary/">binary</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/blog/">blog</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/classification/">classification</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/comment/">comment</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hurrican-tracks/">hurrican tracks</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kNN/">kNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/local-search/">local search</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/music/">music</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/storm-surge/">storm surge</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/visitor-counts/">visitor counts</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/尼采/">尼采</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/API/" style="font-size: 10px;">API</a> <a href="/tags/Astronomy/" style="font-size: 10px;">Astronomy</a> <a href="/tags/Astrostatistics/" style="font-size: 10px;">Astrostatistics</a> <a href="/tags/Birthday/" style="font-size: 10px;">Birthday</a> <a href="/tags/CNN/" style="font-size: 15px;">CNN</a> <a href="/tags/Chinese-Restaurant/" style="font-size: 10px;">Chinese Restaurant</a> <a href="/tags/Classification/" style="font-size: 10px;">Classification</a> <a href="/tags/DNN/" style="font-size: 10px;">DNN</a> <a href="/tags/Data-Fusion/" style="font-size: 10px;">Data Fusion</a> <a href="/tags/Data-Mining/" style="font-size: 10px;">Data Mining</a> <a href="/tags/Deep-Learning/" style="font-size: 10px;">Deep Learning</a> <a href="/tags/Disqus/" style="font-size: 10px;">Disqus</a> <a href="/tags/Distance/" style="font-size: 10px;">Distance</a> <a href="/tags/Ensemble/" style="font-size: 10px;">Ensemble</a> <a href="/tags/Firebase/" style="font-size: 10px;">Firebase</a> <a href="/tags/Firestore/" style="font-size: 10px;">Firestore</a> <a href="/tags/Gaussian-Process/" style="font-size: 10px;">Gaussian Process</a> <a href="/tags/Hexo/" style="font-size: 20px;">Hexo</a> <a href="/tags/Indian-Buffet/" style="font-size: 10px;">Indian Buffet</a> <a href="/tags/KMeans/" style="font-size: 10px;">KMeans</a> <a href="/tags/KNN/" style="font-size: 10px;">KNN</a> <a href="/tags/LeanCloud/" style="font-size: 10px;">LeanCloud</a> <a href="/tags/Network-Analysis/" style="font-size: 10px;">Network Analysis</a> <a href="/tags/Neural-Network/" style="font-size: 10px;">Neural Network</a> <a href="/tags/Nietzsche/" style="font-size: 10px;">Nietzsche</a> <a href="/tags/Nonparametric/" style="font-size: 15px;">Nonparametric</a> <a href="/tags/Philosophy/" style="font-size: 10px;">Philosophy</a> <a href="/tags/Python/" style="font-size: 10px;">Python</a> <a href="/tags/R/" style="font-size: 10px;">R</a> <a href="/tags/RNN/" style="font-size: 15px;">RNN</a> <a href="/tags/Regression/" style="font-size: 10px;">Regression</a> <a href="/tags/SVM/" style="font-size: 10px;">SVM</a> <a href="/tags/Spatial-Statistics/" style="font-size: 10px;">Spatial Statistics</a> <a href="/tags/Statistics/" style="font-size: 20px;">Statistics</a> <a href="/tags/TensorFlow/" style="font-size: 15px;">TensorFlow</a> <a href="/tags/Topic-Modeling/" style="font-size: 15px;">Topic Modeling</a> <a href="/tags/TwoSum/" style="font-size: 10px;">TwoSum</a> <a href="/tags/Valine/" style="font-size: 10px;">Valine</a> <a href="/tags/Vapnik-Chervonenkis/" style="font-size: 10px;">Vapnik-Chervonenkis</a> <a href="/tags/binary/" style="font-size: 10px;">binary</a> <a href="/tags/blog/" style="font-size: 20px;">blog</a> <a href="/tags/classification/" style="font-size: 10px;">classification</a> <a href="/tags/comment/" style="font-size: 10px;">comment</a> <a href="/tags/hurrican-tracks/" style="font-size: 10px;">hurrican tracks</a> <a href="/tags/kNN/" style="font-size: 10px;">kNN</a> <a href="/tags/local-search/" style="font-size: 10px;">local search</a> <a href="/tags/music/" style="font-size: 10px;">music</a> <a href="/tags/storm-surge/" style="font-size: 10px;">storm surge</a> <a href="/tags/visitor-counts/" style="font-size: 10px;">visitor counts</a> <a href="/tags/尼采/" style="font-size: 10px;">尼采</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/03/07/BayesianNonparametrics/">Bayesian Nonparametrics Notes</a>
          </li>
        
          <li>
            <a href="/2019/02/19/GaussianProcess/">Why are Gaussian Process Models called Nonparametric?</a>
          </li>
        
          <li>
            <a href="/2019/01/29/Birthday/">Make a Birthday Cake in R</a>
          </li>
        
          <li>
            <a href="/2019/01/27/BlogTheme/">Blog Theme 日神 x 酒神</a>
          </li>
        
          <li>
            <a href="/2019/01/26/Hexo-View/">Add Article Views to Your Hexo Blog</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Qiuyi Wu<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>