<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  

  
  <title>Data Mining Note 3 - K Nearest Neighbors | Qiuyi&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="This chapter talks about k Nearest Neighbors (kNN) as a metholodogy for both classification and regression problems. The kNN method serves a basic and easy to understand foundational machine learning">
<meta name="keywords" content="kNN,Classification,Regression,Distance">
<meta property="og:type" content="article">
<meta property="og:title" content="Data Mining Note 3 - K Nearest Neighbors">
<meta property="og:url" content="https://qiuyiwu.github.io/2018/01/06/Data-Mining-Note3/index.html">
<meta property="og:site_name" content="Qiuyi&#39;s Blog">
<meta property="og:description" content="This chapter talks about k Nearest Neighbors (kNN) as a metholodogy for both classification and regression problems. The kNN method serves a basic and easy to understand foundational machine learning">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2019-01-26T15:12:41.360Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Data Mining Note 3 - K Nearest Neighbors">
<meta name="twitter:description" content="This chapter talks about k Nearest Neighbors (kNN) as a metholodogy for both classification and regression problems. The kNN method serves a basic and easy to understand foundational machine learning">
  
    <link rel="alternate" href="/atom.xml" title="Qiuyi&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
</html>
<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Qiuyi&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Researcher✨Qiuyi Wu</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://qiuyiwu.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Data-Mining-Note3" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/01/06/Data-Mining-Note3/" class="article-date">
  <time datetime="2018-01-06T16:37:58.000Z" itemprop="datePublished">2018-01-06</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Data-Mining/">Data Mining</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Data Mining Note 3 - K Nearest Neighbors
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>This chapter talks about k Nearest Neighbors (kNN) as a metholodogy for both classification and regression problems. The kNN method serves a basic and easy to understand foundational machine learning and data mining technique. The kNN method is an excellent baseline machine learning technique, and also allows many extensions. It usually performs reasonable well or sometimes very well when compared to more sophisticated techniques.</p>
<p>kNN classification basically means the estimated class of a vector $\mathbf{x}$ is the most frequent class label in the neighborhood of $\mathbf{x}$. kNN classifiers are inherently naturally multi-class, and are used extensively in applications such as image processing, character recognition and general pattern recognition tasks. For kNN regression, the estimated response value of a vector $\mathbf{x}$ is the average of the response values in the neighborhood of $\mathbf{x}$.<br><a id="more"></a></p>
<p><strong>Principle for kNN Classification</strong>: The reasonable class/category for a given object is the most prevalent class among its nearest neighbors.<br><strong>Principle k-Nearest Neighbor Regression</strong>: The reasonable prediction of the response value for a given object is the average of the response values of its nearest neighbors.</p>
<p><strong>General Steps for kNN</strong><br>Comparison of Classification and Regression</p>
<ol>
<li>Choose a distance for measuring how far a given point is from another</li>
<li>Set the <strong>size</strong> of the neighborhood k</li>
<li>Compute the distance from each existing point to the new point</li>
<li>Identify the class labels of the k points closest/nearest to the new point (classification)<br>Identify the response values of the k points closest/nearest to the new point (regression)</li>
<li>Assign the most frequent label to the new point (classification)<br>Compute the average of the response values of those k neighbors as the best estimate of the new point (regression)</li>
</ol>
<p><br></p>
<h3 id="Distance"><a href="#Distance" class="headerlink" title="Distance"></a>Distance</h3><p>First introduce some most commonly used distance below, which would be applied later in kNN.</p>
<ul>
<li><p>Euclidean distance: also known as the $l_2$ distance</p>
<span>$$\begin{align*}
d(\mathbf{x}_i,\mathbf{x}_j)  = \sqrt{\sum_{l=1}^q(x_{il}-x_{jl})^2} = ||\mathbf{x}_i-\mathbf{x}_j||_2 
\end{align*}$$</span><!-- Has MathJax -->
</li>
<li><p>Manhattan distance (city block): also known as $l_2$ distance</p>
<span>$$\begin{align*}
d(\mathbf{x}_i,\mathbf{x}_j)  = \sum_{l=1}^q|x_{il}-x_{jl}| = ||\mathbf{x}_i-\mathbf{x}_j||_1 
\end{align*}$$</span><!-- Has MathJax -->
</li>
<li><p>Maximum distance: also known as the infinity distance</p>
<span>$$\begin{align*}
d(\mathbf{x}_i,\mathbf{x}_j)  = \underset{l=1,...,q}{\text{max}}|x_{il}-x_{jl}| = ||\mathbf{x}_i-\mathbf{x}_j||_\infty
\end{align*}$$</span><!-- Has MathJax -->
</li>
<li><p>Minkowski distance: also known as $l_p$ distance</p>
<span>$$\begin{align*}
d(\mathbf{x}_i,\mathbf{x}_j)  = \Big\{\sum_{l=1}^q|x_{il}-x_{jl}| \Big\}^{1/p}
\end{align*}$$</span><!-- Has MathJax -->
</li>
<li><p>Canberra distance:</p>
<span>$$\begin{align*}
d(\mathbf{x}_i,\mathbf{x}_j)  =\sum_{l=1}^q \frac{ |x_{il}-x_{jl}| }{ |x_{il}+x_{jl}| }  
\end{align*}$$</span><!-- Has MathJax -->
</li>
<li><p>Jaccard/Tanimoto distance: For binary vectors ie $\mathbf{x}_i\in {0,1}^q$</p>
<span>$$\begin{align*}
d(\mathbf{x}_i,\mathbf{x}_j)  =1- \frac{ \mathbf{x}_i\cdot \mathbf{x}_j }{ |\mathbf{x}_i|^2 + |\mathbf{x}_j|^2  - \mathbf{x}_i\cdot \mathbf{x}_j}  
\end{align*}$$</span><!-- Has MathJax -->
</li>
</ul>
<p><br></p>
<h3 id="kNN-Classification"><a href="#kNN-Classification" class="headerlink" title="kNN Classification"></a>kNN Classification</h3><h4 id="Detailed-Steps-for-kNN-Classification"><a href="#Detailed-Steps-for-kNN-Classification" class="headerlink" title="Detailed Steps for kNN Classification"></a>Detailed Steps for kNN Classification</h4><p>$\mathcal{D} = \Big\{(x_1, Y_1),…, (x_n, Y_n) \Big\}$, with $x_i\in \mathcal{X}^q, Y_i\in \{1,…,g\}$</p>
<ol>
<li>Choose the value of k and the distance to be used</li>
<li>Let $\mathbf{x}^*$ be a new point. Compute  $d(\mathbf{x}^*,\mathbf{x}_i)  \quad i=1,2,…n $</li>
<li>Rank all the distances $d^∗_i$ in increasing order: <span>$$\begin{align*}
 d^&lowast;_{(1)} \leq  d^&lowast;_{(2)} \leq ...\leq  d^&lowast;_{(k)} \leq  d^&lowast;_{(k+1)} \leq ... d^&lowast;_{(n)} 
\end{align*}$$</span><!-- Has MathJax --></li>
<li>Form $\mathcal{V}_k(\mathbf{x}^∗)$, the k-Neighborhood of $\mathbf{x}^∗$<span>$$\begin{align*}
 \mathcal{V}_k(\mathbf{x}^&lowast;) = \Big\{ \mathbf{x}_i:  d(\mathbf{x}^*,\mathbf{x}_i)\leq d^*_{(k)}  \Big\}
\end{align*}$$</span><!-- Has MathJax --></li>
<li>Compute the predicted response $\hat{Y}^*$ as <span>$$\begin{align*}
\hat{Y}^*_{kNN} &amp;= \text{Most frequent label in } \mathcal{V}_k(\mathbf{x}^&lowast;) \\ 
 &amp;=  \hat{f}^*_{kNN}(\mathbf{x}^*) = \underset{j\in\{1,...,g\}}{\texttt{argmax}} \Big\{  p_j^{(k)}(\mathbf{x}^*)  \Big\}\\ 
\text{where }  p_j^{(k)}(\mathbf{x}^*)= \frac{1}{k}\sum_{\mathbf{x}^* \in  \mathcal{V}_k(\mathbf{x}^&lowast;) } I(Y_i=j) &amp;
\text{ estimates the probability that } \mathbf{x}^* \text{ belongs to class j based on} \mathcal{V}_k(\mathbf{x}^&lowast;)
\end{align*}$$</span><!-- Has MathJax --></li>
</ol>
<ul>
<li>Note: <strong>[Posterior probability estimate]</strong> $ \frac{1}{k}\sum_{\mathbf{x}^* \in  \mathcal{V}_k(\mathbf{x}^∗) } I(Y_i=j) $ can be regarded as a rough estimate of $ \pi_j (\mathbf{x}^*) = Pr[Y^*=j| \mathbf{x}^* ] $ , the posterior probability of class membership of $ \mathbf{x}^* $</li>
</ul>
<h4 id="Comments"><a href="#Comments" class="headerlink" title="Comments:"></a>Comments:</h4><ul>
<li>kNearest Neighbors (kNN) essentially performs classification by voting for the most popular response among the k nearest neighbors of $\mathbf{x}^*$.</li>
<li>kNN provides the most basic form of nonparametric classification.</li>
<li>Since the fundamental building block of kNN is the distance measure, one can easily perform classification beyond the traditional setting where the predictors are numeric. For instance, classification with kNN can be readily performed on indicator attributes<br>$$  \mathbf{x}^* = (x_{i1},…, x_{ip}  )^T \in \{ 0,1\}^p $$</li>
<li>kNN classifiers are inherently naturally <strong>multi-class</strong>, and are used in many applications.</li>
</ul>
<p><br></p>
<h3 id="kNN-Regression"><a href="#kNN-Regression" class="headerlink" title="kNN Regression"></a>kNN Regression</h3><h4 id="Detailed-Steps-for-kNN-Regression"><a href="#Detailed-Steps-for-kNN-Regression" class="headerlink" title="Detailed Steps for kNN Regression"></a>Detailed Steps for kNN Regression</h4><p>$\mathcal{D} = \Big\{(x_1, Y_1),…, (x_n, Y_n) \Big\}$, with $x_i\in \mathcal{X}^q, Y_i\in \mathbb{R}$</p>
<ol>
<li>Choose the value of k and the distance to be used</li>
<li>Let $\mathbf{x}^*$ be a new point. Compute  $d(\mathbf{x}^*,\mathbf{x}_i)  \quad i=1,2,…n $</li>
<li>Rank all the distances $d^∗_i$ in increasing order: <span>$$\begin{align*}
 d^&lowast;_{(1)} \leq  d^&lowast;_{(2)} \leq ...\leq  d^&lowast;_{(k)} \leq  d^&lowast;_{(k+1)} \leq ... d^&lowast;_{(n)} 
\end{align*}$$</span><!-- Has MathJax --></li>
<li>Form $\mathcal{V}_k(\mathbf{x}^∗)$, the k-Neighborhood of $\mathbf{x}^∗$<span>$$\begin{align*}
 \mathcal{V}_k(\mathbf{x}^&lowast;) = \Big\{ \mathbf{x}_i:  d(\mathbf{x}^*,\mathbf{x}_i)\leq d^*_{(k)}  \Big\}
\end{align*}$$</span><!-- Has MathJax --></li>
<li>Compute the predicted response $\hat{Y}^*$ as <span>$$\begin{align*}
\hat{Y}^*_{kNN} &amp;=  \hat{f}^*_{kNN}(\mathbf{x}^*)\\
 &amp;= \frac{1}{k}\sum_{\mathbf{x}^* \in  \mathcal{V}_k(\mathbf{x}^&lowast;) } Y_i \\ 
 &amp;=  \frac{1}{k}\sum_{i=1}^n Y_i I( \mathbf{x}^* \in \mathcal{V}_k(\mathbf{x}^&lowast;) )
\end{align*}$$</span><!-- Has MathJax -->
</li>
</ol>
<h4 id="Comments-1"><a href="#Comments-1" class="headerlink" title="Comments:"></a>Comments:</h4><ul>
<li>kNearest Neighbors (kNN) essentially performs regression by averaging the responses of the nearest neighbors of $\mathbf{x}^*$.</li>
<li>kNN provides the most basic form of nonparametric regression</li>
<li>Since the fundamental building block of kNN is the distance measure, one can easily perform regression beyond the traditional setting where the predictors are numeric. For instance, Regression vectors of binary with kNN can be readily performed on indicator attributes<br>$$  \mathbf{x}^* = (x_{i1},…, x_{ip}  )^T \in \{ 0,1\}^p $$ </li>
<li>kNN somewhat performs smoothing (filtering).</li>
<li>The estimated response kNN for $\mathbf{x}^*$ is estimator of the average response which is the <strong>conditional expectation of Y given $\mathbf{x}^*$</strong><br>$$ \hat{Y}^*_{kNN} =\mathbb{E}\widehat{[Y^*|\mathbf{x}^*]}  $$</li>
</ul>
<p><br></p>
<h3 id="Basic-kNN-amp-Weighted-kNN"><a href="#Basic-kNN-amp-Weighted-kNN" class="headerlink" title="Basic kNN &amp; Weighted kNN"></a>Basic kNN &amp; Weighted kNN</h3><h4 id="Limitation-of-Basic-kNN"><a href="#Limitation-of-Basic-kNN" class="headerlink" title="Limitation of Basic kNN"></a>Limitation of Basic kNN</h4><ul>
<li><p><strong>Equidistance</strong>: All neighbors are given the same contribution to the estimate of the response; In the estimated probability</p>
<span>$$\begin{align*}
 p_j^{(k)}(\mathbf{x}^*) &amp; = \frac{1}{k}\sum_{\mathbf{x}^* \in  \mathcal{V}_k(\mathbf{x}^&lowast;) } I(Y_i=j) =\sum_{\mathbf{x}^* \in  \mathcal{V}_k(\mathbf{x}^&lowast;) } w_i I(Y_i=j) \\
 \text{the weight } w_i = \frac{1}{k}=\texttt{constant }&amp;\text{for all points in }  \mathcal{V}_k(\mathbf{x}^&lowast;) \text{ regardlessly
of how far they are from } \mathbf{x}^&lowast;
\end{align*}$$</span><!-- Has MathJax -->
</li>
<li><p><strong>No model, weak interpretability</strong>: There is no underlying model, therefore no interpretation of the response relative to the predictor variables. There is no training set, since all happens at prediction. For this reason, kNN is referred to as <strong>lazy method</strong>.</p>
</li>
<li><p><strong>Computationally intensive</strong>: Predictions are computationally very intensive, due to the fact that for each new observation, the whole dataset must be traversed to compute the response</p>
</li>
</ul>
<p><br></p>
<h4 id="Extension-Weighted-kNN"><a href="#Extension-Weighted-kNN" class="headerlink" title="Extension: Weighted kNN"></a>Extension: Weighted kNN</h4><p>kNN classification can be improved by weighting the votes as a function of the distance from $\mathbf{x}^∗$. The weights are defined so as to preserve convexity $ \sum_{i=1}^k w_i = 1$. Some of the common weighting schemes include:</p>
<ul>
<li>Exponential Decay:<span>$$\begin{align*}
w_i = \frac{e^{-d_i^*}}{\sum_{l=1}^k e^{-d_l^*}  } 
\end{align*}$$</span><!-- Has MathJax --></li>
<li>Inverse Distance:<span>$$\begin{align*}
w_i = \frac{ \frac{1}{1+d_i^*}}{\sum_{l=1}^k  \frac{1}{1+d_l^*} } 
\end{align*}$$</span><!-- Has MathJax -->
</li>
</ul>
<p><br></p>
<h3 id="Effect-of-kNN"><a href="#Effect-of-kNN" class="headerlink" title="Effect of kNN"></a>Effect of kNN</h3><h4 id="Effect-of-k"><a href="#Effect-of-k" class="headerlink" title="Effect of k"></a>Effect of k</h4><ul>
<li><p>k controls the complexity of the underlying classifier, with small k yielding very complex classifiers and large k yielding rather simple ones.</p>
</li>
<li><p>If k is <strong>small</strong>, the estimated class is determined based on very few neighbors, the resulting kNN classifier will have very <strong>low bias</strong>, but very <strong>high variance</strong>. Take the example of k=1, the decision boundary will perfectly separate the classes on the training set, but will perform poorly on the test set.</p>
</li>
<li><p>If k is <strong>large</strong>, the estimated class is determined based on very many neighbors from far and wide, the resulting kNN classifier will<br>have very <strong>large bias</strong>, but very <strong>low variance</strong>. In fact, for truly large k, the decision boundary will be a constant hyperplane.</p>
</li>
<li><p>The determination of an optimal k desires the trade-off between bias and variance:<br>Determine k by cross validation<br>Determine k by direct minimization of the estimated prediction error via a suitably chosen test set</p>
<h4 id="Effect-of-n"><a href="#Effect-of-n" class="headerlink" title="Effect of n"></a>Effect of n</h4><p>As stated before, kNN is a lazy method, everything happens at prediction step. Thus the sample size n plays a crucial role, and <strong>large n</strong> would lead to <strong>intense</strong> prediction.</p>
<h4 id="Effect-of-p"><a href="#Effect-of-p" class="headerlink" title="Effect of p"></a>Effect of p</h4><p>The dimensionality p of the input space is <strong>only</strong> felt by the function that computes the <strong>distances</strong>. If the function is optimized, kNN should be <strong>unaffected</strong> by this dimensionality</p>
<h4 id="Effect-of-distance"><a href="#Effect-of-distance" class="headerlink" title="Effect of distance"></a>Effect of distance</h4><p>Some distances are more robust to extreme observations.</p>
</li>
</ul>
<h3 id="Pros-amp-Cons-of-kNN"><a href="#Pros-amp-Cons-of-kNN" class="headerlink" title="Pros &amp; Cons of kNN"></a>Pros &amp; Cons of kNN</h3><h4 id="Strength"><a href="#Strength" class="headerlink" title="Strength"></a>Strength</h4><ul>
<li>The kNN method is intuitively appealing and very easy to understand, explain, program/code and interpret</li>
<li>The kNN method provides a decent estimate of $Pr[Y = j|x]$, the posterior probability of class membership</li>
<li><span style="color:red"> <em>The kNN method easily handles missing values (by restricting distance calculations to subspace)</em></span></li>
<li><span style="color:red"> <em>As the number of training samples grows larger, the asymptotic misclassification error rate is bounded by twice the Bayes risk.</em></span><br>$$ \underset{n\rightarrow \infty}{lim} R(\hat{f}_n^{(kNN)}) \leq 2R^* $$</li>
<li><span style="color:red"> <em>The kNN method is naturally suitable for sequential/incremental machine learning.</em></span></li>
<li><span style="color:red"> <em>The kNN method is also suitable where the hypothesis space is variable in size.</em></span></li>
<li>The kNN method can handle <strong>non-numeric</strong> data as long as the <strong>distance</strong> can be defined.</li>
<li>The kNN methods can handle <strong>mixed types</strong> of data as long as the <strong>distance</strong> are computed as <strong>hybrid or combinations</strong>.</li>
<li>The kNN method is inherently multi-class. This is very important because for some other methods, going beyond binary classification requires some sophisticated mathematics. It also handles very flexible decision boundaries.</li>
</ul>
<h4 id="Weakness"><a href="#Weakness" class="headerlink" title="Weakness"></a>Weakness</h4><ul>
<li>The <strong>computational complexity</strong> of kNN is very high in prediction. Specifically, it is $\mathcal{O}(nmp)$ where n is the training set size, m is the test set size and p is the number of predictor variables. This means that kNN requires <strong>large amount of memory</strong>, and therefore does NOT scale well. This <strong>failure in scalability</strong> is addressed using various heuristics and strategies.</li>
<li>kNN methods <strong>suffer from the Curse of Dimensionality (COD)</strong>. When p is large and n is small (short fat data/ dimension of the space very high), the concept of nearness becomes meaningless to the point of being ill-defined, because the ”neighborhood” becomes very large. Thus, the nearest neighbor could be very far when the space is high dimensional and there are very few observations.</li>
<li>The kNN method does not yield a model, and therefore no parameter to help explain why the method performs as it does.</li>
<li>The kNN method is heavily affected by the local structure, and it is very sensitive to both irrelevant and correlated features. Unless the distance is well chosen and properly calibrated, kNN methods will be sensitive to outliers and all sorts of noise in the data.</li>
<li>Unless the distance is used in some way to weight the neighbors, more <strong>frequent classes will dominate</strong> in the determination of the estimated label. This means one has to be careful with kNN when one class has a far larger proportion of observations than the others.</li>
<li>The measurement scale of each variable affect the kNN method more than most methods. (measurement scale: standardizing, unitizing or cubizing/squeezing the data).</li>
</ul>
<p><br></p>
<h3 id="Application-of-kNN"><a href="#Application-of-kNN" class="headerlink" title="Application of kNN"></a>Application of kNN</h3><ul>
<li><strong>Handwritten Digit Recognition</strong> is usually the first task in some Data Analytics competitions.</li>
<li><strong>Text Mining</strong> and specific topic of text categorization/classification has made successful use of kNearest Neighbors approach</li>
<li><strong>Credit Scoring</strong> is another application that has been connected with k Nearest Neighbors Classification</li>
<li><strong>Disease Diagnostics</strong> also has been tackled using k Nearest Neighbors Classifiers</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://qiuyiwu.github.io/2018/01/06/Data-Mining-Note3/" data-id="cjt38hpbi000fldy52rf7ljnf" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Classification/">Classification</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Distance/">Distance</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Regression/">Regression</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kNN/">kNN</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/01/09/LeetCode1/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          LeetCode in Python3 - Two Sum
        
      </div>
    </a>
  
  
    <a href="/2018/01/05/Data-Mining-Note2/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Data Mining Note 2 - Binary Classification</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Astrostatistics/">Astrostatistics</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Mining/">Data Mining</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/JUJUs/">JUJUs</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/LeetCode/">LeetCode</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Philosophy/">Philosophy</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Research/">Research</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Statistics/">Statistics</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/TensorFlow/">TensorFlow</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/API/">API</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Astronomy/">Astronomy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Astrostatistics/">Astrostatistics</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Birthday/">Birthday</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/">CNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Chinese-Restaurant/">Chinese Restaurant</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Classification/">Classification</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DNN/">DNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Fusion/">Data Fusion</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Mining/">Data Mining</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deep-Learning/">Deep Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Disqus/">Disqus</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Distance/">Distance</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ensemble/">Ensemble</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Firebase/">Firebase</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Firestore/">Firestore</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gaussian-Process/">Gaussian Process</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/">Hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Indian-Buffet/">Indian Buffet</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KMeans/">KMeans</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KNN/">KNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LeanCloud/">LeanCloud</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Network-Analysis/">Network Analysis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neural-Network/">Neural Network</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Nietzsche/">Nietzsche</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Nonparametric/">Nonparametric</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Philosophy/">Philosophy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/R/">R</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/">RNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Regression/">Regression</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/">SVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spatial-Statistics/">Spatial Statistics</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Statistics/">Statistics</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TensorFlow/">TensorFlow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Topic-Modeling/">Topic Modeling</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TwoSum/">TwoSum</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Valine/">Valine</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Vapnik-Chervonenkis/">Vapnik-Chervonenkis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/binary/">binary</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/blog/">blog</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/classification/">classification</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/comment/">comment</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hurrican-tracks/">hurrican tracks</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kNN/">kNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/local-search/">local search</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/music/">music</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/storm-surge/">storm surge</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/visitor-counts/">visitor counts</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/尼采/">尼采</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/API/" style="font-size: 10px;">API</a> <a href="/tags/Astronomy/" style="font-size: 10px;">Astronomy</a> <a href="/tags/Astrostatistics/" style="font-size: 10px;">Astrostatistics</a> <a href="/tags/Birthday/" style="font-size: 10px;">Birthday</a> <a href="/tags/CNN/" style="font-size: 15px;">CNN</a> <a href="/tags/Chinese-Restaurant/" style="font-size: 10px;">Chinese Restaurant</a> <a href="/tags/Classification/" style="font-size: 10px;">Classification</a> <a href="/tags/DNN/" style="font-size: 10px;">DNN</a> <a href="/tags/Data-Fusion/" style="font-size: 10px;">Data Fusion</a> <a href="/tags/Data-Mining/" style="font-size: 10px;">Data Mining</a> <a href="/tags/Deep-Learning/" style="font-size: 10px;">Deep Learning</a> <a href="/tags/Disqus/" style="font-size: 10px;">Disqus</a> <a href="/tags/Distance/" style="font-size: 10px;">Distance</a> <a href="/tags/Ensemble/" style="font-size: 10px;">Ensemble</a> <a href="/tags/Firebase/" style="font-size: 10px;">Firebase</a> <a href="/tags/Firestore/" style="font-size: 10px;">Firestore</a> <a href="/tags/Gaussian-Process/" style="font-size: 10px;">Gaussian Process</a> <a href="/tags/Hexo/" style="font-size: 20px;">Hexo</a> <a href="/tags/Indian-Buffet/" style="font-size: 10px;">Indian Buffet</a> <a href="/tags/KMeans/" style="font-size: 10px;">KMeans</a> <a href="/tags/KNN/" style="font-size: 10px;">KNN</a> <a href="/tags/LeanCloud/" style="font-size: 10px;">LeanCloud</a> <a href="/tags/Network-Analysis/" style="font-size: 10px;">Network Analysis</a> <a href="/tags/Neural-Network/" style="font-size: 10px;">Neural Network</a> <a href="/tags/Nietzsche/" style="font-size: 10px;">Nietzsche</a> <a href="/tags/Nonparametric/" style="font-size: 15px;">Nonparametric</a> <a href="/tags/Philosophy/" style="font-size: 10px;">Philosophy</a> <a href="/tags/Python/" style="font-size: 10px;">Python</a> <a href="/tags/R/" style="font-size: 10px;">R</a> <a href="/tags/RNN/" style="font-size: 15px;">RNN</a> <a href="/tags/Regression/" style="font-size: 10px;">Regression</a> <a href="/tags/SVM/" style="font-size: 10px;">SVM</a> <a href="/tags/Spatial-Statistics/" style="font-size: 10px;">Spatial Statistics</a> <a href="/tags/Statistics/" style="font-size: 20px;">Statistics</a> <a href="/tags/TensorFlow/" style="font-size: 15px;">TensorFlow</a> <a href="/tags/Topic-Modeling/" style="font-size: 15px;">Topic Modeling</a> <a href="/tags/TwoSum/" style="font-size: 10px;">TwoSum</a> <a href="/tags/Valine/" style="font-size: 10px;">Valine</a> <a href="/tags/Vapnik-Chervonenkis/" style="font-size: 10px;">Vapnik-Chervonenkis</a> <a href="/tags/binary/" style="font-size: 10px;">binary</a> <a href="/tags/blog/" style="font-size: 20px;">blog</a> <a href="/tags/classification/" style="font-size: 10px;">classification</a> <a href="/tags/comment/" style="font-size: 10px;">comment</a> <a href="/tags/hurrican-tracks/" style="font-size: 10px;">hurrican tracks</a> <a href="/tags/kNN/" style="font-size: 10px;">kNN</a> <a href="/tags/local-search/" style="font-size: 10px;">local search</a> <a href="/tags/music/" style="font-size: 10px;">music</a> <a href="/tags/storm-surge/" style="font-size: 10px;">storm surge</a> <a href="/tags/visitor-counts/" style="font-size: 10px;">visitor counts</a> <a href="/tags/尼采/" style="font-size: 10px;">尼采</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/03/07/BayesianNonparametrics/">Bayesian Nonparametrics Notes</a>
          </li>
        
          <li>
            <a href="/2019/02/19/GaussianProcess/">Why are Gaussian Process Models called Nonparametric?</a>
          </li>
        
          <li>
            <a href="/2019/01/29/Birthday/">Make a Birthday Cake in R</a>
          </li>
        
          <li>
            <a href="/2019/01/27/BlogTheme/">Blog Theme 日神 x 酒神</a>
          </li>
        
          <li>
            <a href="/2019/01/26/Hexo-View/">Add Article Views to Your Hexo Blog</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Qiuyi Wu<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>